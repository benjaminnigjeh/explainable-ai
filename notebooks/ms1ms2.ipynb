{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d9f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28b1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wholeCasting(folder_path, cast_path):\n",
    "    os.chdir(folder_path)\n",
    "\n",
    "    def helper_regex(text):\n",
    "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
    "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
    "\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    substring_dict_sample = {\"TreatmentA\": [\"TreatmentA\"], \"TreatmentB\": [\"TreatmentB\"], \"TreatmentC\": [\"TreatmentC\"], \"TreatmentD\": [\"TreatmentD\"],}\n",
    " \n",
    "\n",
    "    file_name = []\n",
    "    sample_group = []\n",
    "\n",
    "    scan_type = []\n",
    "    scan_number = []\n",
    "    retention_time = []\n",
    "    cast_spectra = []\n",
    "\n",
    "    mz_value = []\n",
    "\n",
    "    for raw_name in files:\n",
    "        raw = RawFile(raw_name)\n",
    "        print(raw_name)\n",
    "        for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
    "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            file_name.append(raw_name)\n",
    "            sample_group.append(find_matching_keys(raw_name, substring_dict_sample)[0])\n",
    "\n",
    "            if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "                scan_type.append('MS1')\n",
    "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
    "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
    "                mz_value.append('')\n",
    "\n",
    "                data_intensities = [0]*13690\n",
    "                scan_masses = raw_scan.preferred_masses\n",
    "                scan_intensities = raw_scan.preferred_intensities\n",
    "\n",
    "                for j in range(0,len(scan_masses)):\n",
    "                    index = int(round(scan_masses[j], 2)*10)\n",
    "                    if index > 6000 and index < 19360:\n",
    "                        data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
    "\n",
    "                cast_spectra.append(data_intensities)\n",
    "\n",
    "\n",
    "            if str(helper_regex(raw_scan.scan_type)) == 'ms2':\n",
    "                scan_type.append('MS2')\n",
    "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
    "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
    "                mz_value.append(float(re.findall(r'[\\d]*[.][\\d]+', raw_scan.scan_type)[1]))\n",
    "\n",
    "                data_intensities = [0]*1600\n",
    "                scan_masses = raw_scan.preferred_masses\n",
    "                scan_intensities = raw_scan.preferred_intensities\n",
    "\n",
    "                for j in range(0,len(scan_masses)):\n",
    "                    index = round(scan_masses[j])\n",
    "                    if index > 400 and index < 2000:\n",
    "                        data_intensities[index-400] = scan_intensities[j] + data_intensities[index-400]\n",
    "                data_intensities = np.array(data_intensities)\n",
    "                max_value = np.max(data_intensities)\n",
    "                data_intensities_norm = data_intensities / max_value\n",
    "                data_intensities_norm = data_intensities_norm.astype(np.float16)\n",
    "                data_intensities_norm.tolist()\n",
    "                cast_spectra.append(data_intensities_norm)\n",
    "\n",
    "    scan_dict = {'sample_name': file_name, 'group_name': sample_group, 'scan': scan_number,'scan_type': scan_type, 'retntion time': retention_time, 'm/z': mz_value, 'cast spectra': cast_spectra}\n",
    "\n",
    "    with open(cast_path, \"wb\") as f:\n",
    "        pickle.dump(scan_dict, f)\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting(folder_path='D:/test1/', cast_path='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e191a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "def wholeCasting_npz(folder_path: str, out_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Build dense MS1/MS2 matrices from Thermo RAW files in folder_path and save to a compressed NPZ.\n",
    "    Only *.raw (case-insensitive) files are processed.\n",
    "    \"\"\"\n",
    "    # ---- Config ----\n",
    "    MS1_MIN_IDX, MS1_LEN = 6000, 13690  # 600.0 m/z * 10 .. 1935.9\n",
    "    MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "    MS2_MIN_IDX, MS2_LEN = 400, 1600    # m/z 400..1999\n",
    "    MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "    def _scan_type_label(text: str) -> str:\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "        return m.group(1).lower() if m else \"\"\n",
    "\n",
    "    def _group_from_name(name: str) -> str:\n",
    "        for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "            if g in name:\n",
    "                return g\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # ---- Find RAW files (case-insensitive) ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder_path}\"')\n",
    "\n",
    "    raw_files = sorted(\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.raw\"))) |\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.RAW\")))\n",
    "    )\n",
    "\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f'No \".raw\" files found in: \"{folder_path}\"')\n",
    "\n",
    "    # ---- Collectors ----\n",
    "    ms1_rows = []\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_names, group_names = [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_path in raw_files:\n",
    "        raw_name = os.path.basename(raw_path)\n",
    "\n",
    "        # assign file ID\n",
    "        if raw_name not in file_to_id:\n",
    "            file_to_id[raw_name] = len(file_names)\n",
    "            file_names.append(raw_name)\n",
    "\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "\n",
    "        f_id = file_to_id[raw_name]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW (skip gracefully if not readable)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f'[skip] Not a valid RAW or unreadable: {raw_path}')\n",
    "            continue\n",
    "\n",
    "        total_scans = getattr(raw, \"number_of_scans\", 0) or 0\n",
    "        for i in tqdm(range(1, total_scans), desc=raw_name, ncols=100):\n",
    "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            sc_num = raw_scan.scan_statistics.scan_number\n",
    "            rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            stype = _scan_type_label(raw_scan.scan_type)  # 'ms' or 'ms2'\n",
    "\n",
    "            masses = np.asarray(raw_scan.preferred_masses, dtype=float)\n",
    "            intens = np.asarray(raw_scan.preferred_intensities, dtype=float)\n",
    "            if masses.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                idx = np.rint(np.round(masses, 2) * 10).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if mask.any():\n",
    "                    b = idx[mask] - MS1_MIN_IDX\n",
    "                    np.add.at(vec, b, intens[mask].astype(np.float32, copy=False))\n",
    "                vec = vec.astype(np.float32, copy=False)\n",
    "\n",
    "                ms1_rows.append(vec)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                vec = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                idx = np.rint(np.round(masses)).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if mask.any():\n",
    "                    b = idx[mask] - MS2_MIN_IDX\n",
    "                    np.add.at(vec, b, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(vec.max())\n",
    "                vec = (vec / vmax).astype(np.float16, copy=False) if vmax > 0 else vec.astype(np.float16, copy=False)\n",
    "\n",
    "                m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "            else:\n",
    "                # unknown scan type; skip\n",
    "                continue\n",
    "\n",
    "    # ---- Stack to dense matrices ----\n",
    "    MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False) if ms1_rows else np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "    MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False) if ms2_rows else np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "\n",
    "    # ---- Metadata arrays ----\n",
    "    ms1_scan = np.asarray(ms1_scan, dtype=np.int32)\n",
    "    ms1_rt   = np.asarray(ms1_rt,   dtype=np.float32)\n",
    "    ms1_file_id = np.asarray(ms1_file_id, dtype=np.int32)\n",
    "    ms1_group_id = np.asarray(ms1_group_id, dtype=np.int32)\n",
    "\n",
    "    ms2_scan = np.asarray(ms2_scan, dtype=np.int32)\n",
    "    ms2_rt   = np.asarray(ms2_rt,   dtype=np.float32)\n",
    "    ms2_prec_mz = np.asarray(ms2_prec_mz, dtype=np.float32)\n",
    "    ms2_file_id = np.asarray(ms2_file_id, dtype=np.int32)\n",
    "    ms2_group_id = np.asarray(ms2_group_id, dtype=np.int32)\n",
    "\n",
    "    file_names_lookup  = np.asarray(file_names,  dtype=object)\n",
    "    group_names_lookup = np.asarray(group_names, dtype=object)\n",
    "\n",
    "    # ---- Save compressed NPZ (don’t put it inside the RAW folder to avoid accidental pickup) ----\n",
    "    if not out_path.endswith(\".npz\"):\n",
    "        out_path += \".npz\"\n",
    "    out_path = os.path.abspath(out_path)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        ms1_matrix=MS1,\n",
    "        ms2_matrix=MS2,\n",
    "        ms1_scan=ms1_scan,\n",
    "        ms1_rt=ms1_rt,\n",
    "        ms1_file_id=ms1_file_id,\n",
    "        ms1_group_id=ms1_group_id,\n",
    "        ms2_scan=ms2_scan,\n",
    "        ms2_rt=ms2_rt,\n",
    "        ms2_precursor_mz=ms2_prec_mz,\n",
    "        ms2_file_id=ms2_file_id,\n",
    "        ms2_group_id=ms2_group_id,\n",
    "        file_names_lookup=file_names_lookup,\n",
    "        group_names_lookup=group_names_lookup,\n",
    "    )\n",
    "    print(f\"Saved dense matrices: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c2becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test1.raw: 100%|██████████████████████████████████████████████| 20421/20421 [03:58<00:00, 85.79it/s]\n",
      "test2.raw: 100%|██████████████████████████████████████████████| 18112/18112 [03:26<00:00, 87.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dense matrices: D:\\casts\\test2.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\casts\\\\test2.npz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeCasting_npz(folder_path=r\"D:\\raw2\", out_path=r\"D:\\casts\\test2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f6156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms1_matrix', 'ms2_matrix', 'ms1_scan', 'ms1_rt', 'ms1_file_id', 'ms1_group_id', 'ms2_scan', 'ms2_rt', 'ms2_precursor_mz', 'ms2_file_id', 'ms2_group_id', 'file_names_lookup', 'group_names_lookup']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load the file (replace with your real path)\n",
    "data = np.load(r\"D:\\casts\\test.npz\", allow_pickle=True)\n",
    "\n",
    "# list of all arrays inside\n",
    "print(data.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50746d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MS1 = data[\"ms1_matrix\"]        # shape (N_ms1, 13690), float16\n",
    "MS2 = data[\"ms2_matrix\"]        # shape (N_ms2, 1600), float16\n",
    "\n",
    "ms1_rt   = data[\"ms1_rt\"]       # retention times for MS1\n",
    "ms2_rt   = data[\"ms2_rt\"]       # retention times for MS2\n",
    "ms2_prec = data[\"ms2_precursor_mz\"]\n",
    "\n",
    "files  = data[\"file_names_lookup\"]   # lookup table for file IDs\n",
    "groups = data[\"group_names_lookup\"]  # lookup table for group IDs\n",
    "\n",
    "# Example: map first MS1 row back to file/group\n",
    "file_id  = data[\"ms1_file_id\"][0]\n",
    "group_id = data[\"ms1_group_id\"][0]\n",
    "print(\"First MS1 comes from:\", files[file_id], \"group:\", groups[group_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "def wholeCasting_npz(folder_path: str, out_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Build dense MS1/MS2 matrices from Thermo RAW files in folder_path and save to a compressed NPZ.\n",
    "    Only *.raw (case-insensitive) files are processed.\n",
    "\n",
    "    MS1 normalization: column-wise (per m/z bin) by the max across all scans/runs, then cast to float16.\n",
    "    MS2 normalization: per-scan vector / max(vector), stored as float16.\n",
    "    \"\"\"\n",
    "    # ---- Config ----\n",
    "    MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9 -> indices [6000, 199?]\n",
    "    MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN  # exclusive upper bound\n",
    "    MS2_MIN_IDX, MS2_LEN = 400, 1600     # integer m/z 400..1999\n",
    "    MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def _scan_type_label(text: str) -> str:\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "        return m.group(1).lower() if m else \"\"\n",
    "\n",
    "    def _group_from_name(name: str) -> str:\n",
    "        for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "            if g in name:\n",
    "                return g\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def _as_float_array(x):\n",
    "        if x is None:\n",
    "            return np.array([], dtype=float)\n",
    "        a = np.asarray(x)\n",
    "        return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "    # ---- Find RAW files (case-insensitive) ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder_path}\"')\n",
    "\n",
    "    raw_files = sorted(\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.raw\"))) |\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.RAW\")))\n",
    "    )\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f'No \".raw\" files found in: \"{folder_path}\"')\n",
    "\n",
    "    # ---- Collectors ----\n",
    "    ms1_rows = []  # keep as float32 until normalization step\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []  # we will store each as float16 after per-scan normalization\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_names, group_names = [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_path in raw_files:\n",
    "        raw_name = os.path.basename(raw_path)\n",
    "\n",
    "        # assign file & group IDs\n",
    "        if raw_name not in file_to_id:\n",
    "            file_to_id[raw_name] = len(file_names)\n",
    "            file_names.append(raw_name)\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "        f_id = file_to_id[raw_name]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW (skip gracefully if not readable)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "        except Exception:\n",
    "            total_scans = 0\n",
    "\n",
    "        # Thermo scans are typically 1..N inclusive\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=raw_name, ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sc_num = int(raw_scan.scan_statistics.scan_number)\n",
    "            except Exception:\n",
    "                sc_num = i\n",
    "\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)  # 'ms' or 'ms2'\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # 0.1 m/z bins: index = round(mz*10)\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                ms1_rows.append(vec)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                # 1.0 m/z bins: index = round(mz)\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # Try safer precursor extraction; fallback to regex\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Dispose/close if supported\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Stack to dense matrices ----\n",
    "    # MS1: stack as float32, then column-wise normalize by max across scans, then cast to float16.\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "        col_max = MS1.max(axis=0)\n",
    "        # avoid division by zero\n",
    "        col_max[col_max == 0] = 1.0\n",
    "        MS1 = (MS1 / col_max).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "\n",
    "    # MS2: rows were already normalized and cast to float16 individually\n",
    "    MS2 = (np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "           if ms2_rows else np.zeros((0, MS2_LEN), dtype=np.float16))\n",
    "\n",
    "    # ---- Metadata arrays ----\n",
    "    ms1_scan     = np.asarray(ms1_scan,     dtype=np.int32)\n",
    "    ms1_rt       = np.asarray(ms1_rt,       dtype=np.float32)\n",
    "    ms1_file_id  = np.asarray(ms1_file_id,  dtype=np.int32)\n",
    "    ms1_group_id = np.asarray(ms1_group_id, dtype=np.int32)\n",
    "\n",
    "    ms2_scan     = np.asarray(ms2_scan,     dtype=np.int32)\n",
    "    ms2_rt       = np.asarray(ms2_rt,       dtype=np.float32)\n",
    "    ms2_prec_mz  = np.asarray(ms2_prec_mz,  dtype=np.float32)\n",
    "    ms2_file_id  = np.asarray(ms2_file_id,  dtype=np.int32)\n",
    "    ms2_group_id = np.asarray(ms2_group_id, dtype=np.int32)\n",
    "\n",
    "    file_names_lookup  = np.asarray(file_names,  dtype=object)\n",
    "    group_names_lookup = np.asarray(group_names, dtype=object)\n",
    "\n",
    "    # ---- Save compressed NPZ ----\n",
    "    if not out_path.endswith(\".npz\"):\n",
    "        out_path += \".npz\"\n",
    "    out_path = os.path.abspath(out_path)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        ms1_matrix=MS1,\n",
    "        ms2_matrix=MS2,\n",
    "        ms1_scan=ms1_scan,\n",
    "        ms1_rt=ms1_rt,\n",
    "        ms1_file_id=ms1_file_id,\n",
    "        ms1_group_id=ms1_group_id,\n",
    "        ms2_scan=ms2_scan,\n",
    "        ms2_rt=ms2_rt,\n",
    "        ms2_precursor_mz=ms2_prec_mz,\n",
    "        ms2_file_id=ms2_file_id,\n",
    "        ms2_group_id=ms2_group_id,\n",
    "        file_names_lookup=file_names_lookup,\n",
    "        group_names_lookup=group_names_lookup,\n",
    "    )\n",
    "    print(f\"Saved dense matrices: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f4e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test1.raw: 100%|██████████████████████████████████████████████| 20422/20422 [03:55<00:00, 86.55it/s]\n",
      "test2.raw: 100%|██████████████████████████████████████████████| 18113/18113 [03:27<00:00, 87.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dense matrices: D:\\casts\\test3.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\casts\\\\test3.npz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeCasting_npz(folder_path=r\"D:\\raw2\", out_path=r\"D:\\casts\\test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f71f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "def wholeCasting_npz(folder_path: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Build dense MS1/MS2 matrices from Thermo RAW files in folder_path.\n",
    "    Save MS1 and MS2 separately, and save shared metadata separately.\n",
    "\n",
    "    Outputs (derived from out_path):\n",
    "      <base>.ms1.npz  -> {\"ms1_matrix\": float16 [n_ms1_scans, MS1_LEN]}\n",
    "      <base>.ms2.npz  -> {\"ms2_matrix\": float16 [n_ms2_scans, MS2_LEN]}\n",
    "      <base>.meta.npz -> {\n",
    "          \"ms1_scan\",\"ms1_rt\",\"ms1_file_id\",\"ms1_group_id\",\n",
    "          \"ms2_scan\",\"ms2_rt\",\"ms2_precursor_mz\",\"ms2_file_id\",\"ms2_group_id\",\n",
    "          \"file_names_lookup\",\"group_names_lookup\"\n",
    "      }\n",
    "\n",
    "    MS1 normalization: per-bin (column-wise) max across all scans -> [0,1], then float16.\n",
    "    MS2 normalization: per-scan max -> [0,1], then float16.\n",
    "    \"\"\"\n",
    "    # ---- Config ----\n",
    "    MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9 -> indices [6000, 6000+13690)\n",
    "    MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "    MS2_MIN_IDX, MS2_LEN = 400, 1600     # integer m/z 400..1999 -> indices [400, 400+1600)\n",
    "    MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def _scan_type_label(text: str) -> str:\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "        return m.group(1).lower() if m else \"\"\n",
    "\n",
    "    def _group_from_name(name: str) -> str:\n",
    "        for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "            if g in name:\n",
    "                return g\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def _as_float_array(x):\n",
    "        if x is None:\n",
    "            return np.array([], dtype=float)\n",
    "        a = np.asarray(x)\n",
    "        return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "    # ---- Find RAW files ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder_path}\"')\n",
    "\n",
    "    raw_files = sorted(\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.raw\"))) |\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.RAW\")))\n",
    "    )\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f'No \".raw\" files found in: \"{folder_path}\"')\n",
    "\n",
    "    # ---- Collectors ----\n",
    "    ms1_rows = []  # float32 until normalization\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []  # store each row as float16 after per-scan normalization\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_names, group_names = [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_path in raw_files:\n",
    "        raw_name = os.path.basename(raw_path)\n",
    "\n",
    "        # assign file & group IDs\n",
    "        if raw_name not in file_to_id:\n",
    "            file_to_id[raw_name] = len(file_names)\n",
    "            file_names.append(raw_name)\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "        f_id = file_to_id[raw_name]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW (skip gracefully if not readable)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "        except Exception:\n",
    "            total_scans = 0\n",
    "\n",
    "        # Thermo scans are typically 1..N inclusive\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=raw_name, ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sc_num = int(raw_scan.scan_statistics.scan_number)\n",
    "            except Exception:\n",
    "                sc_num = i\n",
    "\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)  # 'ms' or 'ms2'\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # 0.1 m/z bins (index = round(mz*10))\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                ms1_rows.append(vec)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                # 1.0 m/z bins (index = round(mz))\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # Precursor extraction (prefer attribute, fallback to regex)\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Dispose/close if supported\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Stack & normalize ----\n",
    "    # MS1: stack as float32, column-wise normalize by max across scans, cast to float16\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "        col_max = MS1.max(axis=0)\n",
    "        col_max[col_max == 0] = 1.0\n",
    "        MS1 = (MS1 / col_max).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "\n",
    "    # MS2: rows are already normalized & float16\n",
    "    MS2 = (np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "           if ms2_rows else np.zeros((0, MS2_LEN), dtype=np.float16))\n",
    "\n",
    "    # ---- Metadata arrays (shared) ----\n",
    "    ms1_scan     = np.asarray(ms1_scan,     dtype=np.int32)\n",
    "    ms1_rt       = np.asarray(ms1_rt,       dtype=np.float32)\n",
    "    ms1_file_id  = np.asarray(ms1_file_id,  dtype=np.int32)\n",
    "    ms1_group_id = np.asarray(ms1_group_id, dtype=np.int32)\n",
    "\n",
    "    ms2_scan     = np.asarray(ms2_scan,     dtype=np.int32)\n",
    "    ms2_rt       = np.asarray(ms2_rt,       dtype=np.float32)\n",
    "    ms2_prec_mz  = np.asarray(ms2_prec_mz,  dtype=np.float32)\n",
    "    ms2_file_id  = np.asarray(ms2_file_id,  dtype=np.int32)\n",
    "    ms2_group_id = np.asarray(ms2_group_id, dtype=np.int32)\n",
    "\n",
    "    file_names_lookup  = np.asarray(file_names,  dtype=object)\n",
    "    group_names_lookup = np.asarray(group_names, dtype=object)\n",
    "\n",
    "    # ---- Save outputs (separate data + shared meta) ----\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "\n",
    "    ms1_path  = f\"{base}.ms1.npz\"\n",
    "    ms2_path  = f\"{base}.ms2.npz\"\n",
    "    meta_path = f\"{base}.meta.npz\"\n",
    "\n",
    "    # data files\n",
    "    np.savez_compressed(ms1_path, ms1_matrix=MS1)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2)\n",
    "\n",
    "    # metadata file\n",
    "    np.savez_compressed(\n",
    "        meta_path,\n",
    "        ms1_scan=ms1_scan,\n",
    "        ms1_rt=ms1_rt,\n",
    "        ms1_file_id=ms1_file_id,\n",
    "        ms1_group_id=ms1_group_id,\n",
    "        ms2_scan=ms2_scan,\n",
    "        ms2_rt=ms2_rt,\n",
    "        ms2_precursor_mz=ms2_prec_mz,\n",
    "        ms2_file_id=ms2_file_id,\n",
    "        ms2_group_id=ms2_group_id,\n",
    "        file_names_lookup=file_names_lookup,\n",
    "        group_names_lookup=group_names_lookup,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved MS1:  {ms1_path}  shape={MS1.shape}\")\n",
    "    print(f\"Saved MS2:  {ms2_path}  shape={MS2.shape}\")\n",
    "    print(f\"Saved meta: {meta_path}\")\n",
    "    return {\"ms1\": ms1_path, \"ms2\": ms2_path, \"meta\": meta_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1db30e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwholeCasting_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcasts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 89\u001b[0m, in \u001b[0;36mwholeCasting_npz\u001b[1;34m(folder_path, out_path)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# open RAW (skip gracefully if not readable)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mRawFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[skip] Cannot open RAW: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benja\\miniconda3\\envs\\casting\\lib\\site-packages\\fisher_py\\raw_file.py:81\u001b[0m, in \u001b[0;36mRawFile.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ms2_retention_times \u001b[38;5;241m=\u001b[39m rt\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# fetch filters for MS2\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m scan_numbers, filter_masses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ms2_scan_numbers_and_masses_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ms2_filter_scan_numbers \u001b[38;5;241m=\u001b[39m scan_numbers\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ms2_filter_masses \u001b[38;5;241m=\u001b[39m filter_masses\n",
      "File \u001b[1;32mc:\\Users\\benja\\miniconda3\\envs\\casting\\lib\\site-packages\\fisher_py\\raw_file.py:109\u001b[0m, in \u001b[0;36mRawFile._get_ms2_scan_numbers_and_masses_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m scan_number \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m first_scan\n\u001b[1;32m--> 109\u001b[0m precursor_mass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scan_filter_precursor_mass_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precursor_mass \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benja\\miniconda3\\envs\\casting\\lib\\site-packages\\fisher_py\\raw_file.py:135\u001b[0m, in \u001b[0;36mRawFile._get_scan_filter_precursor_mass_\u001b[1;34m(self, scan_number)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_scan_filter_precursor_mass_\u001b[39m(\u001b[38;5;28mself\u001b[39m, scan_number: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     scan_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_file_access\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scan_event_for_scan_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scan_event\u001b[38;5;241m.\u001b[39mms_order \u001b[38;5;241m!=\u001b[39m MsOrderType\u001b[38;5;241m.\u001b[39mMs2:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benja\\miniconda3\\envs\\casting\\lib\\site-packages\\fisher_py\\raw_file_reader\\raw_file_access.py:398\u001b[0m, in \u001b[0;36mRawFileAccess.get_scan_event_for_scan_number\u001b[1;34m(self, scan)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_scan_event_for_scan_number\u001b[39m(\u001b[38;5;28mself\u001b[39m, scan: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScanEvent:\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    Summary:\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Gets the scan event details for a scan. Determines how this scan was programmed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        Thrown if the selected device is not of type MS\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ScanEvent\u001b[38;5;241m.\u001b[39m_get_wrapper_(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_wrapped_object_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetScanEventForScanNumber\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wholeCasting_npz(folder_path=r\"D:\\raw2\", out_path=r\"D:\\casts\\test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006c1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "def wholeCasting_npz(folder_path: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Build dense MS1/MS2 matrices from Thermo RAW files in folder_path.\n",
    "    Save MS2 first (with shared metadata), then MS1 (with the same metadata embedded).\n",
    "\n",
    "    Outputs (derived from out_path):\n",
    "      <base>.ms2.npz  -> ms2_matrix (float16) + shared metadata\n",
    "      <base>.ms1.npz  -> ms1_matrix (float16) + shared metadata\n",
    "\n",
    "    MS1 normalization: per-bin (column-wise) max across all scans -> [0,1], then float16.\n",
    "    MS2 normalization: per-scan max -> [0,1], then float16.\n",
    "    \"\"\"\n",
    "    # ---- Config ----\n",
    "    MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9 -> indices [6000, 6000+13690)\n",
    "    MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "    MS2_MIN_IDX, MS2_LEN = 400, 1600     # integer m/z 400..1999 -> indices [400, 400+1600)\n",
    "    MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def _scan_type_label(text: str) -> str:\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "        return m.group(1).lower() if m else \"\"\n",
    "\n",
    "    def _group_from_name(name: str) -> str:\n",
    "        for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "            if g in name:\n",
    "                return g\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def _as_float_array(x):\n",
    "        if x is None:\n",
    "            return np.array([], dtype=float)\n",
    "        a = np.asarray(x)\n",
    "        return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "    # ---- Find RAW files ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder_path}\"')\n",
    "\n",
    "    raw_files = sorted(\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.raw\"))) |\n",
    "        set(glob.glob(os.path.join(folder_path, \"*.RAW\")))\n",
    "    )\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f'No \".raw\" files found in: \"{folder_path}\"')\n",
    "\n",
    "    # ---- Collectors ----\n",
    "    ms1_rows = []  # float32 until normalization\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []  # store rows as float16 after per-scan normalization\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_names, group_names = [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_path in raw_files:\n",
    "        raw_name = os.path.basename(raw_path)\n",
    "\n",
    "        # assign file & group IDs\n",
    "        if raw_name not in file_to_id:\n",
    "            file_to_id[raw_name] = len(file_names)\n",
    "            file_names.append(raw_name)\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "        f_id = file_to_id[raw_name]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW (skip gracefully if not readable)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "        except Exception:\n",
    "            total_scans = 0\n",
    "\n",
    "        # Thermo scans are typically 1..N inclusive\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=raw_name, ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sc_num = int(raw_scan.scan_statistics.scan_number)\n",
    "            except Exception:\n",
    "                sc_num = i\n",
    "\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)  # 'ms' or 'ms2'\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # 0.1 m/z bins (index = round(mz*10))\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                ms1_rows.append(vec)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                # 1.0 m/z bins (index = round(mz))\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # Precursor extraction (prefer attribute, fallback to regex)\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Dispose/close if supported\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Build shared metadata once ----\n",
    "    ms1_scan_arr     = np.asarray(ms1_scan,     dtype=np.int32)\n",
    "    ms1_rt_arr       = np.asarray(ms1_rt,       dtype=np.float32)\n",
    "    ms1_file_id_arr  = np.asarray(ms1_file_id,  dtype=np.int32)\n",
    "    ms1_group_id_arr = np.asarray(ms1_group_id, dtype=np.int32)\n",
    "\n",
    "    ms2_scan_arr     = np.asarray(ms2_scan,     dtype=np.int32)\n",
    "    ms2_rt_arr       = np.asarray(ms2_rt,       dtype=np.float32)\n",
    "    ms2_prec_mz_arr  = np.asarray(ms2_prec_mz,  dtype=np.float32)\n",
    "    ms2_file_id_arr  = np.asarray(ms2_file_id,  dtype=np.int32)\n",
    "    ms2_group_id_arr = np.asarray(ms2_group_id, dtype=np.int32)\n",
    "\n",
    "    file_names_lookup  = np.asarray(file_names,  dtype=object)\n",
    "    group_names_lookup = np.asarray(group_names, dtype=object)\n",
    "\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "    ms2_path = f\"{base}.ms2.npz\"\n",
    "    ms1_path = f\"{base}.ms1.npz\"\n",
    "\n",
    "    # ---- Save MS2 first (then free) ----\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        ms2_path,\n",
    "        ms2_matrix=MS2,\n",
    "        # embed shared metadata\n",
    "        ms1_scan=ms1_scan_arr,\n",
    "        ms1_rt=ms1_rt_arr,\n",
    "        ms1_file_id=ms1_file_id_arr,\n",
    "        ms1_group_id=ms1_group_id_arr,\n",
    "        ms2_scan=ms2_scan_arr,\n",
    "        ms2_rt=ms2_rt_arr,\n",
    "        ms2_precursor_mz=ms2_prec_mz_arr,\n",
    "        ms2_file_id=ms2_file_id_arr,\n",
    "        ms2_group_id=ms2_group_id_arr,\n",
    "        file_names_lookup=file_names_lookup,\n",
    "        group_names_lookup=group_names_lookup,\n",
    "    )\n",
    "    print(f\"Saved MS2: {ms2_path}  shape={MS2.shape}\")\n",
    "    del MS2, ms2_rows  # free memory\n",
    "    gc.collect()\n",
    "\n",
    "    # ---- Now build & save MS1 (second) ----\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "        col_max = MS1.max(axis=0)\n",
    "        col_max[col_max == 0] = 1.0\n",
    "        MS1 = (MS1 / col_max).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        ms1_path,\n",
    "        ms1_matrix=MS1,\n",
    "        # embed the same shared metadata again\n",
    "        ms1_scan=ms1_scan_arr,\n",
    "        ms1_rt=ms1_rt_arr,\n",
    "        ms1_file_id=ms1_file_id_arr,\n",
    "        ms1_group_id=ms1_group_id_arr,\n",
    "        ms2_scan=ms2_scan_arr,\n",
    "        ms2_rt=ms2_rt_arr,\n",
    "        ms2_precursor_mz=ms2_prec_mz_arr,\n",
    "        ms2_file_id=ms2_file_id_arr,\n",
    "        ms2_group_id=ms2_group_id_arr,\n",
    "        file_names_lookup=file_names_lookup,\n",
    "        group_names_lookup=group_names_lookup,\n",
    "    )\n",
    "    print(f\"Saved MS1: {ms1_path}  shape={MS1.shape}\")\n",
    "\n",
    "    # Optionally free MS1 too\n",
    "    del MS1, ms1_rows\n",
    "    gc.collect()\n",
    "\n",
    "    return {\"ms2\": ms2_path, \"ms1\": ms1_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting_npz(folder_path=r\"D:\\raw2\", out_path=r\"D:\\casts\\test5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f55bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "def wholeCasting_npz(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Build dense MS1/MS2 matrices from Thermo RAW files found in one or more folders.\n",
    "    Saves MS2 first (with metadata), then MS1 (with metadata), and also writes a standalone metadata file.\n",
    "\n",
    "    folder_paths: str or Iterable[str] of directories\n",
    "    Outputs (derived from out_path):\n",
    "      <base>.ms2.npz   -> ms2_matrix (float16) + metadata\n",
    "      <base>.ms1.npz   -> ms1_matrix (float16) + metadata\n",
    "      <base>.meta.npz  -> metadata only (no matrices)\n",
    "\n",
    "    MS1 normalization: per-bin (column-wise) max across all scans -> [0,1], then float16.\n",
    "    MS2 normalization: per-scan max -> [0,1], then float16.\n",
    "    \"\"\"\n",
    "    # ---- Config ----\n",
    "    MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9\n",
    "    MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "    MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999\n",
    "    MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def _scan_type_label(text: str) -> str:\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "        return m.group(1).lower() if m else \"\"\n",
    "\n",
    "    def _group_from_name(name: str) -> str:\n",
    "        for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "            if g in name:\n",
    "                return g\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def _as_float_array(x):\n",
    "        if x is None:\n",
    "            return np.array([], dtype=float)\n",
    "        a = np.asarray(x)\n",
    "        return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "    def _ensure_folder_list(paths):\n",
    "        if isinstance(paths, (list, tuple)):\n",
    "            return list(paths)\n",
    "        return [paths]\n",
    "\n",
    "    # ---- Gather RAW files from all folders ----\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}')\n",
    "\n",
    "    # ---- Collectors ----\n",
    "    ms1_rows = []\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_basenames, file_abspaths, group_names = [], [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_path in raw_files:\n",
    "        raw_abs = os.path.abspath(raw_path)\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "\n",
    "        f_id = file_to_id[raw_abs]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=raw_name, ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                ms1_rows.append(vec)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Build shared metadata ----\n",
    "    metadata = dict(\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "        ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "        ms2_group_id=np.asarray(ms2_group_id, dtype=np.int32),\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=object),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=object),\n",
    "        group_names_lookup=np.asarray(group_names, dtype=object),\n",
    "    )\n",
    "\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "    ms2_path = f\"{base}.ms2.npz\"\n",
    "    ms1_path = f\"{base}.ms1.npz\"\n",
    "    meta_path = f\"{base}.meta.npz\"\n",
    "\n",
    "    # ---- Save MS2 first ----\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"Saved MS2: {ms2_path}  shape={MS2.shape}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # ---- Save MS1 second ----\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "        col_max = MS1.max(axis=0)\n",
    "        col_max[col_max == 0] = 1.0\n",
    "        MS1 = (MS1 / col_max).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms1_path, ms1_matrix=MS1, **metadata)\n",
    "    print(f\"Saved MS1: {ms1_path}  shape={MS1.shape}\")\n",
    "    del MS1, ms1_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # ---- Save metadata standalone ----\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"Saved META: {meta_path}\")\n",
    "\n",
    "    return {\"ms2\": ms2_path, \"ms1\": ms1_path, \"meta\": meta_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting_npz([r\"D:\\TreatmentABC\", r\"D:\\TreatmentD\"], r\"D:\\casts\\test6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import tempfile\n",
    "\n",
    "# If you have fisher_py installed, uncomment:\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "# -----------------------------\n",
    "# Shared config / helpers\n",
    "# -----------------------------\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _group_from_name(name: str) -> str:\n",
    "    for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "        if g in name:\n",
    "            return g\n",
    "    return \"Unknown\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _ensure_folder_list(paths):\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        return list(paths)\n",
    "    return [paths]\n",
    "\n",
    "def _gather_raw_files(folder_paths):\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\n",
    "            f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}'\n",
    "        )\n",
    "    return raw_files\n",
    "\n",
    "def _base_paths(out_path: str):\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "    return (f\"{base}.ms2.npz\", f\"{base}.ms1.npz\", f\"{base}.meta.npz\")\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 1: MS2 + Metadata\n",
    "# -----------------------------\n",
    "def build_ms2_and_meta(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Build MS2 (per-scan normalized) and shared metadata, saving:\n",
    "      <base>.ms2.npz  -> ms2_matrix (float16) + metadata\n",
    "      <base>.meta.npz -> metadata only\n",
    "    \"\"\"\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_basenames, file_abspaths, group_names = [], [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "\n",
    "        f_id = file_to_id[raw_abs]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass1-MS2] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # Just metadata for MS1 in this pass\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # precursor m/z extraction\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Build metadata (shared by both MS1 and MS2 output files)\n",
    "    metadata = dict(\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "        ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "        ms2_group_id=np.asarray(ms2_group_id, dtype=np.int32),\n",
    "\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=object),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=object),\n",
    "        group_names_lookup=np.asarray(group_names, dtype=object),\n",
    "    )\n",
    "\n",
    "    ms2_path, _, meta_path = _base_paths(out_path)\n",
    "\n",
    "    # Save MS2\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"Saved MS2: {ms2_path}  shape={MS2.shape}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Save metadata standalone\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"Saved META: {meta_path}\")\n",
    "\n",
    "    return {\"ms2\": ms2_path, \"meta\": meta_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 2: MS1 only (streamed, low-RAM)\n",
    "# -----------------------------\n",
    "def build_ms1_only(folder_paths, out_path: str, meta_path: str = None):\n",
    "    \"\"\"\n",
    "    Build MS1 matrix in a streamed way (low RAM) and save:\n",
    "      <base>.ms1.npz -> ms1_matrix (float16) + metadata\n",
    "\n",
    "    MS1 normalization: per-column (bin-wise) max across *all* MS1 scans.\n",
    "\n",
    "    Two sweeps:\n",
    "      1) Count MS1 rows + compute col_max (no storage)\n",
    "      2) Allocate memmap [n_rows, MS1_LEN], fill normalized rows, save compressed NPZ.\n",
    "\n",
    "    If meta_path is provided, that metadata is embedded into the MS1 NPZ; otherwise it is\n",
    "    rebuilt on the fly (slower) and saved alongside.\n",
    "    \"\"\"\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "    ms1_count = 0\n",
    "    col_max = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "    # If metadata was saved in Pass 1, load it to re-use\n",
    "    metadata = None\n",
    "    if meta_path is not None and os.path.exists(meta_path):\n",
    "        with np.load(meta_path, allow_pickle=True) as meta_npz:\n",
    "            metadata = {k: meta_npz[k] for k in meta_npz.files}\n",
    "\n",
    "    # If no metadata given, we’ll rebuild only MS1-related metadata\n",
    "    rebuild_metadata = metadata is None\n",
    "\n",
    "    # For (re)building MS1 metadata if needed\n",
    "    if rebuild_metadata:\n",
    "        file_basenames, file_abspaths, group_names = [], [], []\n",
    "        file_to_id, group_to_id = {}, {}\n",
    "        ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    # -------- Pass 2a: count & col_max --------\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if rebuild_metadata:\n",
    "            if raw_abs not in file_to_id:\n",
    "                file_to_id[raw_abs] = len(file_basenames)\n",
    "                file_basenames.append(raw_name)\n",
    "                file_abspaths.append(raw_abs)\n",
    "\n",
    "            group = _group_from_name(raw_name)\n",
    "            if group not in group_to_id:\n",
    "                group_to_id[group] = len(group_names)\n",
    "                group_names.append(group)\n",
    "\n",
    "            f_id = file_to_id[raw_abs]\n",
    "            g_id = group_to_id[group]\n",
    "        else:\n",
    "            f_id = g_id = None  # unused\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2a-MS1 count] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            if stype != \"ms\":\n",
    "                continue\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "            mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            # Update col_max without storing the full row\n",
    "            vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "            np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "            np.maximum(col_max, vec, out=col_max)\n",
    "            ms1_count += 1\n",
    "\n",
    "            # (Re)build minimal metadata for MS1 if needed\n",
    "            if rebuild_metadata:\n",
    "                sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "                try:\n",
    "                    rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "                except Exception:\n",
    "                    rt = np.nan\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Guard: no MS1 scans\n",
    "    if ms1_count == 0:\n",
    "        # Build/save empty file with metadata\n",
    "        ms1_path = _base_paths(out_path)[1]\n",
    "        if metadata is None and rebuild_metadata:\n",
    "            metadata = dict(\n",
    "                ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "                ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "                ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "                ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "                file_names_lookup=np.asarray(file_basenames, dtype=object),\n",
    "                file_paths_lookup=np.asarray(file_abspaths, dtype=object),\n",
    "                group_names_lookup=np.asarray(group_names, dtype=object),\n",
    "            )\n",
    "        elif metadata is None:\n",
    "            metadata = {}\n",
    "        empty = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "        np.savez_compressed(ms1_path, ms1_matrix=empty, **metadata)\n",
    "        print(f\"Saved MS1 (empty): {ms1_path}\")\n",
    "        return {\"ms1\": ms1_path}\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    col_max[col_max == 0] = 1.0\n",
    "\n",
    "    # If we had to rebuild metadata, finalize it now. Otherwise we already have it.\n",
    "    if rebuild_metadata:\n",
    "        metadata = dict(\n",
    "            ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "            ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "            ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "            ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "            # Note: MS2-related fields will be missing (that’s OK if you only need MS1 here)\n",
    "            file_names_lookup=np.asarray(file_basenames, dtype=object),\n",
    "            file_paths_lookup=np.asarray(file_abspaths, dtype=object),\n",
    "            group_names_lookup=np.asarray(group_names, dtype=object),\n",
    "        )\n",
    "\n",
    "    # -------- Pass 2b: fill normalized rows into a memmap --------\n",
    "    ms1_path = _base_paths(out_path)[1]\n",
    "\n",
    "    # Create a temp directory for the memmap file\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        memmap_path = os.path.join(tmpdir, \"ms1_memmap.npy\")\n",
    "        MS1_map = np.memmap(memmap_path, dtype=np.float16, mode=\"w+\", shape=(ms1_count, MS1_LEN))\n",
    "\n",
    "        row = 0\n",
    "        for raw_abs in raw_files:\n",
    "            raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "            try:\n",
    "                raw = RawFile(raw_abs)\n",
    "            except Exception as e:\n",
    "                print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "                continue\n",
    "\n",
    "            total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "            for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2b-MS1 write] {raw_name}\", ncols=100):\n",
    "                if row >= ms1_count:\n",
    "                    break  # safety\n",
    "\n",
    "                try:\n",
    "                    raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                stype = _scan_type_label(raw_scan.scan_type)\n",
    "                if stype != \"ms\":\n",
    "                    continue\n",
    "\n",
    "                masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "                intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "                if masses.size == 0 or intens.size == 0:\n",
    "                    continue\n",
    "\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                v32 /= col_max  # per-column normalization from Pass 2a\n",
    "                MS1_map[row, :] = v32.astype(np.float16, copy=False)\n",
    "                row += 1\n",
    "\n",
    "            try:\n",
    "                raw.dispose()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Flush memmap to disk\n",
    "        del MS1_map\n",
    "\n",
    "        # Load the memmap file and write compressed NPZ with metadata\n",
    "        MS1_final = np.load(memmap_path, mmap_mode=\"r\")\n",
    "        np.savez_compressed(ms1_path, ms1_matrix=MS1_final, **metadata)\n",
    "        print(f\"Saved MS1: {ms1_path}  shape={MS1_final.shape}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return {\"ms1\": ms1_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: convenience wrapper\n",
    "# -----------------------------\n",
    "def wholeCasting_npz_split(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Convenience: run Pass 1 then Pass 2.\n",
    "    Returns dict with paths.\n",
    "    \"\"\"\n",
    "    paths1 = build_ms2_and_meta(folder_paths, out_path)\n",
    "    paths2 = build_ms1_only(folder_paths, out_path, meta_path=paths1[\"meta\"])\n",
    "    return {\"ms2\": paths1[\"ms2\"], \"ms1\": paths2[\"ms1\"], \"meta\": paths1[\"meta\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6d44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import tempfile\n",
    "\n",
    "# If you have fisher_py installed, uncomment:\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "# -----------------------------\n",
    "# Shared config / helpers\n",
    "# -----------------------------\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _group_from_name(name: str) -> str:\n",
    "    for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "        if g in name:\n",
    "            return g\n",
    "    return \"Unknown\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _ensure_folder_list(paths):\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        return list(paths)\n",
    "    return [paths]\n",
    "\n",
    "def _gather_raw_files(folder_paths):\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\n",
    "            f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}'\n",
    "        )\n",
    "    return raw_files\n",
    "\n",
    "def _base_paths(out_path: str):\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "    return (f\"{base}.ms2.npz\", f\"{base}.ms1.npz\", f\"{base}.meta.npz\")\n",
    "\n",
    "def _sanitize_metadata_dict(md: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Ensure metadata contains only numeric arrays or Unicode string arrays.\n",
    "    This prevents NumPy from needing pickle to save/load.\n",
    "    \"\"\"\n",
    "    safe = {}\n",
    "    for k, v in md.items():\n",
    "        if isinstance(v, (int, float, np.number, np.bool_)):\n",
    "            safe[k] = np.array(v)\n",
    "            continue\n",
    "        # Arrays/lists/tuples\n",
    "        if isinstance(v, (list, tuple, np.ndarray)):\n",
    "            arr = np.asarray(v)\n",
    "            # If object dtype, try numeric then fall back to Unicode\n",
    "            if arr.dtype == object:\n",
    "                try:\n",
    "                    arr = arr.astype(np.float32)\n",
    "                except Exception:\n",
    "                    arr = arr.astype(\"U\")\n",
    "            # Force strings to be Unicode, not object\n",
    "            if np.issubdtype(arr.dtype, np.character):\n",
    "                arr = arr.astype(\"U\")\n",
    "            safe[k] = arr\n",
    "            continue\n",
    "        # Strings -> Unicode array (scalar ok)\n",
    "        if isinstance(v, str):\n",
    "            safe[k] = np.array(v, dtype=\"U\")\n",
    "            continue\n",
    "        # Fallback: stringify scalars/objects to Unicode (rare)\n",
    "        safe[k] = np.array(str(v), dtype=\"U\")\n",
    "    return safe\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 1: MS2 + Metadata\n",
    "# -----------------------------\n",
    "def build_ms2_and_meta(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Build MS2 (per-scan normalized) and shared metadata, saving:\n",
    "      <base>.ms2.npz  -> ms2_matrix (float16) + metadata\n",
    "      <base>.meta.npz -> metadata only\n",
    "    \"\"\"\n",
    "    # Guard: require fisher_py (uncomment import above if installed)\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError as _:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_basenames, file_abspaths, group_names = [], [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "\n",
    "        f_id = file_to_id[raw_abs]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass1-MS2] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # Just metadata for MS1 in this pass\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # precursor m/z extraction\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Build metadata (shared by both MS1 and MS2 output files)\n",
    "    metadata_raw = dict(\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "        ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "        ms2_group_id=np.asarray(ms2_group_id, dtype=np.int32),\n",
    "\n",
    "        # ensure Unicode (not object)\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "        group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "    )\n",
    "    metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    ms2_path, _, meta_path = _base_paths(out_path)\n",
    "\n",
    "    # Save MS2\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"Saved MS2: {ms2_path}  shape={MS2.shape}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Save metadata standalone\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"Saved META: {meta_path}\")\n",
    "\n",
    "    return {\"ms2\": ms2_path, \"meta\": meta_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 2: MS1 only (streamed, low-RAM)\n",
    "# -----------------------------\n",
    "def build_ms1_only(folder_paths, out_path: str, meta_path: str = None):\n",
    "    \"\"\"\n",
    "    Build MS1 matrix in a streamed way (low RAM) and save:\n",
    "      <base>.ms1.npz -> ms1_matrix (float16) + metadata\n",
    "\n",
    "    MS1 normalization: per-column (bin-wise) max across *all* MS1 scans.\n",
    "\n",
    "    Two sweeps:\n",
    "      1) Count MS1 rows + compute col_max (no storage)\n",
    "      2) Allocate headered .npy memmap [n_rows, MS1_LEN], fill normalized rows, save compressed NPZ.\n",
    "\n",
    "    If meta_path is provided, that metadata is embedded into the MS1 NPZ; otherwise it is\n",
    "    rebuilt on the fly (slower) and saved alongside.\n",
    "    \"\"\"\n",
    "    # Guard: require fisher_py (uncomment import above if installed)\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError as _:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    from numpy.lib.format import open_memmap  # headered .npy memmap\n",
    "\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "    ms1_count = 0\n",
    "    col_max = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "    # Load metadata from Pass 1 if available\n",
    "    metadata = None\n",
    "    if meta_path is not None and os.path.exists(meta_path):\n",
    "        with np.load(meta_path, allow_pickle=False) as meta_npz:\n",
    "            metadata = {k: meta_npz[k] for k in meta_npz.files}\n",
    "        # Just in case an older meta had object dtypes:\n",
    "        metadata = _sanitize_metadata_dict(metadata)\n",
    "\n",
    "    rebuild_metadata = metadata is None\n",
    "\n",
    "    if rebuild_metadata:\n",
    "        file_basenames, file_abspaths, group_names = [], [], []\n",
    "        file_to_id, group_to_id = {}, {}\n",
    "        ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    # -------- Pass 2a: count & col_max --------\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if rebuild_metadata:\n",
    "            if raw_abs not in file_to_id:\n",
    "                file_to_id[raw_abs] = len(file_basenames)\n",
    "                file_basenames.append(raw_name)\n",
    "                file_abspaths.append(raw_abs)\n",
    "\n",
    "            group = _group_from_name(raw_name)\n",
    "            if group not in group_to_id:\n",
    "                group_to_id[group] = len(group_names)\n",
    "                group_names.append(group)\n",
    "\n",
    "            f_id = file_to_id[raw_abs]\n",
    "            g_id = group_to_id[group]\n",
    "        else:\n",
    "            f_id = g_id = None  # unused\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2a-MS1 count] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            if stype != \"ms\":\n",
    "                continue\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "            mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            # Update col_max without storing the full row\n",
    "            vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "            np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "            np.maximum(col_max, vec, out=col_max)\n",
    "            ms1_count += 1\n",
    "\n",
    "            # (Re)build minimal metadata for MS1 if needed\n",
    "            if rebuild_metadata:\n",
    "                sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "                try:\n",
    "                    rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "                except Exception:\n",
    "                    rt = np.nan\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Guard: no MS1 scans\n",
    "    ms1_path = _base_paths(out_path)[1]\n",
    "    if ms1_count == 0:\n",
    "        if metadata is None and rebuild_metadata:\n",
    "            metadata = dict(\n",
    "                ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "                ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "                ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "                ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "                file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "                file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "                group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "            )\n",
    "        elif metadata is None:\n",
    "            metadata = {}\n",
    "        metadata = _sanitize_metadata_dict(metadata)\n",
    "        empty = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "        np.savez_compressed(ms1_path, ms1_matrix=empty, **metadata)\n",
    "        print(f\"Saved MS1 (empty): {ms1_path}\")\n",
    "        return {\"ms1\": ms1_path}\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    col_max[col_max == 0] = 1.0\n",
    "\n",
    "    # If we had to rebuild metadata, finalize it now. Otherwise we already have it.\n",
    "    if rebuild_metadata:\n",
    "        metadata_raw = dict(\n",
    "            ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "            ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "            ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "            ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "            file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "            file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "            group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "        )\n",
    "        metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    # -------- Pass 2b: fill normalized rows into a headered .npy memmap --------\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        memmap_path = os.path.join(tmpdir, \"ms1_memmap.npy\")\n",
    "\n",
    "        # Use headered .npy memmap to allow np.load(..., allow_pickle=False)\n",
    "        from numpy.lib.format import open_memmap\n",
    "        MS1_map = open_memmap(memmap_path, mode=\"w+\", dtype=np.float16, shape=(ms1_count, MS1_LEN))\n",
    "\n",
    "        row = 0\n",
    "        for raw_abs in raw_files:\n",
    "            raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "            try:\n",
    "                raw = RawFile(raw_abs)\n",
    "            except Exception as e:\n",
    "                print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "                continue\n",
    "\n",
    "            total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "            for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2b-MS1 write] {raw_name}\", ncols=100):\n",
    "                if row >= ms1_count:\n",
    "                    break  # safety\n",
    "\n",
    "                try:\n",
    "                    raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                stype = _scan_type_label(raw_scan.scan_type)\n",
    "                if stype != \"ms\":\n",
    "                    continue\n",
    "\n",
    "                masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "                intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "                if masses.size == 0 or intens.size == 0:\n",
    "                    continue\n",
    "\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "\n",
    "                v32 /= col_max  # per-column normalization from Pass 2a\n",
    "                MS1_map[row, :] = v32.astype(np.float16, copy=False)\n",
    "                row += 1\n",
    "\n",
    "            try:\n",
    "                raw.dispose()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Flush memmap to disk by dropping reference\n",
    "        del MS1_map\n",
    "\n",
    "        # Load the headered .npy without pickle and write compressed NPZ with metadata\n",
    "        MS1_final = np.load(memmap_path, mmap_mode=\"r\", allow_pickle=False)\n",
    "        np.savez_compressed(ms1_path, ms1_matrix=MS1_final, **metadata)\n",
    "        print(f\"Saved MS1: {ms1_path}  shape={MS1_final.shape}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return {\"ms1\": ms1_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: convenience wrapper\n",
    "# -----------------------------\n",
    "def wholeCasting_npz_split(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Convenience: run Pass 1 then Pass 2.\n",
    "    Returns dict with paths.\n",
    "    \"\"\"\n",
    "    paths1 = build_ms2_and_meta(folder_paths, out_path)\n",
    "    paths2 = build_ms1_only(folder_paths, out_path, meta_path=paths1[\"meta\"])\n",
    "    return {\"ms2\": paths1[\"ms2\"], \"ms1\": paths2[\"ms1\"], \"meta\": paths1[\"meta\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f1c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# If you have fisher_py installed, uncomment:\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "# -----------------------------\n",
    "# Shared config / helpers\n",
    "# -----------------------------\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _group_from_name(name: str) -> str:\n",
    "    for g in (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\"):\n",
    "        if g in name:\n",
    "            return g\n",
    "    return \"Unknown\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _ensure_folder_list(paths):\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        return list(paths)\n",
    "    return [paths]\n",
    "\n",
    "def _gather_raw_files(folder_paths):\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\n",
    "            f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}'\n",
    "        )\n",
    "    return raw_files\n",
    "\n",
    "def _base_paths(out_path: str):\n",
    "    base = os.path.abspath(out_path)\n",
    "    if base.lower().endswith(\".npz\"):\n",
    "        base = base[:-4]\n",
    "    return (f\"{base}.ms2.npz\", f\"{base}.ms1.npz\", f\"{base}.meta.npz\")\n",
    "\n",
    "def _sanitize_metadata_dict(md: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Keep only numeric arrays or Unicode string arrays (no object dtype),\n",
    "    so NumPy never needs pickle to save/load.\n",
    "    \"\"\"\n",
    "    safe = {}\n",
    "    for k, v in md.items():\n",
    "        if isinstance(v, (int, float, np.number, np.bool_)):\n",
    "            safe[k] = np.array(v)\n",
    "            continue\n",
    "        if isinstance(v, (list, tuple, np.ndarray)):\n",
    "            arr = np.asarray(v)\n",
    "            if arr.dtype == object:\n",
    "                try:\n",
    "                    arr = arr.astype(np.float32)\n",
    "                except Exception:\n",
    "                    arr = arr.astype(\"U\")\n",
    "            if np.issubdtype(arr.dtype, np.character):\n",
    "                arr = arr.astype(\"U\")\n",
    "            safe[k] = arr\n",
    "            continue\n",
    "        if isinstance(v, str):\n",
    "            safe[k] = np.array(v, dtype=\"U\")\n",
    "            continue\n",
    "        safe[k] = np.array(str(v), dtype=\"U\")\n",
    "    return safe\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 1: MS2 + Metadata\n",
    "# -----------------------------\n",
    "def build_ms2_and_meta(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Build MS2 (per-scan normalized) and shared metadata, saving:\n",
    "      <base>.ms2.npz  -> ms2_matrix (float16) + metadata\n",
    "      <base>.meta.npz -> metadata only\n",
    "    \"\"\"\n",
    "    # Guard: require fisher_py (uncomment import above if installed)\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError as _:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "\n",
    "    ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    ms2_rows = []\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id, ms2_group_id = [], [], [], [], []\n",
    "\n",
    "    file_basenames, file_abspaths, group_names = [], [], []\n",
    "    file_to_id, group_to_id = {}, {}\n",
    "\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "\n",
    "        group = _group_from_name(raw_name)\n",
    "        if group not in group_to_id:\n",
    "            group_to_id[group] = len(group_names)\n",
    "            group_names.append(group)\n",
    "\n",
    "        f_id = file_to_id[raw_abs]\n",
    "        g_id = group_to_id[group]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass1-MS2] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # MS1 metadata only in this pass\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # precursor m/z extraction\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "                ms2_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Build metadata (shared by both MS1 and MS2 output files)\n",
    "    metadata_raw = dict(\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "        ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "        ms2_group_id=np.asarray(ms2_group_id, dtype=np.int32),\n",
    "\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "        group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "    )\n",
    "    metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    ms2_path, _, meta_path = _base_paths(out_path)\n",
    "\n",
    "    # Save MS2\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"Saved MS2: {ms2_path}  shape={MS2.shape}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Save metadata standalone\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"Saved META: {meta_path}\")\n",
    "\n",
    "    return {\"ms2\": ms2_path, \"meta\": meta_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pass 2: MS1 only (streamed, low-RAM)\n",
    "# -----------------------------\n",
    "def build_ms1_only(folder_paths, out_path: str, meta_path: str = None):\n",
    "    \"\"\"\n",
    "    Build MS1 matrix in a streamed way (low RAM) and save:\n",
    "      <base>.ms1.npz -> ms1_matrix (float16) + metadata\n",
    "\n",
    "    MS1 normalization: per-column (bin-wise) max across *all* MS1 scans.\n",
    "\n",
    "    Two sweeps:\n",
    "      1) Count MS1 rows + compute col_max (no storage)\n",
    "      2) Allocate headered .npy memmap [n_rows, MS1_LEN], fill normalized rows, save compressed NPZ.\n",
    "\n",
    "    If meta_path is provided, that metadata is embedded into the MS1 NPZ; otherwise it is\n",
    "    rebuilt on the fly (slower) and saved alongside.\n",
    "    \"\"\"\n",
    "    # Guard: require fisher_py (uncomment import above if installed)\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError as _:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    from numpy.lib.format import open_memmap  # headered .npy memmap\n",
    "\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "    ms1_count = 0\n",
    "    col_max = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "    # Load metadata from Pass 1 if available\n",
    "    metadata = None\n",
    "    if meta_path is not None and os.path.exists(meta_path):\n",
    "        with np.load(meta_path, allow_pickle=False) as meta_npz:\n",
    "            metadata = {k: meta_npz[k] for k in meta_npz.files}\n",
    "        metadata = _sanitize_metadata_dict(metadata)\n",
    "\n",
    "    rebuild_metadata = metadata is None\n",
    "\n",
    "    if rebuild_metadata:\n",
    "        file_basenames, file_abspaths, group_names = [], [], []\n",
    "        file_to_id, group_to_id = {}, {}\n",
    "        ms1_scan, ms1_rt, ms1_file_id, ms1_group_id = [], [], [], []\n",
    "\n",
    "    # -------- Pass 2a: count & col_max --------\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        if rebuild_metadata:\n",
    "            if raw_abs not in file_to_id:\n",
    "                file_to_id[raw_abs] = len(file_basenames)\n",
    "                file_basenames.append(raw_name)\n",
    "                file_abspaths.append(raw_abs)\n",
    "\n",
    "            group = _group_from_name(raw_name)\n",
    "            if group not in group_to_id:\n",
    "                group_to_id[group] = len(group_names)\n",
    "                group_names.append(group)\n",
    "\n",
    "            f_id = file_to_id[raw_abs]\n",
    "            g_id = group_to_id[group]\n",
    "        else:\n",
    "            f_id = g_id = None  # unused\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2a-MS1 count] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            if stype != \"ms\":\n",
    "                continue\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "            mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            vec = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "            np.add.at(vec, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "            np.maximum(col_max, vec, out=col_max)\n",
    "            ms1_count += 1\n",
    "\n",
    "            if rebuild_metadata:\n",
    "                sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "                try:\n",
    "                    rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "                except Exception:\n",
    "                    rt = np.nan\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "                ms1_group_id.append(g_id)\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Guard: no MS1 scans\n",
    "    ms1_path = _base_paths(out_path)[1]\n",
    "    if ms1_count == 0:\n",
    "        if metadata is None and rebuild_metadata:\n",
    "            metadata = dict(\n",
    "                ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "                ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "                ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "                ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "                file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "                file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "                group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "            )\n",
    "        elif metadata is None:\n",
    "            metadata = {}\n",
    "        metadata = _sanitize_metadata_dict(metadata)\n",
    "        empty = np.zeros((0, MS1_LEN), dtype=np.float16)\n",
    "        np.savez_compressed(ms1_path, ms1_matrix=empty, **metadata)\n",
    "        print(f\"Saved MS1 (empty): {ms1_path}\")\n",
    "        return {\"ms1\": ms1_path}\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    col_max[col_max == 0] = 1.0\n",
    "\n",
    "    # Finalize metadata if rebuilt\n",
    "    if rebuild_metadata:\n",
    "        metadata_raw = dict(\n",
    "            ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "            ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "            ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "            ms1_group_id=np.asarray(ms1_group_id, dtype=np.int32),\n",
    "            file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "            file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "            group_names_lookup=np.asarray(group_names, dtype=\"U\"),\n",
    "        )\n",
    "        metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    # -------- Pass 2b: fill normalized rows into a headered .npy memmap --------\n",
    "    # Write the temp memmap next to the final outputs to avoid Windows handle issues.\n",
    "    base_dir = os.path.dirname(os.path.abspath(ms1_path)) or \".\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    tmp_memmap_path = os.path.join(base_dir, os.path.basename(ms1_path).replace(\".ms1.npz\", \".ms1.tmp.npy\"))\n",
    "\n",
    "    from numpy.lib.format import open_memmap\n",
    "    MS1_map = open_memmap(tmp_memmap_path, mode=\"w+\", dtype=np.float16, shape=(ms1_count, MS1_LEN))\n",
    "\n",
    "    row = 0\n",
    "    for raw_abs in raw_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[Pass2b-MS1 write] {raw_name}\", ncols=100):\n",
    "            if row >= ms1_count:\n",
    "                break  # safety\n",
    "\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            if stype != \"ms\":\n",
    "                continue\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "            mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            v32 = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "            np.add.at(v32, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "            v32 /= col_max  # per-column normalization\n",
    "            MS1_map[row, :] = v32.astype(np.float16, copy=False)\n",
    "            row += 1\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- CRITICAL ON WINDOWS ---\n",
    "    # Flush and drop all references to the memmap BEFORE we read/delete it.\n",
    "    try:\n",
    "        MS1_map.flush()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del MS1_map\n",
    "    gc.collect()\n",
    "\n",
    "    # Load into RAM (no mmap) so no handle remains on the file; then save compressed NPZ.\n",
    "    MS1_final = np.load(tmp_memmap_path, mmap_mode=None, allow_pickle=False)\n",
    "    np.savez_compressed(ms1_path, ms1_matrix=MS1_final, **metadata)\n",
    "    print(f\"Saved MS1: {ms1_path}  shape={MS1_final.shape}\")\n",
    "\n",
    "    # Now it is safe to remove the temp .npy (all references gone).\n",
    "    del MS1_final\n",
    "    gc.collect()\n",
    "    try:\n",
    "        os.remove(tmp_memmap_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not remove temp memmap ({tmp_memmap_path}): {e}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return {\"ms1\": ms1_path}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: convenience wrapper\n",
    "# -----------------------------\n",
    "def wholeCasting_npz_split(folder_paths, out_path: str):\n",
    "    \"\"\"\n",
    "    Convenience: run Pass 1 then Pass 2.\n",
    "    Returns dict with paths.\n",
    "    \"\"\"\n",
    "    paths1 = build_ms2_and_meta(folder_paths, out_path)\n",
    "    paths2 = build_ms1_only(folder_paths, out_path, meta_path=paths1[\"meta\"])\n",
    "    return {\"ms2\": paths1[\"ms2\"], \"ms1\": paths2[\"ms1\"], \"meta\": paths1[\"meta\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting_npz_split([\"D:/TreatmentABC\", \"D:/TreatmentD\" ], out_path=\"D:/casts/databank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122a2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# If you have fisher_py installed, UNCOMMENT these:\n",
    "# from fisher_py.raw_file import RawFile\n",
    "# from fisher_py.scan import Scan\n",
    "\n",
    "# -----------------------------\n",
    "# Config / binning\n",
    "# -----------------------------\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9 (10 pts per m/z)\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999 (1 pt per m/z)\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "GROUPS = (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _group_from_name(name: str) -> str:\n",
    "    for g in GROUPS:\n",
    "        if g in name:\n",
    "            return g\n",
    "    return \"Unknown\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _ensure_folder_list(paths):\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        return list(paths)\n",
    "    return [paths]\n",
    "\n",
    "def _gather_raw_files(folder_paths):\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\n",
    "            f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}'\n",
    "        )\n",
    "    return raw_files\n",
    "\n",
    "def _sanitize_metadata_dict(md: dict) -> dict:\n",
    "    \"\"\"Ensure arrays are numeric or Unicode (never object dtype).\"\"\"\n",
    "    safe = {}\n",
    "    for k, v in md.items():\n",
    "        if isinstance(v, (int, float, np.number, np.bool_)):\n",
    "            safe[k] = np.array(v)\n",
    "            continue\n",
    "        if isinstance(v, (list, tuple, np.ndarray)):\n",
    "            arr = np.asarray(v)\n",
    "            if arr.dtype == object:\n",
    "                try:\n",
    "                    arr = arr.astype(np.float32)\n",
    "                except Exception:\n",
    "                    arr = arr.astype(\"U\")\n",
    "            if np.issubdtype(arr.dtype, np.character):\n",
    "                arr = arr.astype(\"U\")\n",
    "            safe[k] = arr\n",
    "            continue\n",
    "        if isinstance(v, str):\n",
    "            safe[k] = np.array(v, dtype=\"U\")\n",
    "            continue\n",
    "        safe[k] = np.array(str(v), dtype=\"U\")\n",
    "    return safe\n",
    "\n",
    "def _out_paths(out_dir: str, group: str):\n",
    "    base = os.path.join(os.path.abspath(out_dir), group)\n",
    "    return (f\"{base}.ms1.npz\", f\"{base}.ms2.npz\", f\"{base}.meta.npz\")\n",
    "\n",
    "# -----------------------------\n",
    "# Core: process one treatment group at a time\n",
    "# -----------------------------\n",
    "def _process_group(group: str, group_files: list, out_dir: str):\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "      - MS1 (float32, UNnormalized) stacked per MS1 scan for this group\n",
    "      - MS2 (float16, per-scan normalized) stacked per MS2 scan for this group\n",
    "      - METADATA aligned to the two matrices\n",
    "    Saves three NPZ files and frees RAM.\n",
    "    \"\"\"\n",
    "    if not group_files:\n",
    "        return None\n",
    "\n",
    "    # Guard: require fisher_py\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ms1_path, ms2_path, meta_path = _out_paths(out_dir, group)\n",
    "\n",
    "    # Per-group accumulators\n",
    "    file_basenames, file_abspaths = [], []\n",
    "    file_to_id = {}\n",
    "\n",
    "    # MS1\n",
    "    ms1_rows = []                               # list of vectors (float32)\n",
    "    ms1_scan, ms1_rt, ms1_file_id = [], [], []  # aligned to ms1_rows\n",
    "\n",
    "    # MS2\n",
    "    ms2_rows = []                               # list of vectors (float16)\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id = [], [], [], []\n",
    "\n",
    "    # Iterate files in this group\n",
    "    for raw_abs in group_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "        f_id = file_to_id[raw_abs]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[{group}] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # Build UNnormalized float32 MS1 row\n",
    "                # Bin at 0.1 m/z: index = round(m/z*10)\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v32 = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                ms1_rows.append(v32)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                # Build per-scan normalized MS2 row (float16 for compact size)\n",
    "                # Bin at 1.0 m/z: index = round(m/z)\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # Precursor m/z (fallback to parsing scan_type text)\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "\n",
    "        # dispose RAW handle\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Build metadata (per-group) ----\n",
    "    # Note: IDs are per-group (0..n_files_in_group-1)\n",
    "    metadata_raw = dict(\n",
    "        group_name=np.array(group, dtype=\"U\"),\n",
    "\n",
    "        # MS1 row-aligned meta\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "\n",
    "        # MS2 row-aligned meta\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "\n",
    "        # Lookups\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "    )\n",
    "    metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    # ---- Stack & save (release RAM right after) ----\n",
    "    # MS1 (float32, UNnormalized)\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float32)\n",
    "    np.savez_compressed(ms1_path, ms1_matrix=MS1, **metadata)\n",
    "    print(f\"[{group}] Saved MS1: {ms1_path}  shape={MS1.shape}, dtype={MS1.dtype}\")\n",
    "    del MS1, ms1_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # MS2 (float16, normalized per scan)\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"[{group}] Saved MS2: {ms2_path}  shape={MS2.shape}, dtype={MS2.dtype}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Save metadata standalone (useful if you want to load meta without matrices)\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"[{group}] Saved META: {meta_path}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    del metadata, metadata_raw\n",
    "    gc.collect()\n",
    "\n",
    "    return {\"group\": group, \"ms1\": ms1_path, \"ms2\": ms2_path, \"meta\": meta_path}\n",
    "\n",
    "# -----------------------------\n",
    "# Public API\n",
    "# -----------------------------\n",
    "def wholeCasting_per_group(folder_paths, out_dir: str):\n",
    "    \"\"\"\n",
    "    Scans RAW files, partitions by TreatmentA/B/C/D (using filename contains),\n",
    "    and for each group writes:\n",
    "      <out_dir>/<Group>.ms1.npz  (float32, UNnormalized)\n",
    "      <out_dir>/<Group>.ms2.npz  (float16, per-scan normalized)\n",
    "      <out_dir>/<Group>.meta.npz\n",
    "\n",
    "    RAM is freed between groups.\n",
    "    Returns a dict of outputs keyed by group.\n",
    "    \"\"\"\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "    by_group = {g: [] for g in GROUPS}\n",
    "    for p in raw_files:\n",
    "        g = _group_from_name(os.path.basename(p))\n",
    "        if g in by_group:\n",
    "            by_group[g].append(p)\n",
    "\n",
    "    outputs = {}\n",
    "    for g in GROUPS:\n",
    "        paths = _process_group(g, by_group[g], out_dir)\n",
    "        outputs[g] = paths\n",
    "        # safety: ensure memory is really freed between groups\n",
    "        gc.collect()\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b40e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentA] 20220315_chm134_Cirrhosis_FlowChip15_AA13001EM1_TreatmentA_biorep01_techrep01.raw: 100%\n",
      "[TreatmentA] 20220315_chm134_Cirrhosis_FlowChip15_AA13001EM1_TreatmentA_biorep01_techrep02.raw: 100%\n",
      "[TreatmentA] 20220315_chm134_Cirrhosis_FlowChip15_AA13001EM1_TreatmentA_biorep01_techrep03.raw: 100%\n",
      "[TreatmentA] 20220317_chm134_Cirrhosis_FlowChip15_AA26021EM1_TreatmentA_biorep21_techrep01.raw: 100%\n",
      "[TreatmentA] 20220317_chm134_Cirrhosis_FlowChip15_AA26021EM1_TreatmentA_biorep21_techrep02.raw: 100%\n",
      "[TreatmentA] 20220317_chm134_Cirrhosis_FlowChip15_AA26021EM1_TreatmentA_biorep21_techrep03.raw: 100%\n",
      "[TreatmentA] 20220320_chm134_Cirrhosis_FlowChip15_AA18009EM1_TreatmentA_biorep09_techrep01.raw: 100%\n",
      "[TreatmentA] 20220320_chm134_Cirrhosis_FlowChip15_AA18009EM1_TreatmentA_biorep09_techrep02.raw: 100%\n",
      "[TreatmentA] 20220320_chm134_Cirrhosis_FlowChip15_AA18009EM1_TreatmentA_biorep09_techrep03.raw: 100%\n",
      "[TreatmentA] 20220322_chm134_Cirrhosis_FlowChip15_AA18011EM1_TreatmentA_biorep11_techrep01.raw: 100%\n",
      "[TreatmentA] 20220322_chm134_Cirrhosis_FlowChip15_AA18011EM1_TreatmentA_biorep11_techrep02.raw: 100%\n",
      "[TreatmentA] 20220322_chm134_Cirrhosis_FlowChip15_AA18011EM1_TreatmentA_biorep11_techrep03.raw: 100%\n",
      "[TreatmentA] 20220403_chm134_Cirrhosis_FlowChip15_AA26017EM1_TreatmentA_biorep17_techrep01.raw: 100%\n",
      "[TreatmentA] 20220403_chm134_Cirrhosis_FlowChip15_AA26017EM1_TreatmentA_biorep17_techrep02.raw: 100%\n",
      "[TreatmentA] 20220403_chm134_Cirrhosis_FlowChip15_AA26017EM1_TreatmentA_biorep17_techrep03.raw: 100%\n",
      "[TreatmentA] 20221103_chm134_Cirrhosis_FlowChip15_AA15004EM1_TreatmentA_biorep04_techrep01.raw: 100%\n",
      "[TreatmentA] 20221103_chm134_Cirrhosis_FlowChip15_AA15004EM1_TreatmentA_biorep04_techrep02.raw: 100%\n",
      "[TreatmentA] 20221103_chm134_Cirrhosis_FlowChip15_AA15004EM1_TreatmentA_biorep04_techrep03.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AA15008EM1_TreatmentA_biorep08_techrep01.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AA15008EM1_TreatmentA_biorep08_techrep02.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AA15008EM1_TreatmentA_biorep08_techrep03.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AY13033EM1_TreatmentA_biorep33_techrep01.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AY13033EM1_TreatmentA_biorep33_techrep02.raw: 100%\n",
      "[TreatmentA] 20221105_chm134_Cirrhosis_FlowChip15_AY13033EM1_TreatmentA_biorep33_techrep03.raw: 100%\n",
      "[TreatmentA] 20221108_chm134_Cirrhosis_FlowChip15_AA18010EM1_TreatmentA_biorep10_techrep01.raw: 100%\n",
      "[TreatmentA] 20221108_chm134_Cirrhosis_FlowChip15_AA18010EM1_TreatmentA_biorep10_techrep02.raw: 100%\n",
      "[TreatmentA] 20221108_chm134_Cirrhosis_FlowChip15_AA18010EM1_TreatmentA_biorep10_techrep03.raw: 100%\n",
      "[TreatmentA] 20221111_chm134_Cirrhosis_FlowChip15_AA15003EM1_TreatmentA_biorep03_techrep01.raw: 100%\n",
      "[TreatmentA] 20221111_chm134_Cirrhosis_FlowChip15_AA15003EM1_TreatmentA_biorep03_techrep02.raw: 100%\n",
      "[TreatmentA] 20221111_chm134_Cirrhosis_FlowChip15_AA15003EM1_TreatmentA_biorep03_techrep03.raw: 100%\n",
      "[TreatmentA] 20230403_chm134_Cirrhosis_FlowChip15_AA26016EM1_TreatmentA_biorep16_techrep01.raw: 100%\n",
      "[TreatmentA] 20230403_chm134_Cirrhosis_FlowChip15_AA26016EM1_TreatmentA_biorep16_techrep02.raw: 100%\n",
      "[TreatmentA] 20230403_chm134_Cirrhosis_FlowChip15_AA26016EM1_TreatmentA_biorep16_techrep03.raw: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TreatmentA] Saved MS1: D:\\casts\\databank\\TreatmentA.ms1.npz  shape=(147057, 13690), dtype=float32\n",
      "[TreatmentA] Saved MS2: D:\\casts\\databank\\TreatmentA.ms2.npz  shape=(336425, 1600), dtype=float16\n",
      "[TreatmentA] Saved META: D:\\casts\\databank\\TreatmentA.meta.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentB] 20220315_chm134_Cirrhosis_FlowChip15_AA26018EM1_TreatmentB_biorep18_techrep01.raw: 100%\n",
      "[TreatmentB] 20220315_chm134_Cirrhosis_FlowChip15_AA26018EM1_TreatmentB_biorep18_techrep02.raw: 100%\n",
      "[TreatmentB] 20220315_chm134_Cirrhosis_FlowChip15_AA26018EM1_TreatmentB_biorep18_techrep03.raw: 100%\n",
      "[TreatmentB] 20220317_chm134_Cirrhosis_FlowChip15_AY6027EM1_TreatmentB_biorep27_techrep01.raw: 100%|\n",
      "[TreatmentB] 20220317_chm134_Cirrhosis_FlowChip15_AY6027EM1_TreatmentB_biorep27_techrep02.raw: 100%|\n",
      "[TreatmentB] 20220317_chm134_Cirrhosis_FlowChip15_AY6027EM1_TreatmentB_biorep27_techrep03.raw: 100%|\n",
      "[TreatmentB] 20220320_chm134_Cirrhosis_FlowChip15_AY2023EM1_TreatmentB_biorep23_techrep01.raw: 100%|\n",
      "[TreatmentB] 20220320_chm134_Cirrhosis_FlowChip15_AY2023EM1_TreatmentB_biorep23_techrep02.raw: 100%|\n",
      "[TreatmentB] 20220320_chm134_Cirrhosis_FlowChip15_AY2023EM1_TreatmentB_biorep23_techrep03.raw: 100%|\n",
      "[TreatmentB] 20220322_chm134_Cirrhosis_FlowChip15_AY6029EM1_TreatmentB_biorep29_techrep01.raw: 100%|\n",
      "[TreatmentB] 20220322_chm134_Cirrhosis_FlowChip15_AY6029EM1_TreatmentB_biorep29_techrep02.raw: 100%|\n",
      "[TreatmentB] 20220322_chm134_Cirrhosis_FlowChip15_AY6029EM1_TreatmentB_biorep29_techrep03.raw: 100%|\n",
      "[TreatmentB] 20220329_chm134_Cirrhosis_FlowChip15_AY6026EM1_TreatmentB_biorep26_techrep01.raw: 100%|\n",
      "[TreatmentB] 20220329_chm134_Cirrhosis_FlowChip15_AY6026EM1_TreatmentB_biorep26_techrep02.raw: 100%|\n",
      "[TreatmentB] 20220329_chm134_Cirrhosis_FlowChip15_AY6026EM1_TreatmentB_biorep26_techrep03.raw: 100%|\n",
      "[TreatmentB] 20220330_chm134_Cirrhosis_FlowChip15_AY12031EM1_TreatmentB_biorep31_techrep01.raw: 100%\n",
      "[TreatmentB] 20220330_chm134_Cirrhosis_FlowChip15_AY12031EM1_TreatmentB_biorep31_techrep02.raw: 100%\n",
      "[TreatmentB] 20220330_chm134_Cirrhosis_FlowChip15_AY12031EM1_TreatmentB_biorep31_techrep03.raw: 100%\n",
      "[TreatmentB] 20221103_chm134_Cirrhosis_FlowChip15_AY2024EM1_TreatmentB_biorep24_techrep01.raw: 100%|\n",
      "[TreatmentB] 20221103_chm134_Cirrhosis_FlowChip15_AY2024EM1_TreatmentB_biorep24_techrep02.raw: 100%|\n",
      "[TreatmentB] 20221103_chm134_Cirrhosis_FlowChip15_AY2024EM1_TreatmentB_biorep24_techrep03.raw: 100%|\n",
      "[TreatmentB] 20221105_chm134_Cirrhosis_FlowChip15_AA14002EM1_TreatmentB_biorep02_techrep01.raw: 100%\n",
      "[TreatmentB] 20221105_chm134_Cirrhosis_FlowChip15_AA14002EM1_TreatmentB_biorep02_techrep02.raw: 100%\n",
      "[TreatmentB] 20221105_chm134_Cirrhosis_FlowChip15_AA14002EM1_TreatmentB_biorep02_techrep03.raw: 100%\n",
      "[TreatmentB] 20221108_chm134_Cirrhosis_FlowChip15_AA2022EM1_TreatmentB_biorep22_techrep01.raw: 100%|\n",
      "[TreatmentB] 20221108_chm134_Cirrhosis_FlowChip15_AA2022EM1_TreatmentB_biorep22_techrep02.raw: 100%|\n",
      "[TreatmentB] 20221108_chm134_Cirrhosis_FlowChip15_AA2022EM1_TreatmentB_biorep22_techrep03.raw: 100%|\n",
      "[TreatmentB] 20221111_chm134_Cirrhosis_FlowChip15_AA15007EM1_TreatmentB_biorep07_techrep01.raw: 100%\n",
      "[TreatmentB] 20221111_chm134_Cirrhosis_FlowChip15_AA15007EM1_TreatmentB_biorep07_techrep02.raw: 100%\n",
      "[TreatmentB] 20221111_chm134_Cirrhosis_FlowChip15_AA15007EM1_TreatmentB_biorep07_techrep03.raw: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TreatmentB] Saved MS1: D:\\casts\\databank\\TreatmentB.ms1.npz  shape=(130974, 13690), dtype=float32\n",
      "[TreatmentB] Saved MS2: D:\\casts\\databank\\TreatmentB.ms2.npz  shape=(327286, 1600), dtype=float16\n",
      "[TreatmentB] Saved META: D:\\casts\\databank\\TreatmentB.meta.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentC] 20220315_chm134_Cirrhosis_FlowChip15_AA21013EM1_TreatmentC_biorep13_techrep01.raw: 100%\n",
      "[TreatmentC] 20220315_chm134_Cirrhosis_FlowChip15_AA21013EM1_TreatmentC_biorep13_techrep02.raw: 100%\n",
      "[TreatmentC] 20220315_chm134_Cirrhosis_FlowChip15_AA21013EM1_TreatmentC_biorep13_techrep03.raw: 100%\n",
      "[TreatmentC] 20220317_chm134_Cirrhosis_FlowChip15_AY6028EM1_TreatmentC_biorep28_techrep01.raw: 100%|\n",
      "[TreatmentC] 20220317_chm134_Cirrhosis_FlowChip15_AY6028EM1_TreatmentC_biorep28_techrep02.raw: 100%|\n",
      "[TreatmentC] 20220317_chm134_Cirrhosis_FlowChip15_AY6028EM1_TreatmentC_biorep28_techrep03.raw: 100%|\n",
      "[TreatmentC] 20220320_chm134_Cirrhosis_FlowChip15_AA21014EM1_TreatmentC_biorep14_techrep01.raw: 100%\n",
      "[TreatmentC] 20220320_chm134_Cirrhosis_FlowChip15_AA21014EM1_TreatmentC_biorep14_techrep02.raw: 100%\n",
      "[TreatmentC] 20220320_chm134_Cirrhosis_FlowChip15_AA21014EM1_TreatmentC_biorep14_techrep03.raw: 100%\n",
      "[TreatmentC] 20220322_chm134_Cirrhosis_FlowChip15_AY9030EM1_TreatmentC_biorep30_techrep01.raw: 100%|\n",
      "[TreatmentC] 20220322_chm134_Cirrhosis_FlowChip15_AY9030EM1_TreatmentC_biorep30_techrep02.raw: 100%|\n",
      "[TreatmentC] 20220322_chm134_Cirrhosis_FlowChip15_AY9030EM1_TreatmentC_biorep30_techrep03.raw: 100%|\n",
      "[TreatmentC] 20220329_chm134_Cirrhosis_FlowChip15_AY2025EM1_TreatmentC_biorep25_techrep01.raw: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] Cannot open RAW: D:\\TreatmentABC\\20220329_chm134_Cirrhosis_FlowChip15_AY2025EM1_TreatmentC_biorep25_techrep02.raw (Instrument index not available for requested device\n",
      "Parameter name: instrumentIndex\n",
      "   at ThermoFisher.CommonCore.RawFileReader.RawFileAccessBase.SelectInstrument(Device instrumentType, Int32 instrumentIndex))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentC] 20220329_chm134_Cirrhosis_FlowChip15_AY2025EM1_TreatmentC_biorep25_techrep03.raw: 100%|\n",
      "[TreatmentC] 20220330_chm134_Cirrhosis_FlowChip15_AU3034EM1_TreatmentC_biorep34_techrep01.raw: 100%|\n",
      "[TreatmentC] 20220330_chm134_Cirrhosis_FlowChip15_AU3034EM1_TreatmentC_biorep34_techrep02.raw: 100%|\n",
      "[TreatmentC] 20220330_chm134_Cirrhosis_FlowChip15_AU3034EM1_TreatmentC_biorep34_techrep03.raw: 100%|\n",
      "[TreatmentC] 20221103_chm134_Cirrhosis_FlowChip15_AA15006EM1_TreatmentC_biorep06_techrep01.raw: 100%\n",
      "[TreatmentC] 20221103_chm134_Cirrhosis_FlowChip15_AA15006EM1_TreatmentC_biorep06_techrep02.raw: 100%\n",
      "[TreatmentC] 20221103_chm134_Cirrhosis_FlowChip15_AA15006EM1_TreatmentC_biorep06_techrep03.raw: 100%\n",
      "[TreatmentC] 20221108_chm134_Cirrhosis_FlowChip15_AY12032EM1_TreatmentC_biorep32_techrep01.raw: 100%\n",
      "[TreatmentC] 20221108_chm134_Cirrhosis_FlowChip15_AY12032EM1_TreatmentC_biorep32_techrep02.raw: 100%\n",
      "[TreatmentC] 20221108_chm134_Cirrhosis_FlowChip15_AY12032EM1_TreatmentC_biorep32_techrep03.raw: 100%\n",
      "[TreatmentC] 20221111_chm134_Cirrhosis_FlowChip15_AA21012EM1_TreatmentC_biorep12_techrep01.raw: 100%\n",
      "[TreatmentC] 20221111_chm134_Cirrhosis_FlowChip15_AA21012EM1_TreatmentC_biorep12_techrep02.raw: 100%\n",
      "[TreatmentC] 20221111_chm134_Cirrhosis_FlowChip15_AA21012EM1_TreatmentC_biorep12_techrep03.raw: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TreatmentC] Saved MS1: D:\\casts\\databank\\TreatmentC.ms1.npz  shape=(123807, 13690), dtype=float32\n",
      "[TreatmentC] Saved MS2: D:\\casts\\databank\\TreatmentC.ms2.npz  shape=(284409, 1600), dtype=float16\n",
      "[TreatmentC] Saved META: D:\\casts\\databank\\TreatmentC.meta.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentD] 20220315_chm134_Cirrhosis_FlowChip15_AG2635BC1_TreatmentD_biorep35_techrep01.raw: 100%|\n",
      "[TreatmentD] 20220315_chm134_Cirrhosis_FlowChip15_AG2635BC1_TreatmentD_biorep35_techrep02.raw: 100%|\n",
      "[TreatmentD] 20220315_chm134_Cirrhosis_FlowChip15_AG2635BC1_TreatmentD_biorep35_techrep03.raw: 100%|\n",
      "[TreatmentD] 20220317_chm134_Cirrhosis_FlowChip15_AG2639BC1_TreatmentD_biorep38_techrep03.raw: 100%|\n",
      "[TreatmentD] 20220317_chm134_Cirrhosis_FlowChip15_AG2639BC1_TreatmentD_biorep39_techrep01.raw: 100%|\n",
      "[TreatmentD] 20220317_chm134_Cirrhosis_FlowChip15_AG2639BC1_TreatmentD_biorep39_techrep02.raw: 100%|\n",
      "[TreatmentD] 20220317_chm134_Cirrhosis_FlowChip15_AG2639BC1_TreatmentD_biorep39_techrep03.raw: 100%|\n",
      "[TreatmentD] 20220320_chm134_Cirrhosis_FlowChip15_AG2638BC1_TreatmentD_biorep38_techrep01.raw: 100%|\n",
      "[TreatmentD] 20220320_chm134_Cirrhosis_FlowChip15_AG2638BC1_TreatmentD_biorep38_techrep02.raw: 100%|\n",
      "[TreatmentD] 20220322_chm134_Cirrhosis_FlowChip15_AG31050BC1_TreatmentD_biorep50_techrep01.raw: 100%\n",
      "[TreatmentD] 20220322_chm134_Cirrhosis_FlowChip15_AG31050BC1_TreatmentD_biorep50_techrep02.raw: 100%\n",
      "[TreatmentD] 20220322_chm134_Cirrhosis_FlowChip15_AG31050BC1_TreatmentD_biorep50_techrep03.raw: 100%\n",
      "[TreatmentD] 20220329_chm134_Cirrhosis_FlowChip15_AG31046BC1_TreatmentD_biorep46_techrep01.raw: 100%\n",
      "[TreatmentD] 20220329_chm134_Cirrhosis_FlowChip15_AG31046BC1_TreatmentD_biorep46_techrep02.raw: 100%\n",
      "[TreatmentD] 20220329_chm134_Cirrhosis_FlowChip15_AG31046BC1_TreatmentD_biorep46_techrep03.raw: 100%\n",
      "[TreatmentD] 20220330_chm134_Cirrhosis_FlowChip15_AG31048BC1_TreatmentD_biorep48_techrep01.raw: 100%\n",
      "[TreatmentD] 20220330_chm134_Cirrhosis_FlowChip15_AG31048BC1_TreatmentD_biorep48_techrep02.raw: 100%\n",
      "[TreatmentD] 20220330_chm134_Cirrhosis_FlowChip15_AG31048BC1_TreatmentD_biorep48_techrep03.raw: 100%\n",
      "[TreatmentD] 20221103_chm134_Cirrhosis_FlowChip15_AG31049BC1_TreatmentD_biorep49_techrep01.raw: 100%\n",
      "[TreatmentD] 20221103_chm134_Cirrhosis_FlowChip15_AG31049BC1_TreatmentD_biorep49_techrep02.raw: 100%\n",
      "[TreatmentD] 20221103_chm134_Cirrhosis_FlowChip15_AG31049BC1_TreatmentD_biorep49_techrep03.raw: 100%\n",
      "[TreatmentD] 20221105_chm134_Cirrhosis_FlowChip15_AG31047BC1_TreatmentD_biorep47_techrep01.raw: 100%\n",
      "[TreatmentD] 20221105_chm134_Cirrhosis_FlowChip15_AG31047BC1_TreatmentD_biorep47_techrep02.raw: 100%\n",
      "[TreatmentD] 20221105_chm134_Cirrhosis_FlowChip15_AG31047BC1_TreatmentD_biorep47_techrep03.raw: 100%\n",
      "[TreatmentD] 20221108_chm134_Cirrhosis_FlowChip15_AG2637BC1_TreatmentD_biorep37_techrep01.raw: 100%|\n",
      "[TreatmentD] 20221108_chm134_Cirrhosis_FlowChip15_AG2637BC1_TreatmentD_biorep37_techrep02.raw: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] Cannot open RAW: D:\\TreatmentD\\20221108_chm134_Cirrhosis_FlowChip15_AG2637BC1_TreatmentD_biorep37_techrep03.raw (Instrument index not available for requested device\n",
      "Parameter name: instrumentIndex\n",
      "   at ThermoFisher.CommonCore.RawFileReader.RawFileAccessBase.SelectInstrument(Device instrumentType, Int32 instrumentIndex))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TreatmentD] 20221111_chm134_Cirrhosis_FlowChip15_AG2640BC1_TreatmentD_biorep40_techrep01.raw: 100%|\n",
      "[TreatmentD] 20221111_chm134_Cirrhosis_FlowChip15_AG2640BC1_TreatmentD_biorep40_techrep02.raw: 100%|\n",
      "[TreatmentD] 20221111_chm134_Cirrhosis_FlowChip15_AG2640BC1_TreatmentD_biorep40_techrep03.raw: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TreatmentD] Saved MS1: D:\\casts\\databank\\TreatmentD.ms1.npz  shape=(130735, 13690), dtype=float32\n",
      "[TreatmentD] Saved MS2: D:\\casts\\databank\\TreatmentD.ms2.npz  shape=(271277, 1600), dtype=float16\n",
      "[TreatmentD] Saved META: D:\\casts\\databank\\TreatmentD.meta.npz\n"
     ]
    }
   ],
   "source": [
    "results = wholeCasting_per_group(\n",
    "    [\"D:/TreatmentABC\", \"D:/TreatmentD\"],\n",
    "    out_dir=\"D:/casts/databank\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
