{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104e0c9",
   "metadata": {},
   "source": [
    "Generating deconvoluted spectra from the informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975db4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: bin5_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB\n",
      "Processing: bin15_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: bin15_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: bin15_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: bin15_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runB\n",
      "‚úÖ All files processed. Results saved in: F:\\test\\neuro\\result\\result\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_te7_e7g8\\bin15_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\test\\neuro\\result\\result\\bin15_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "üìÇ Clean result folder ready with only *_mass.txt files: F:\\test\\neuro\\result\\result\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def _unique_dst_path(dst_dir, fname):\n",
    "    \"\"\"Return a unique path in dst_dir for fname, adding a numeric suffix if needed.\"\"\"\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    candidate = os.path.join(dst_dir, fname)\n",
    "    i = 1\n",
    "    while os.path.exists(candidate):\n",
    "        candidate = os.path.join(dst_dir, f\"{base}__{i}{ext}\")\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "def _prefixed_name(src_path, result_root):\n",
    "    \"\"\"\n",
    "    Build a safer filename using the immediate parent folder under result/ as a prefix\n",
    "    to reduce collisions: e.g., result/sampleA/sampleA_mass.txt -> sampleA__sampleA_mass.txt\n",
    "    \"\"\"\n",
    "    # src_path like .../result/<parent>/<file>\n",
    "    parent = os.path.basename(os.path.dirname(src_path))\n",
    "    fname = os.path.basename(src_path)\n",
    "    return f\"{parent}__{fname}\" if parent and parent != \"result\" else fname\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder (top-level only)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} ‚Üí {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"‚úÖ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "    # 1) Collect *_mass.txt paths from result_root (including subfolders)\n",
    "    collected = []\n",
    "    for root, _, files in os.walk(result_root):\n",
    "        for f in files:\n",
    "            if f.endswith(\"_mass.txt\"):\n",
    "                collected.append(os.path.join(root, f))\n",
    "\n",
    "    if not collected:\n",
    "        print(\"‚ö†Ô∏è No *_mass.txt files found under:\", result_root)\n",
    "        return\n",
    "\n",
    "    # 2) Copy them to a temp folder FIRST (so deleting result/ content won't break src paths)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"mass_collect_\")\n",
    "    copied = []\n",
    "    for src in collected:\n",
    "        try:\n",
    "            # Prefix with subfolder name to avoid collisions\n",
    "            safe_name = _prefixed_name(src, result_root)\n",
    "            dst = os.path.join(temp_dir, safe_name)\n",
    "            dst = _unique_dst_path(temp_dir, os.path.basename(dst))  # ensure uniqueness\n",
    "            print(f\"Staging: {src} ‚Üí {dst}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            copied.append(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skip (copy error): {src} ‚Äî {e}\")\n",
    "\n",
    "    # 3) Clean the result_root completely\n",
    "    for item in os.listdir(result_root):\n",
    "        item_path = os.path.join(result_root, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove {item_path}: {e}\")\n",
    "\n",
    "    # 4) Move staged files back into a clean result_root\n",
    "    for staged in copied:\n",
    "        try:\n",
    "            final_dst = os.path.join(result_root, os.path.basename(staged))\n",
    "            final_dst = _unique_dst_path(result_root, os.path.basename(final_dst))\n",
    "            print(f\"Finalizing: {staged} ‚Üí {final_dst}\")\n",
    "            shutil.move(staged, final_dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Move error for {staged}: {e}\")\n",
    "\n",
    "    # 5) Remove temp dir (ignore errors)\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"üìÇ Clean result folder ready with only *_mass.txt files:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\test\\neuro\\result\"  # <-- replace with your folder\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9152b0",
   "metadata": {},
   "source": [
    "Discovery of proteoforms with S/N more than 10, and matching them with their charge states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75b343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Neutral-mass detection] 62 peaks ‚Üí F:\\test\\5__pos_1__neg_0_pos_runA_mass_detected_signals.csv\n",
      "[Neutral-mass detection] Plot saved ‚Üí F:\\test\\5__pos_1__neg_0_pos_runA_mass_detected_signals.png\n",
      "Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': '5__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:/new\\neutral_mass_spectrum.png\n",
      "Saved plot: F:/new\\mirror_assigned_vs_total.png\n",
      "Saved plot: F:/new\\mirror_unassigned_vs_total.png\n",
      "Saved plot: F:/new\\mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== Summary ===\n",
      "Raw MS1 peaks (rows): 7,346\n",
      "Detected neutral-mass peaks: 62\n",
      "Assigned raw peaks: 1,352\n",
      "Non-assigned raw peaks: 5,994\n",
      "Saved: F:/new\\assigned_ms1_with_peaks.csv\n",
      "Saved: F:/new\\assignments_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pipeline:\n",
    "1) Detect neutral-mass peaks from a deconvoluted spectrum (mass intensity; whitespace- or csv-delimited).\n",
    "2) Use detected peaks as candidate proteins and assign raw MS1 peaks by charge-series matching.\n",
    "\n",
    "Outputs:\n",
    "- <deconv_stem>_detected_signals.csv / .png   (neutral-mass peak picks + metadata, includes SNR)\n",
    "- OUT_DIR/assigned_ms1_with_peaks.csv         (annotated raw MS1 with assigned_mass/charge)\n",
    "- OUT_DIR/assignments_summary.csv             (one row per accepted neutral mass; now includes SNR)\n",
    "- OUT_DIR/<several_plots>.png                 (mirror plots + neutral-mass spectrum)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --------------------  PEAK DETECTION  ----------------------\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakFindingParams:\n",
    "    min_prominence: float | None = None\n",
    "    min_height: float | None = None\n",
    "    min_distance_pts: int = 10\n",
    "    smooth_window: int = 0\n",
    "    min_snr: float = 0.0\n",
    "\n",
    "\n",
    "def _mad_sigma(y: np.ndarray) -> float:\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    med = np.median(y)\n",
    "    mad = np.median(np.abs(y - med))\n",
    "    return 1.4826 * mad\n",
    "\n",
    "\n",
    "def _smooth(y: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window < 3 or window % 2 == 0:\n",
    "        return y\n",
    "    kernel = np.ones(window, dtype=float) / window\n",
    "    return np.convolve(y, kernel, mode=\"same\")\n",
    "\n",
    "\n",
    "def _extract_id_list(name: str, key: str) -> list[int] | None:\n",
    "    \"\"\"\n",
    "    Extract an underscore- or hyphen-separated list of integers after a key.\n",
    "    Examples:\n",
    "      \"__pos_3__\"          -> [3]\n",
    "      \"__neg_0_1_2_\"       -> [0,1,2]\n",
    "      \"-pos-10-11\"         -> [10,11]\n",
    "    \"\"\"\n",
    "    m = re.search(rf\"(?:^|[_-]){key}((?:[_-]\\d+)+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if not m:\n",
    "        m1 = re.search(rf\"(?:^|[_-]){key}[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "        if m1:\n",
    "            return [int(m1.group(1))]\n",
    "        return None\n",
    "    parts = re.findall(r\"\\d+\", m.group(1))\n",
    "    return [int(x) for x in parts] if parts else None\n",
    "\n",
    "\n",
    "def parse_metadata_from_filename(path: str | Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract bin, experiments/controls (IDs + counts), regulation, replicate, source_file.\n",
    "    Regulation token is taken as the LAST occurrence among (negabs|posabs|neg|pos).\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    name = p.stem\n",
    "\n",
    "    meta = {\n",
    "        \"bin\": None,\n",
    "        \"experiments\": None,\n",
    "        \"controls\": None,\n",
    "        \"experiments_ids\": None,\n",
    "        \"controls_ids\": None,\n",
    "        \"experiments_n\": None,\n",
    "        \"controls_n\": None,\n",
    "        \"regulation\": None,\n",
    "        \"replicate\": None,\n",
    "        \"source_file\": p.name,\n",
    "    }\n",
    "\n",
    "    # bin: \"bin5\"/\"bin_5\" or a leading number \"75__pos_...\"\n",
    "    m = re.search(r\"(?:^|[_-])bin[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if m:\n",
    "        meta[\"bin\"] = int(m.group(1))\n",
    "    else:\n",
    "        m2 = re.match(r\"^(\\d+)(?=[_-])\", name)\n",
    "        if m2:\n",
    "            meta[\"bin\"] = int(m2.group(1))\n",
    "\n",
    "    exp_ids = _extract_id_list(name, \"pos\")\n",
    "    ctl_ids = _extract_id_list(name, \"neg\")\n",
    "    if exp_ids is not None:\n",
    "        meta[\"experiments_ids\"] = \",\".join(str(x) for x in exp_ids)\n",
    "        meta[\"experiments_n\"] = len(exp_ids)\n",
    "        meta[\"experiments\"] = len(exp_ids)\n",
    "    if ctl_ids is not None:\n",
    "        meta[\"controls_ids\"] = \",\".join(str(x) for x in ctl_ids)\n",
    "        meta[\"controls_n\"] = len(ctl_ids)\n",
    "        meta[\"controls\"] = len(ctl_ids)\n",
    "\n",
    "    reg_tokens = [m.group(1).lower() for m in re.finditer(\n",
    "        r\"(?:^|[_-])(negabs|posabs|neg|pos)(?=[_-]|$)\", name, flags=re.I\n",
    "    )]\n",
    "    if reg_tokens:\n",
    "        token = reg_tokens[-1]\n",
    "        reg_map = {\"negabs\": \"downregulated\", \"neg\": \"downregulated\",\n",
    "                   \"posabs\": \"upregulated\", \"pos\": \"upregulated\"}\n",
    "        meta[\"regulation\"] = reg_map.get(token)\n",
    "\n",
    "    m = re.search(r\"(?:^|[_-])run([A-Za-z])(?=[_-]|$)\", name)\n",
    "    if m:\n",
    "        meta[\"replicate\"] = m.group(1).upper()\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def load_space_separated(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    # whitespace-delimited by default; fallback to CSV\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", header=None,\n",
    "                         names=[\"mass\", \"intensity\"], comment=\"#\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"mass\", \"intensity\"]\n",
    "        else:\n",
    "            raise ValueError(\"Deconvoluted file must have at least two columns: mass intensity\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_signals(\n",
    "    df: pd.DataFrame,\n",
    "    params: PeakFindingParams = PeakFindingParams()\n",
    ") -> pd.DataFrame:\n",
    "    # normalize columns\n",
    "    if not {\"mass\", \"intensity\"}.issubset(df.columns):\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.copy()\n",
    "            df.columns = [\"mass\", \"intensity\"] + [f\"col{i}\" for i in range(2, df.shape[1])]\n",
    "        else:\n",
    "            raise ValueError(\"Input DataFrame must have columns ['mass','intensity'].\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"mass\", \"intensity\"])\n",
    "    df = df.sort_values(\"mass\").reset_index(drop=True)\n",
    "\n",
    "    x = df[\"mass\"].to_numpy(float)\n",
    "    y = df[\"intensity\"].to_numpy(float)\n",
    "\n",
    "    y_proc = _smooth(y, params.smooth_window)\n",
    "\n",
    "    sigma = _mad_sigma(y_proc)\n",
    "    ymax = float(np.max(y_proc)) if y_proc.size else 0.0\n",
    "\n",
    "    min_prom = params.min_prominence or max(6.0 * sigma, 0.001 * ymax)\n",
    "    min_h    = params.min_height     or max(4.0 * sigma, 0.0005 * ymax)\n",
    "\n",
    "    peaks, props = find_peaks(\n",
    "        y_proc,\n",
    "        prominence=min_prom,\n",
    "        height=min_h,\n",
    "        distance=max(1, int(params.min_distance_pts))\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mass\": x[peaks],\n",
    "        \"intensity\": y[peaks],\n",
    "        \"prominence\": props.get(\"prominences\", np.full(peaks.shape, np.nan)),\n",
    "        \"left_base_idx\": props.get(\"left_bases\", np.full(peaks.shape, -1)),\n",
    "        \"right_base_idx\": props.get(\"right_bases\", np.full(peaks.shape, -1)),\n",
    "    })\n",
    "\n",
    "    # SNR estimate and filter\n",
    "    snr_den = sigma if sigma > 0 else (np.std(y_proc) if y_proc.size else 1.0)\n",
    "    snr_den = snr_den if snr_den > 0 else 1.0\n",
    "    out[\"snr\"] = out[\"intensity\"] / snr_den\n",
    "\n",
    "    if params.min_snr > 0:\n",
    "        out = out[out[\"snr\"] >= params.min_snr].reset_index(drop=True)\n",
    "\n",
    "    return out.sort_values(\"intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def plot_spectrum_with_peaks(\n",
    "    df: pd.DataFrame,\n",
    "    peaks_df: pd.DataFrame,\n",
    "    out_png: str | Path | None = None,\n",
    "    title: str = \"Detected Neutral-Mass Signals\"\n",
    ") -> None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"mass\"].to_numpy(), df[\"intensity\"].to_numpy(), linewidth=1)\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        plt.scatter(\n",
    "            peaks_df[\"mass\"].to_numpy(),\n",
    "            peaks_df[\"intensity\"].to_numpy(),  # use peaks‚Äô intensities\n",
    "            s=18\n",
    "        )\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# -----------------  CHARGE-SERIES MATCHING  -----------------\n",
    "# ============================================================\n",
    "\n",
    "# Matching parameters (tweak as needed)\n",
    "PROTON_MASS = 1.007276466812  # Da\n",
    "\n",
    "Z_MIN, Z_MAX = 5, 50\n",
    "PPM_TOL = 1000.0              # ppm window for m/z match\n",
    "ABS_DA_TOL = 1.0              # absolute Da floor (used with ppm)\n",
    "MIN_MATCHED_CHARGE_STATES = 4 # require ‚â• N charge-state hits to accept a protein\n",
    "\n",
    "\n",
    "def _read_raw_ms1(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly read raw MS1 CSV. Expected two columns (m/z, intensity), with or without headers.\n",
    "    If more columns exist, pick the best 'mz' and 'intensity' columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "\n",
    "    if df.shape[1] == 2:\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "    else:\n",
    "        cols_lower = [str(c).lower() for c in df.columns]\n",
    "        mz_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                         if (\"mz\" in c) or (\"m/z\" in c) or (\"mass/charge\" in c) or (c.strip() == \"m z\")]\n",
    "        int_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                          if (\"int\" in c) or (\"abund\" in c) or (\"height\" in c) or (\"signal\" in c)]\n",
    "        if not mz_candidates:\n",
    "            mz_candidates = [0]\n",
    "        if not int_candidates:\n",
    "            int_candidates = [1 if df.shape[1] > 1 else 0]\n",
    "        df = df.iloc[:, [mz_candidates[0], int_candidates[0]]].copy()\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df = df[df[\"intensity\"] > 0].copy()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"mz\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ppm_window(target_mz: float, ppm: float, abs_da: float) -> tuple[float, float]:\n",
    "    da = target_mz * ppm * 1e-6\n",
    "    tol = max(da, abs_da)\n",
    "    return target_mz - tol, target_mz + tol\n",
    "\n",
    "\n",
    "def _match_targets(sorted_mz: np.ndarray, targets: np.ndarray,\n",
    "                   ppm: float, abs_da: float, available_mask: np.ndarray) -> dict[int, int | None]:\n",
    "    results: dict[int, int | None] = {}\n",
    "    for ti, t in enumerate(targets):\n",
    "        lo, hi = _ppm_window(t, ppm, abs_da)\n",
    "        j = bisect_left(sorted_mz, t)\n",
    "        best_idx = None\n",
    "        best_delta = float(\"inf\")\n",
    "        for k in (j, j-1, j+1, j-2, j+2, j-3, j+3):\n",
    "            if 0 <= k < len(sorted_mz):\n",
    "                mz_k = sorted_mz[k]\n",
    "                if available_mask[k] and (lo <= mz_k <= hi):\n",
    "                    delta = abs(mz_k - t)\n",
    "                    if delta < best_delta:\n",
    "                        best_delta = delta\n",
    "                        best_idx = k\n",
    "        results[ti] = best_idx\n",
    "    return results\n",
    "\n",
    "\n",
    "def _generate_charge_series(neutral_mass: float, z_min: int, z_max: int) -> pd.DataFrame:\n",
    "    z = np.arange(z_min, z_max + 1, dtype=int)\n",
    "    mz = (neutral_mass + z * PROTON_MASS) / z\n",
    "    return pd.DataFrame({\"z\": z, \"target_mz\": mz})\n",
    "\n",
    "\n",
    "def assign_ms1_peaks(raw_df: pd.DataFrame, deconv_peaks_df: pd.DataFrame,\n",
    "                     meta: dict | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assign raw MS1 peaks to detected neutral masses (from deconvoluted spectrum) by charge-series matching.\n",
    "\n",
    "    Returns:\n",
    "      assigned_raw: raw_df with columns [assigned_mass, assigned_charge, is_assigned]\n",
    "      assignments_summary: one row per accepted neutral mass with metadata, includes SNR\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.sort_values(\"mz\").reset_index(drop=True)\n",
    "    mz_arr = raw_df[\"mz\"].to_numpy()\n",
    "    inten_arr = raw_df[\"intensity\"].to_numpy()\n",
    "    available = np.ones(len(raw_df), dtype=bool)\n",
    "\n",
    "    assigned_mass = np.full(len(raw_df), np.nan)\n",
    "    assigned_z    = np.full(len(raw_df), np.nan)\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for r in deconv_peaks_df.itertuples(index=False):\n",
    "        mass = float(r.mass)\n",
    "        mass_intensity = float(r.intensity)\n",
    "        mass_snr = float(getattr(r, \"snr\", np.nan))  # <-- carry SNR into the summary\n",
    "\n",
    "        series = _generate_charge_series(mass, Z_MIN, Z_MAX)\n",
    "        targets = series[\"target_mz\"].to_numpy()\n",
    "\n",
    "        matches = _match_targets(mz_arr, targets, PPM_TOL, ABS_DA_TOL, available_mask=available)\n",
    "\n",
    "        matched_indices = []\n",
    "        matched_z_list  = []\n",
    "        matched_mz_list = []\n",
    "\n",
    "        for ti, k in matches.items():\n",
    "            if k is not None:\n",
    "                matched_indices.append(k)\n",
    "                matched_z_list.append(int(series.iloc[ti][\"z\"]))\n",
    "                matched_mz_list.append(mz_arr[k])\n",
    "\n",
    "        if len(matched_indices) >= MIN_MATCHED_CHARGE_STATES:\n",
    "            # accept and mark assigned\n",
    "            for idx, z_val in zip(matched_indices, matched_z_list):\n",
    "                if available[idx]:\n",
    "                    available[idx] = False\n",
    "                    assigned_mass[idx] = mass\n",
    "                    assigned_z[idx] = z_val\n",
    "\n",
    "            frac_intensity_removed = (\n",
    "                float(np.sum(inten_arr[matched_indices])) / float(np.sum(inten_arr))\n",
    "                if inten_arr.sum() > 0 else 0.0\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"neutral_mass\": mass,\n",
    "                \"deconv_intensity\": mass_intensity,\n",
    "                \"snr\": mass_snr,  # <-- new column in assignments_summary\n",
    "                \"n_matches\": len(matched_indices),\n",
    "                \"matched_z_list\": json.dumps(matched_z_list),\n",
    "                \"matched_mz_list\": json.dumps([round(float(x), 1) for x in matched_mz_list]),\n",
    "                \"ppm_tol\": PPM_TOL,\n",
    "                \"abs_da_tol\": ABS_DA_TOL,\n",
    "                \"fraction_total_intensity_captured\": frac_intensity_removed\n",
    "            }\n",
    "            # attach filename metadata if available\n",
    "            if meta:\n",
    "                row.update({\n",
    "                    \"bin\": meta.get(\"bin\"),\n",
    "                    \"experiments_ids\": meta.get(\"experiments_ids\"),\n",
    "                    \"controls_ids\": meta.get(\"controls_ids\"),\n",
    "                    \"regulation\": meta.get(\"regulation\"),\n",
    "                    \"replicate\": meta.get(\"replicate\"),\n",
    "                    \"source_file\": meta.get(\"source_file\"),\n",
    "                })\n",
    "            summary_rows.append(row)\n",
    "\n",
    "    assigned_raw = raw_df.copy()\n",
    "    assigned_raw[\"assigned_mass\"] = assigned_mass\n",
    "    assigned_raw[\"assigned_charge\"] = assigned_z\n",
    "    assigned_raw[\"is_assigned\"] = ~np.isnan(assigned_mass)\n",
    "\n",
    "    assignments_summary = pd.DataFrame(summary_rows).sort_values(\n",
    "        \"deconv_intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # if meta present, also add bin to assigned_raw (useful downstream)\n",
    "    if meta and \"bin\" in meta:\n",
    "        assigned_raw[\"bin\"] = meta[\"bin\"]\n",
    "\n",
    "    return assigned_raw, assignments_summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  PLOTTING  --------------------------\n",
    "# ============================================================\n",
    "\n",
    "def plot_neutral_mass_spectrum(deconv_peaks_df: pd.DataFrame, out_dir: str,\n",
    "                               filename: str = \"neutral_mass_spectrum.png\"):\n",
    "    if deconv_peaks_df.empty:\n",
    "        print(\"No neutral masses to plot.\")\n",
    "        return\n",
    "    masses = deconv_peaks_df[\"mass\"].to_numpy()\n",
    "    intens = deconv_peaks_df[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(9, 4.5))\n",
    "    plt.vlines(masses, 0, intens, linewidth=1)\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Neutral Mass Spectrum (detected peaks)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                  filename: str = \"mirror_assigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    assigned_mask = assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    assigned_int = np.where(assigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                 # Top: total\n",
    "    plt.vlines(mz[assigned_mask], 0, -assigned_int[assigned_mask], linewidth=0.8)  # Bottom: assigned (neg)\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -assigned_int.max() if assigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_unassigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                    filename: str = \"mirror_unassigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    unassigned_mask = ~assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    unassigned_int = np.where(unassigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                     # Top: total\n",
    "    plt.vlines(mz[unassigned_mask], 0, -unassigned_int[unassigned_mask], linewidth=0.8)  # Bottom: unassigned\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -unassigned_int.max() if unassigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Non-assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_by_protein_vs_total(\n",
    "    assigned_raw: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    filename: str = \"mirror_assigned_by_protein_vs_total.png\",\n",
    "    max_legend_items: int = 20\n",
    "):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(11, 5.6))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.5)  # top: total\n",
    "\n",
    "    df_assigned_only = assigned_raw[assigned_raw[\"is_assigned\"]].copy()\n",
    "    if df_assigned_only.empty:\n",
    "        plt.xlabel(\"m/z\")\n",
    "        plt.ylabel(\"Intensity (arb.)\")\n",
    "        plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {out_path}\")\n",
    "        return\n",
    "\n",
    "    counts = (\n",
    "        df_assigned_only.groupby(\"assigned_mass\", dropna=True)[\"is_assigned\"]\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    proteins_in_order = counts.index.tolist()\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get(\n",
    "        'color', ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "    )\n",
    "\n",
    "    legend_entries = 0\n",
    "    for i, mass in enumerate(proteins_in_order):\n",
    "        mask = (assigned_raw[\"assigned_mass\"] == mass)\n",
    "        mz_i = assigned_raw.loc[mask, \"mz\"].to_numpy()\n",
    "        inten_i = assigned_raw.loc[mask, \"intensity\"].to_numpy()\n",
    "\n",
    "        label = None\n",
    "        if legend_entries < max_legend_items:\n",
    "            label = f\"{mass/1000:.2f} kDa (n={len(mz_i)})\"\n",
    "            legend_entries += 1\n",
    "\n",
    "        plt.vlines(\n",
    "            mz_i, 0, -inten_i,\n",
    "            linewidth=0.8,\n",
    "            color=color_cycle[i % len(color_cycle)],\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -df_assigned_only[\"intensity\"].max() if len(df_assigned_only) else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    if legend_entries:\n",
    "        plt.legend(title=\"Assigned proteins\", loc=\"upper right\", fontsize=8, ncol=1)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  MAIN SCRIPT  -----------------------\n",
    "# ============================================================\n",
    "\n",
    "# ---- User-configurable paths ----\n",
    "RAW_MS1_CSV = r\"F:/test/5__pos_1__neg_0_pos_runA.csv\"        # raw MS1 (m/z, intensity)\n",
    "DECONV_TXT  = r\"F:/test/5__pos_1__neg_0_pos_runA_mass.txt\"   # deconvoluted neutral masses (mass intensity)\n",
    "OUT_DIR     = r\"F:/new\"\n",
    "\n",
    "# ---- Neutral-mass peak detection parameters ----\n",
    "DECONV_DETECT_PARAMS = PeakFindingParams(\n",
    "    min_distance_pts=20,   # decon masses can be coarse; 2 is a good start\n",
    "    min_snr=10,          # enforce minimum SNR\n",
    "    smooth_window=0,      # set to 5/7 if your decon spectrum is very noisy\n",
    "    # min_prominence=None, min_height=None  # auto from MAD if None\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- 1) Load deconvoluted spectrum & detect neutral-mass peaks ---\n",
    "    meta = parse_metadata_from_filename(DECONV_TXT)\n",
    "    deconv_raw = load_space_separated(DECONV_TXT)\n",
    "    deconv_peaks = detect_signals(deconv_raw, params=DECONV_DETECT_PARAMS)\n",
    "\n",
    "    # attach filename metadata to neutral-mass peaks table for traceability\n",
    "    deconv_peaks = deconv_peaks.assign(\n",
    "        bin=meta.get(\"bin\"),\n",
    "        experiments_ids=meta.get(\"experiments_ids\"),\n",
    "        controls_ids=meta.get(\"controls_ids\"),\n",
    "        regulation=meta.get(\"regulation\"),\n",
    "        replicate=meta.get(\"replicate\"),\n",
    "        source_file=meta.get(\"source_file\"),\n",
    "    )\n",
    "\n",
    "    # Save + plot neutral-mass detections alongside the deconv file\n",
    "    in_path = Path(DECONV_TXT)\n",
    "    out_detect_csv = str(in_path.with_name(in_path.stem + \"_detected_signals.csv\"))\n",
    "    out_detect_png = str(in_path.with_name(in_path.stem + \"_detected_signals.png\"))\n",
    "    deconv_peaks.to_csv(out_detect_csv, index=False)\n",
    "    plot_spectrum_with_peaks(deconv_raw, deconv_peaks, out_png=out_detect_png)\n",
    "    print(f\"[Neutral-mass detection] {len(deconv_peaks)} peaks ‚Üí {out_detect_csv}\")\n",
    "    print(f\"[Neutral-mass detection] Plot saved ‚Üí {out_detect_png}\")\n",
    "    print(\"Parsed filename metadata:\", meta)\n",
    "\n",
    "    # --- 2) Read raw MS1 and assign charge-series to detected masses ---\n",
    "    raw_df = _read_raw_ms1(RAW_MS1_CSV)\n",
    "\n",
    "    assigned_raw, summary = assign_ms1_peaks(raw_df, deconv_peaks, meta=meta)\n",
    "\n",
    "    out_assigned = os.path.join(OUT_DIR, \"assigned_ms1_with_peaks.csv\")\n",
    "    out_summary  = os.path.join(OUT_DIR, \"assignments_summary.csv\")\n",
    "    assigned_raw.to_csv(out_assigned, index=False)\n",
    "    summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # --- 3) Plots on assignments ---\n",
    "    plot_neutral_mass_spectrum(deconv_peaks, OUT_DIR, filename=\"neutral_mass_spectrum.png\")\n",
    "    plot_mirror_assigned_vs_total(assigned_raw, OUT_DIR, filename=\"mirror_assigned_vs_total.png\")\n",
    "    plot_mirror_unassigned_vs_total(assigned_raw, OUT_DIR, filename=\"mirror_unassigned_vs_total.png\")\n",
    "    plot_mirror_assigned_by_protein_vs_total(\n",
    "        assigned_raw, OUT_DIR, filename=\"mirror_assigned_by_protein_vs_total.png\", max_legend_items=20\n",
    "    )\n",
    "\n",
    "    # --- 4) Console report ---\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Raw MS1 peaks (rows): {len(raw_df):,}\")\n",
    "    print(f\"Detected neutral-mass peaks: {len(deconv_peaks):,}\")\n",
    "    print(f\"Assigned raw peaks: {int(assigned_raw['is_assigned'].sum()):,}\")\n",
    "    print(f\"Non-assigned raw peaks: {int((~assigned_raw['is_assigned']).sum()):,}\")\n",
    "    print(f\"Saved: {out_assigned}\")\n",
    "    print(f\"Saved: {out_summary}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb9849",
   "metadata": {},
   "source": [
    "Batch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d35567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 decon file(s) in F:/test/neuro/decon\n",
      "Found 8 raw CSV file(s) in F:/test/neuro/raw\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 48 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': 15, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'bin15_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin15_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 4,657\n",
      "Detected neutral-mass peaks: 48\n",
      "Assigned raw peaks: 833\n",
      "Non-assigned raw peaks: 3,824\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 42 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[bin15_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': 15, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'bin15_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin15_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 5,133\n",
      "Detected neutral-mass peaks: 42\n",
      "Assigned raw peaks: 746\n",
      "Non-assigned raw peaks: 4,387\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 58 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': 15, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'bin15_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin15_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 5,343\n",
      "Detected neutral-mass peaks: 58\n",
      "Assigned raw peaks: 1,257\n",
      "Non-assigned raw peaks: 4,086\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 43 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[bin15_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': 15, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'bin15_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin15_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 4,867\n",
      "Detected neutral-mass peaks: 43\n",
      "Assigned raw peaks: 967\n",
      "Non-assigned raw peaks: 3,900\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 88 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin5_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 2,987\n",
      "Detected neutral-mass peaks: 88\n",
      "Assigned raw peaks: 1,497\n",
      "Non-assigned raw peaks: 1,490\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 92 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[bin5_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin5_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 2,586\n",
      "Detected neutral-mass peaks: 92\n",
      "Assigned raw peaks: 1,489\n",
      "Non-assigned raw peaks: 1,097\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 16 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin5_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 7,013\n",
      "Detected neutral-mass peaks: 16\n",
      "Assigned raw peaks: 318\n",
      "Non-assigned raw peaks: 6,695\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 15 peaks ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[bin5_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [bin5_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 7,414\n",
      "Detected neutral-mass peaks: 15\n",
      "Assigned raw peaks: 297\n",
      "Non-assigned raw peaks: 7,117\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "‚úÖ Batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ----------------------  BATCH MAIN  ------------------------\n",
    "# ============================================================\n",
    "# Process many files: deconvoluted spectra in one folder,\n",
    "# raw MS1 spectra in another folder. Outputs go to a separate folder.\n",
    "#\n",
    "# Pairing rule: files are matched by a shared \"base key\"\n",
    "#   - deconv: <base>_mass.txt\n",
    "#   - raw:    <base>.csv\n",
    "# Example:\n",
    "#   RAW_DIR:    F:/raw_folder\n",
    "#       5__pos_1__neg_0_pos_runA.csv\n",
    "#   DECONV_DIR: F:/deconv_folder\n",
    "#       5__pos_1__neg_0_pos_runA_mass.txt\n",
    "#   -> base key = \"5__pos_1__neg_0_pos_runA\"\n",
    "#\n",
    "# Notes:\n",
    "# - Neutral-mass peak detection is always run (on decon files).\n",
    "# - Assignment & mirror plots are run only if the matching raw CSV exists.\n",
    "# - All outputs (CSVs/PNGs) go under OUT_DIR/<base>/...\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------- User-configurable ----------------------\n",
    "RAW_DIR    = r\"F:/test/neuro/raw\"      # folder with raw MS1 CSVs (m/z, intensity)\n",
    "DECONV_DIR = r\"F:/test/neuro/decon\"   # folder with *_mass.txt (mass intensity)\n",
    "OUT_DIR    = r\"F:/test/neuro/firstpass\"   # folder to hold all outputs\n",
    "\n",
    "# Glob patterns (adjust if your extensions differ)\n",
    "RAW_GLOB    = \"*.csv\"\n",
    "DECONV_GLOB = \"*_mass.txt\"\n",
    "\n",
    "# ---- Neutral-mass peak detection parameters ----\n",
    "DECONV_DETECT_PARAMS = PeakFindingParams(\n",
    "    min_distance_pts=20,  # decon masses can be coarse; 20 is a good start\n",
    "    min_snr=10,           # enforce minimum SNR\n",
    "    smooth_window=0,      # set to 5/7 if your decon spectrum is very noisy\n",
    "    # min_prominence=None, min_height=None  # auto from MAD if None\n",
    ")\n",
    "\n",
    "\n",
    "def _base_key_from_deconv(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    For '.../<base>_mass.txt' ‚Üí return '<base>'.\n",
    "    \"\"\"\n",
    "    stem = path.stem\n",
    "    if stem.endswith(\"_mass\"):\n",
    "        return stem[:-5]  # drop \"_mass\"\n",
    "    return stem  # fallback\n",
    "\n",
    "\n",
    "def _base_key_from_raw(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    For '.../<base>.csv' ‚Üí return '<base>'.\n",
    "    \"\"\"\n",
    "    return path.stem\n",
    "\n",
    "\n",
    "def _ensure_dir(p: str | Path) -> None:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_one_pair(deconv_path: Path, raw_path: Path | None) -> None:\n",
    "    \"\"\"\n",
    "    - Detect neutral-mass peaks from deconv_path (always).\n",
    "    - If raw_path exists, assign charge-series and make plots.\n",
    "    - Write all outputs under OUT_DIR/<base>/...\n",
    "    \"\"\"\n",
    "    base = _base_key_from_deconv(deconv_path)\n",
    "    out_root = Path(OUT_DIR) / base\n",
    "    _ensure_dir(out_root)\n",
    "\n",
    "    # --- 1) Load deconvoluted spectrum & detect neutral-mass peaks ---\n",
    "    meta = parse_metadata_from_filename(str(deconv_path))\n",
    "    deconv_raw = load_space_separated(str(deconv_path))\n",
    "    deconv_peaks = detect_signals(deconv_raw, params=DECONV_DETECT_PARAMS)\n",
    "\n",
    "    # Attach filename metadata to neutral-mass peaks table\n",
    "    deconv_peaks = deconv_peaks.assign(\n",
    "        bin=meta.get(\"bin\"),\n",
    "        experiments_ids=meta.get(\"experiments_ids\"),\n",
    "        controls_ids=meta.get(\"controls_ids\"),\n",
    "        regulation=meta.get(\"regulation\"),\n",
    "        replicate=meta.get(\"replicate\"),\n",
    "        source_file=meta.get(\"source_file\"),\n",
    "    )\n",
    "\n",
    "    # Save + plot neutral-mass detections (to OUT_DIR/<base>/...)\n",
    "    out_detect_csv = out_root / f\"{base}_detected_signals.csv\"\n",
    "    out_detect_png = out_root / f\"{base}_detected_signals.png\"\n",
    "    deconv_peaks.to_csv(out_detect_csv, index=False)\n",
    "    plot_spectrum_with_peaks(deconv_raw, deconv_peaks, out_png=str(out_detect_png))\n",
    "    print(f\"[{base}] Neutral-mass detection: {len(deconv_peaks)} peaks ‚Üí {out_detect_csv}\")\n",
    "    print(f\"[{base}] Detection plot saved ‚Üí {out_detect_png}\")\n",
    "    print(f\"[{base}] Parsed filename metadata: {meta}\")\n",
    "\n",
    "    # --- 2) If we have the matching raw MS1 CSV, assign charge-series ---\n",
    "    if raw_path is None or not raw_path.exists():\n",
    "        print(f\"[{base}] ‚ö† No matching RAW CSV found. Skipping assignment.\")\n",
    "        return\n",
    "\n",
    "    raw_df = _read_raw_ms1(str(raw_path))\n",
    "    assigned_raw, summary = assign_ms1_peaks(raw_df, deconv_peaks, meta=meta)\n",
    "\n",
    "    out_assigned = out_root / f\"{base}_assigned_ms1_with_peaks.csv\"\n",
    "    out_summary  = out_root / f\"{base}_assignments_summary.csv\"\n",
    "    assigned_raw.to_csv(out_assigned, index=False)\n",
    "    summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # --- 3) Plots on assignments ---\n",
    "    plot_neutral_mass_spectrum(deconv_peaks, str(out_root), filename=f\"{base}_neutral_mass_spectrum.png\")\n",
    "    plot_mirror_assigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_vs_total.png\")\n",
    "    plot_mirror_unassigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_unassigned_vs_total.png\")\n",
    "    plot_mirror_assigned_by_protein_vs_total(\n",
    "        assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_by_protein_vs_total.png\", max_legend_items=20\n",
    "    )\n",
    "\n",
    "    # --- 4) Console report ---\n",
    "    print(f\"\\n=== [{base}] Summary ===\")\n",
    "    print(f\"Raw MS1 peaks (rows): {len(raw_df):,}\")\n",
    "    print(f\"Detected neutral-mass peaks: {len(deconv_peaks):,}\")\n",
    "    print(f\"Assigned raw peaks: {int(assigned_raw['is_assigned'].sum()):,}\")\n",
    "    print(f\"Non-assigned raw peaks: {int((~assigned_raw['is_assigned']).sum()):,}\")\n",
    "    print(f\"Saved: {out_assigned}\")\n",
    "    print(f\"Saved: {out_summary}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    _ensure_dir(OUT_DIR)\n",
    "\n",
    "    # Index raw files by base key\n",
    "    raw_files = [Path(p) for p in glob.glob(str(Path(RAW_DIR) / RAW_GLOB))]\n",
    "    raw_index = {_base_key_from_raw(p): p for p in raw_files}\n",
    "\n",
    "    # Walk all deconvoluted files and process\n",
    "    deconv_files = [Path(p) for p in glob.glob(str(Path(DECONV_DIR) / DECONV_GLOB))]\n",
    "    if not deconv_files:\n",
    "        print(f\"‚ö† No deconvoluted files found in: {DECONV_DIR} (pattern: {DECONV_GLOB})\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(deconv_files)} decon file(s) in {DECONV_DIR}\")\n",
    "    print(f\"Found {len(raw_files)} raw CSV file(s) in {RAW_DIR}\")\n",
    "\n",
    "    for deconv_path in sorted(deconv_files):\n",
    "        base = _base_key_from_deconv(deconv_path)\n",
    "        raw_path = raw_index.get(base, None)\n",
    "        try:\n",
    "            process_one_pair(deconv_path, raw_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[{base}] ‚ùå Error: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Batch processing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee3783",
   "metadata": {},
   "source": [
    "collecting all the reports to a single folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4f077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 summary file(s). Copying to F:\\test\\neuro\\firstpass\\report...\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runA\\bin15_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_negabs_runB\\bin15_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runA\\bin15_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin15_grads_AB__pos_1__neg_0_pos_runB\\bin15_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runA\\bin5_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_negabs_runB\\bin5_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runA\\bin5_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\test\\neuro\\firstpass\\bin5_grads_AB__pos_1__neg_0_pos_runB\\bin5_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------- User settings ----------------------\n",
    "BATCH_OUT_DIR = r\"F:\\test\\neuro\\firstpass\"         # where all subfolders were created\n",
    "SUMMARY_OUT   = r\"F:\\test\\neuro\\firstpass\\report\"         # folder where you want to collect them\n",
    "PATTERN       = \"*_assignments_summary.csv\" # filename pattern\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    os.makedirs(SUMMARY_OUT, exist_ok=True)\n",
    "\n",
    "    # Search recursively for *_assignments_summary.csv\n",
    "    summary_files = glob.glob(str(Path(BATCH_OUT_DIR) / \"**\" / PATTERN), recursive=True)\n",
    "    if not summary_files:\n",
    "        print(\"‚ö† No summary files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(summary_files)} summary file(s). Copying to {SUMMARY_OUT}...\")\n",
    "    for f in summary_files:\n",
    "        src = Path(f)\n",
    "        dst = Path(SUMMARY_OUT) / src.name\n",
    "        # Avoid overwriting if same name appears ‚Üí add parent folder name\n",
    "        if dst.exists():\n",
    "            dst = Path(SUMMARY_OUT) / f\"{src.parent.name}_{src.name}\"\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied: {src} ‚Üí {dst}\")\n",
    "\n",
    "    print(\"‚úÖ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06695ed1",
   "metadata": {},
   "source": [
    "Concatenate all the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f3cd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin15_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\test\\neuro\\firstpass\\report\\bin5_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Combined 8 files ‚Üí F:\\test\\neuro\\firstpass\\report\\report.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "FOLDER_PATH = r\"F:\\test\\neuro\\firstpass\\report\"   # change to your folder\n",
    "OUTPUT_FILE = r\"F:\\test\\neuro\\firstpass\\report\\report.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN\n",
    "# ----------------------------\n",
    "def concat_csvs(folder_path: str, output_file: str):\n",
    "    # Find all .csv files\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the folder!\")\n",
    "\n",
    "    # Read and concatenate\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Reading {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Save result\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Combined {len(csv_files)} files ‚Üí {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    concat_csvs(FOLDER_PATH, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45e0d",
   "metadata": {},
   "source": [
    "Quantification of all proteoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15558a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\bioinfor\\assignments_with_quant_sums.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Config (edit paths)\n",
    "# --------------------\n",
    "DATASET_RT_PATH = r\"F:\\casts\\databank\\csv_files\\dataset_rt.csv\"          # wide matrix with cast_* columns\n",
    "ASSIGNMENTS_PATH = r\"F:\\bioinfor\\report.csv\"          # has 'bin' and 'matched_mz_list'\n",
    "OUT_PATH = os.path.join(\n",
    "    os.path.dirname(ASSIGNMENTS_PATH) or \".\",\n",
    "    \"assignments_with_quant_sums.csv\"\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def to_cast_col(n: float) -> str:\n",
    "    \"\"\"Map an m/z to its cast_* column name: int((mz-600)*10), zero-padded.\"\"\"\n",
    "    col_num = int((float(n) - 600.0) * 10.0)\n",
    "    return \"cast_\" + str(col_num).zfill(5)\n",
    "\n",
    "def parse_mz_list(val):\n",
    "    \"\"\"Safely parse matched_mz_list cells that look like '[864.9, 865.2, ...]'.\"\"\"\n",
    "    try:\n",
    "        out = ast.literal_eval(str(val))\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return [float(x) for x in out]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df_rt = pd.read_csv(DATASET_RT_PATH)\n",
    "df_asn = pd.read_csv(ASSIGNMENTS_PATH)\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"bin\", \"target\"]:\n",
    "    if col not in df_rt.columns:\n",
    "        raise KeyError(f\"'{col}' column is required in dataset_rt.csv\")\n",
    "\n",
    "if \"bin\" not in df_asn.columns or \"matched_mz_list\" not in df_asn.columns:\n",
    "    raise KeyError(\"assignments CSV must contain 'bin' and 'matched_mz_list' columns\")\n",
    "\n",
    "# NEW columns to be added to assignments\n",
    "new_cols = [\"group_0_sum\", \"group_1_sum\", \"group_2_sum\", \"group_3_sum\",\n",
    "            \"n_mz_used\", \"n_mz_found\", \"missing_cast_columns\"]\n",
    "for c in new_cols:\n",
    "    if c in df_asn.columns:\n",
    "        # avoid accidental overwrite\n",
    "        df_asn.drop(columns=[c], inplace=True)\n",
    "\n",
    "# --------------------\n",
    "# Row-wise quantification\n",
    "# --------------------\n",
    "results = []\n",
    "for idx, row in df_asn.iterrows():\n",
    "    bin_value = float(row[\"bin\"])\n",
    "    mz_list = parse_mz_list(row[\"matched_mz_list\"])\n",
    "    cast_cols = [to_cast_col(mz) for mz in mz_list]\n",
    "\n",
    "    # Filter dataset_rt to this bin\n",
    "    df_bin = df_rt[df_rt[\"bin\"] == bin_value]\n",
    "    if df_bin.empty:\n",
    "        res = dict(\n",
    "            group_0_sum=float(\"nan\"),\n",
    "            group_1_sum=float(\"nan\"),\n",
    "            group_2_sum=float(\"nan\"),\n",
    "            group_3_sum=float(\"nan\"),\n",
    "            n_mz_used=len(cast_cols),\n",
    "            n_mz_found=0,\n",
    "            missing_cast_columns=\", \".join(cast_cols) if cast_cols else \"\"\n",
    "        )\n",
    "        results.append(res)\n",
    "        continue\n",
    "\n",
    "    # Ensure target present\n",
    "    if \"target\" not in df_bin.columns:\n",
    "        raise KeyError(\"Column 'target' not found in dataset_rt.csv\")\n",
    "\n",
    "    existing = [c for c in cast_cols if c in df_bin.columns]\n",
    "    missing = [c for c in cast_cols if c not in df_bin.columns]\n",
    "\n",
    "    if not existing:\n",
    "        sums = {0: float(\"nan\"), 1: float(\"nan\"), 2: float(\"nan\"), 3: float(\"nan\")}\n",
    "    else:\n",
    "        # Sum intensities across all selected cast_* columns per target\n",
    "        grouped = df_bin.groupby(\"target\")[existing].sum()\n",
    "        total_per_target = grouped.sum(axis=1)  # sum across those cast_* columns\n",
    "        sums = {t: float(total_per_target.get(t, float(\"nan\"))) for t in [0, 1, 2, 3]}\n",
    "\n",
    "    res = dict(\n",
    "        group_0_sum=sums[0],\n",
    "        group_1_sum=sums[1],\n",
    "        group_2_sum=sums[2],\n",
    "        group_3_sum=sums[3],\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "# Attach results\n",
    "df_quant = pd.DataFrame(results, index=df_asn.index)\n",
    "df_asn_out = pd.concat([df_asn, df_quant], axis=1)\n",
    "\n",
    "# --------------------\n",
    "# Save updated CSV\n",
    "# --------------------\n",
    "df_asn_out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Saved: {OUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc503a",
   "metadata": {},
   "source": [
    "identification of PFR by matching with tdportal report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combine charge-assignment summary with best matches from a databank,\n",
    "supporting MULTIPLE (rt_window, mz_tol, mass_tol) triplets in one run.\n",
    "\n",
    "- Reads:\n",
    "    df1: assignments_summary (must have: neutral_mass, bin (or 'bin '), matched_mz_list)\n",
    "    df2: databank_with_ids (must have: rt_aligned, precursor_mz, MASS, Accession, PFR)\n",
    "\n",
    "- For each row in df1, for each m/z in matched_mz_list:\n",
    "    find the single best df2 row where ALL hold:\n",
    "        |rt_aligned - bin|    <= rt_window\n",
    "        |precursor_mz - m/z|  <= mz_tol\n",
    "        |MASS - neutral_mass| <= mass_tol\n",
    "  Then format:\n",
    "      best_match_* : \"[<mz>: <Accession>, <MASS_from_df2>, <PFR>] ...\"  (PFR optional)\n",
    "      matched_pfr_*: \"[<PFR_or_null_per_mz> ...]\"  (aligned with matched_mz_list)\n",
    "      mode_pfr_*   : most common non-null PFR across matches (per row)\n",
    "      mode_pfr_count_* : frequency (count) of that PFR (non-null only)\n",
    "      mode_accession_* : most common Accession across matches (per row), shown only if mode_pfr_count_* >= MIN_MODE_PFR_COUNT\n",
    "\n",
    "- Outputs:\n",
    "    One CSV with multiple columns per tolerance triplet:\n",
    "      best_match_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      matched_pfr_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_pfr_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_pfr_count_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_accession_rt<RT>_mz<MZ>_mass<MASS>\n",
    "\n",
    "Edit the 3 PATHS and the PARAM_SETS below before running.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import ast\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG: edit these paths\n",
    "# ----------------------------\n",
    "CHARGE_FILE_PATH = r\"F:/idbenchmark/assignments_with_quant_sums.csv\"\n",
    "DATABANK_PATH    = r\"F:/idbenchmark/databank_pfr_clean.csv\"\n",
    "OUTPUT_PATH      = r\"F:/idbenchmark/assignments_with_quant_sums_pfr.csv\"\n",
    "\n",
    "# Provide one or more (rt_window, mz_tol, mass_tol) triplets here.\n",
    "PARAM_SETS: List[Tuple[float, float, float]] = [\n",
    "    (55.0, 2.0, 90.0),\n",
    "    # (30.0, 1.0, 50.0),\n",
    "]\n",
    "\n",
    "# Keep \"null\" placeholders in matched_pfr_* so positions align with mz_list.\n",
    "PFR_KEEP_PLACEHOLDERS: bool = True\n",
    "\n",
    "# Minimum frequency required to report mode PFR and mode Accession.\n",
    "MIN_MODE_PFR_COUNT: int = 1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to numeric, invalid ‚Üí NaN.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _to_scalar(x: Any) -> Any:\n",
    "    \"\"\"Flatten 0-d arrays and coerce numeric strings to float when possible.\"\"\"\n",
    "    if isinstance(x, np.ndarray) and x.ndim == 0:\n",
    "        x = x.item()\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    \"\"\"Convert a string-repr list into a Python list of floats safely.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, required: List[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "\n",
    "def _fmt_suffix(v: float) -> str:\n",
    "    \"\"\"\n",
    "    Make a tidy string for column suffixes (avoid many decimals).\n",
    "    e.g., 10 -> '10', 2.0 -> '2', 1.5 -> '1p5'\n",
    "    \"\"\"\n",
    "    if float(v).is_integer():\n",
    "        return f\"{int(v)}\"\n",
    "    # Replace '.' with 'p' to keep it column-name friendly\n",
    "    return str(v).replace('.', 'p')\n",
    "\n",
    "\n",
    "def _mode_or_none(items: List[Any]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Return the most common value in `items` excluding None/NaN.\n",
    "    If tie, Counter.most_common returns first encountered top count.\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for v in items:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, float) and np.isnan(v):\n",
    "            continue\n",
    "        clean.append(v)\n",
    "    if not clean:\n",
    "        return None\n",
    "    return Counter(clean).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def _mode_and_count(items: List[Any]) -> Tuple[Optional[Any], int]:\n",
    "    \"\"\"\n",
    "    Return (mode_value, count) over non-null items.\n",
    "    If no non-null items, returns (None, 0).\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for v in items:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, float) and np.isnan(v):\n",
    "            continue\n",
    "        clean.append(v)\n",
    "    if not clean:\n",
    "        return None, 0\n",
    "    val, cnt = Counter(clean).most_common(1)[0]\n",
    "    return val, int(cnt)\n",
    "\n",
    "\n",
    "def _mode_and_count_with_cutoff(\n",
    "    pfrs: List[Any], accs: List[Any], min_count: int\n",
    ") -> Tuple[Optional[Any], int, Optional[Any]]:\n",
    "    \"\"\"\n",
    "    Return (mode_pfr, count, mode_accession) applying a frequency cutoff on PFR.\n",
    "    If the mode PFR count < min_count, return (None, 0, None).\n",
    "    Otherwise return (mode_pfr, count, mode_accession).\n",
    "    \"\"\"\n",
    "    pfr_val, pfr_cnt = _mode_and_count(pfrs)\n",
    "    acc_val = _mode_or_none(accs)\n",
    "\n",
    "    if pfr_cnt < min_count:\n",
    "        return None, 0, None\n",
    "    return pfr_val, pfr_cnt, acc_val\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Core search\n",
    "# ----------------------------\n",
    "def search_best(\n",
    "    df2: pd.DataFrame,\n",
    "    rt_query: float,\n",
    "    mz_query: float,\n",
    "    mass_query: float,\n",
    "    rt_window: float,\n",
    "    mz_tol: float,\n",
    "    mass_tol: float\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Return the single best match (row as dict) if ALL three criteria match:\n",
    "      |rt - rt_query| <= rt_window\n",
    "      |mz - mz_query| <= mz_tol\n",
    "      |mass - mass_query| <= mass_tol\n",
    "    Otherwise returns None.\n",
    "    \"\"\"\n",
    "    d_rt   = (df2[\"rt_aligned\"] - float(rt_query)).abs()\n",
    "    d_mz   = (df2[\"precursor_mz\"] - float(mz_query)).abs()\n",
    "    d_mass = (df2[\"MASS\"] - float(mass_query)).abs()\n",
    "\n",
    "    mask = (d_rt <= rt_window) & (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "    if not mask.any():\n",
    "        return None\n",
    "\n",
    "    cand = df2.loc[mask].copy()\n",
    "    cand[\"score\"] = (\n",
    "        d_rt.loc[cand.index] / max(rt_window, 1e-12) +\n",
    "        d_mz.loc[cand.index] / max(mz_tol, 1e-12) +\n",
    "        d_mass.loc[cand.index] / max(mass_tol, 1e-12)\n",
    "    )\n",
    "    best_row = cand.sort_values(\"score\", kind=\"mergesort\").iloc[0]\n",
    "    return best_row.to_dict()\n",
    "\n",
    "\n",
    "# ---------- per-row collectors (single pass across m/z list) ----------\n",
    "def _collect_matches_for_row(\n",
    "    row: pd.Series,\n",
    "    df2: pd.DataFrame,\n",
    "    rt_window: float,\n",
    "    mz_tol: float,\n",
    "    mass_tol: float\n",
    ") -> Tuple[List[str], List[Optional[Any]], List[Optional[str]], List[Optional[float]]]:\n",
    "    \"\"\"\n",
    "    For a df1 row, iterate over matched_mz_list and collect:\n",
    "      - mz_tokens for best_match string (aligned, with placeholders)\n",
    "      - pfr_list  (aligned, None for missing/NaN)\n",
    "      - acc_list  (aligned, None for no match)\n",
    "      - mass_list (aligned, MASS from df2 if matched, else None)\n",
    "\n",
    "    Returns (best_match_tokens, pfr_list, acc_list, mass_list).\n",
    "    \"\"\"\n",
    "    neutral_mass   = row.get(\"neutral_mass\", np.nan)\n",
    "    retention_time = row.get(\"bin\", row.get(\"bin \", np.nan))\n",
    "    mz_list        = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "\n",
    "    if pd.isna(neutral_mass) or pd.isna(retention_time) or not mz_list:\n",
    "        return [], [], [], []\n",
    "\n",
    "    tokens: List[str] = []\n",
    "    pfrs:   List[Optional[Any]]   = []\n",
    "    accs:   List[Optional[str]]   = []\n",
    "    masses: List[Optional[float]] = []\n",
    "\n",
    "    for mz_value in mz_list:\n",
    "        res = search_best(\n",
    "            df2,\n",
    "            rt_query=float(retention_time),\n",
    "            mz_query=float(mz_value),\n",
    "            mass_query=float(neutral_mass),\n",
    "            rt_window=rt_window,\n",
    "            mz_tol=mz_tol,\n",
    "            mass_tol=mass_tol,\n",
    "        )\n",
    "        if res is not None:\n",
    "            uniprot_id = res.get(\"Accession\", \"NA\")\n",
    "            mass_match = _to_scalar(res.get(\"MASS\", neutral_mass))\n",
    "            pfr_val    = _to_scalar(res.get(\"PFR\", None))\n",
    "            if pfr_val is None or (isinstance(pfr_val, float) and np.isnan(pfr_val)):\n",
    "                tokens.append(f\"{mz_value}: {uniprot_id}, {mass_match}\")\n",
    "                pfrs.append(None)\n",
    "            else:\n",
    "                tokens.append(f\"{mz_value}: {uniprot_id}, {mass_match}, {pfr_val}\")\n",
    "                pfrs.append(pfr_val)\n",
    "            accs.append(uniprot_id)\n",
    "            masses.append(mass_match)\n",
    "        else:\n",
    "            tokens.append(f\"{mz_value}: NA\")\n",
    "            pfrs.append(None)\n",
    "            accs.append(None)\n",
    "            masses.append(None)\n",
    "\n",
    "    return tokens, pfrs, accs, masses\n",
    "\n",
    "\n",
    "def best_match_formatter_from_tokens(tokens: List[str]) -> Optional[str]:\n",
    "    if not tokens:\n",
    "        return None\n",
    "    return \"[\" + \", \".join(tokens) + \"]\"\n",
    "\n",
    "\n",
    "def matched_pfr_from_list(pfrs: List[Optional[Any]], keep_placeholders: bool) -> Optional[str]:\n",
    "    if not pfrs:\n",
    "        return None\n",
    "    if keep_placeholders:\n",
    "        return \"[\" + \", \".join(\"null\" if v is None else str(v) for v in pfrs) + \"]\"\n",
    "    else:\n",
    "        pruned = [str(v) for v in pfrs if v is not None]\n",
    "        return \"[\" + \", \".join(pruned) + \"]\" if pruned else None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load CSVs\n",
    "    if not os.path.exists(CHARGE_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {CHARGE_FILE_PATH}\")\n",
    "    if not os.path.exists(DATABANK_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {DATABANK_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(CHARGE_FILE_PATH)\n",
    "    df2 = pd.read_csv(DATABANK_PATH)\n",
    "\n",
    "    # Normalize df1 column names to handle accidental trailing spaces, capitalization, etc.\n",
    "    df1.columns = [c.strip() for c in df1.columns]\n",
    "\n",
    "    # Ensure required columns in both tables (with tolerant check for 'bin' / 'bin ')\n",
    "    need_df1 = [\"neutral_mass\", \"matched_mz_list\"]\n",
    "    _ensure_columns(df1, need_df1)\n",
    "    if \"bin\" not in df1.columns and \"bin \" not in df1.columns:\n",
    "        raise KeyError(\"df1 must contain 'bin' (or 'bin ').\")\n",
    "\n",
    "    # Ensure essential df2 columns (PFR required for the new output)\n",
    "    _ensure_columns(df2, [\"rt_aligned\", \"precursor_mz\", \"MASS\", \"Accession\", \"PFR\"])\n",
    "\n",
    "    # If df1 had 'bin ' originally, create 'bin' as an alias to simplify downstream code\n",
    "    if \"bin\" not in df1.columns and \"bin \" in df1.columns:\n",
    "        df1[\"bin\"] = df1[\"bin \"]\n",
    "\n",
    "    # Pre-coerce df2 numerics once (for speed)\n",
    "    df2 = df2.copy()\n",
    "    df2[\"rt_aligned\"]   = _num(df2[\"rt_aligned\"])\n",
    "    df2[\"precursor_mz\"] = _num(df2[\"precursor_mz\"])\n",
    "    df2[\"MASS\"]         = _num(df2[\"MASS\"])\n",
    "    # PFR can be numeric or categorical; try to coerce but keep strings if not\n",
    "    try:\n",
    "        df2[\"PFR\"] = pd.to_numeric(df2[\"PFR\"], errors=\"ignore\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Build output columns per tolerance triplet\n",
    "    for (rt_w, mz_t, mass_t) in PARAM_SETS:\n",
    "        suffix = f\"rt{_fmt_suffix(rt_w)}_mz{_fmt_suffix(mz_t)}_mass{_fmt_suffix(mass_t)}\"\n",
    "\n",
    "        match_col   = f\"best_match_{suffix}\"\n",
    "        pfr_col     = f\"matched_pfr_{suffix}\"\n",
    "        mode_pfr    = f\"mode_pfr_{suffix}\"\n",
    "        mode_pfr_n  = f\"mode_pfr_count_{suffix}\"\n",
    "        mode_acc    = f\"mode_accession_{suffix}\"\n",
    "\n",
    "        best_tokens_series: List[List[str]] = []\n",
    "        pfr_list_series:    List[List[Optional[Any]]] = []\n",
    "        acc_list_series:    List[List[Optional[str]]] = []\n",
    "\n",
    "        # Compute per-row tokens and lists in one pass\n",
    "        for _, row in df1.iterrows():\n",
    "            tokens, pfrs, accs, _masses = _collect_matches_for_row(\n",
    "                row, df2, rt_window=rt_w, mz_tol=mz_t, mass_tol=mass_t\n",
    "            )\n",
    "            best_tokens_series.append(tokens)\n",
    "            pfr_list_series.append(pfrs)\n",
    "            acc_list_series.append(accs)\n",
    "\n",
    "        # Populate columns\n",
    "        df1[match_col] = [best_match_formatter_from_tokens(toks) for toks in best_tokens_series]\n",
    "        df1[pfr_col]   = [matched_pfr_from_list(pfrs, keep_placeholders=PFR_KEEP_PLACEHOLDERS)\n",
    "                          for pfrs in pfr_list_series]\n",
    "\n",
    "        # Most common PFR + its frequency (with cutoff), and most common Accession (masked if cutoff not met)\n",
    "        mode_results = [\n",
    "            _mode_and_count_with_cutoff(pfrs, accs, min_count=MIN_MODE_PFR_COUNT)\n",
    "            for pfrs, accs in zip(pfr_list_series, acc_list_series)\n",
    "        ]\n",
    "        df1[mode_pfr]   = [mr[0] for mr in mode_results]\n",
    "        df1[mode_pfr_n] = [mr[1] for mr in mode_results]\n",
    "        df1[mode_acc]   = [mr[2] for mr in mode_results]\n",
    "\n",
    "    # Save single CSV containing all columns\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(\n",
    "        f\"Saved with {len(PARAM_SETS)} sets of columns \"\n",
    "        f\"(best_match / matched_pfr / mode_pfr / mode_pfr_count / mode_accession) ‚Üí {OUTPUT_PATH}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
