{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104e0c9",
   "metadata": {},
   "source": [
    "Generating deconvoluted spectra from the informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975db4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def _unique_dst_path(dst_dir, fname):\n",
    "    \"\"\"Return a unique path in dst_dir for fname, adding a numeric suffix if needed.\"\"\"\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    candidate = os.path.join(dst_dir, fname)\n",
    "    i = 1\n",
    "    while os.path.exists(candidate):\n",
    "        candidate = os.path.join(dst_dir, f\"{base}__{i}{ext}\")\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "def _prefixed_name(src_path, result_root):\n",
    "    \"\"\"\n",
    "    Build a safer filename using the immediate parent folder under result/ as a prefix\n",
    "    to reduce collisions: e.g., result/sampleA/sampleA_mass.txt -> sampleA__sampleA_mass.txt\n",
    "    \"\"\"\n",
    "    # src_path like .../result/<parent>/<file>\n",
    "    parent = os.path.basename(os.path.dirname(src_path))\n",
    "    fname = os.path.basename(src_path)\n",
    "    return f\"{parent}__{fname}\" if parent and parent != \"result\" else fname\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder (top-level only)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} ‚Üí {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"‚úÖ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "    # 1) Collect *_mass.txt paths from result_root (including subfolders)\n",
    "    collected = []\n",
    "    for root, _, files in os.walk(result_root):\n",
    "        for f in files:\n",
    "            if f.endswith(\"_mass.txt\"):\n",
    "                collected.append(os.path.join(root, f))\n",
    "\n",
    "    if not collected:\n",
    "        print(\"‚ö†Ô∏è No *_mass.txt files found under:\", result_root)\n",
    "        return\n",
    "\n",
    "    # 2) Copy them to a temp folder FIRST (so deleting result/ content won't break src paths)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"mass_collect_\")\n",
    "    copied = []\n",
    "    for src in collected:\n",
    "        try:\n",
    "            # Prefix with subfolder name to avoid collisions\n",
    "            safe_name = _prefixed_name(src, result_root)\n",
    "            dst = os.path.join(temp_dir, safe_name)\n",
    "            dst = _unique_dst_path(temp_dir, os.path.basename(dst))  # ensure uniqueness\n",
    "            print(f\"Staging: {src} ‚Üí {dst}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            copied.append(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skip (copy error): {src} ‚Äî {e}\")\n",
    "\n",
    "    # 3) Clean the result_root completely\n",
    "    for item in os.listdir(result_root):\n",
    "        item_path = os.path.join(result_root, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove {item_path}: {e}\")\n",
    "\n",
    "    # 4) Move staged files back into a clean result_root\n",
    "    for staged in copied:\n",
    "        try:\n",
    "            final_dst = os.path.join(result_root, os.path.basename(staged))\n",
    "            final_dst = _unique_dst_path(result_root, os.path.basename(final_dst))\n",
    "            print(f\"Finalizing: {staged} ‚Üí {final_dst}\")\n",
    "            shutil.move(staged, final_dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Move error for {staged}: {e}\")\n",
    "\n",
    "    # 5) Remove temp dir (ignore errors)\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"üìÇ Clean result folder ready with only *_mass.txt files:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\test\\resr\\group_compare_only\\bin_5\\csv\"  # <-- replace with your folder\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9152b0",
   "metadata": {},
   "source": [
    "Discovery of proteoforms with S/N more than 10, and matching them with their charge states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f75b343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Neutral-mass detection] 62 peaks ‚Üí F:\\test\\5__pos_1__neg_0_pos_runA_mass_detected_signals.csv\n",
      "[Neutral-mass detection] Plot saved ‚Üí F:\\test\\5__pos_1__neg_0_pos_runA_mass_detected_signals.png\n",
      "Parsed filename metadata: {'bin': 5, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': '5__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:/new\\neutral_mass_spectrum.png\n",
      "Saved plot: F:/new\\mirror_assigned_vs_total.png\n",
      "Saved plot: F:/new\\mirror_unassigned_vs_total.png\n",
      "Saved plot: F:/new\\mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== Summary ===\n",
      "Raw MS1 peaks (rows): 7,346\n",
      "Detected neutral-mass peaks: 62\n",
      "Assigned raw peaks: 1,352\n",
      "Non-assigned raw peaks: 5,994\n",
      "Saved: F:/new\\assigned_ms1_with_peaks.csv\n",
      "Saved: F:/new\\assignments_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pipeline:\n",
    "1) Detect neutral-mass peaks from a deconvoluted spectrum (mass intensity; whitespace- or csv-delimited).\n",
    "2) Use detected peaks as candidate proteins and assign raw MS1 peaks by charge-series matching.\n",
    "\n",
    "Outputs:\n",
    "- <deconv_stem>_detected_signals.csv / .png   (neutral-mass peak picks + metadata, includes SNR)\n",
    "- OUT_DIR/assigned_ms1_with_peaks.csv         (annotated raw MS1 with assigned_mass/charge)\n",
    "- OUT_DIR/assignments_summary.csv             (one row per accepted neutral mass; now includes SNR)\n",
    "- OUT_DIR/<several_plots>.png                 (mirror plots + neutral-mass spectrum)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --------------------  PEAK DETECTION  ----------------------\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakFindingParams:\n",
    "    min_prominence: float | None = None\n",
    "    min_height: float | None = None\n",
    "    min_distance_pts: int = 10\n",
    "    smooth_window: int = 0\n",
    "    min_snr: float = 0.0\n",
    "\n",
    "\n",
    "def _mad_sigma(y: np.ndarray) -> float:\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    med = np.median(y)\n",
    "    mad = np.median(np.abs(y - med))\n",
    "    return 1.4826 * mad\n",
    "\n",
    "\n",
    "def _smooth(y: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window < 3 or window % 2 == 0:\n",
    "        return y\n",
    "    kernel = np.ones(window, dtype=float) / window\n",
    "    return np.convolve(y, kernel, mode=\"same\")\n",
    "\n",
    "\n",
    "def _extract_id_list(name: str, key: str) -> list[int] | None:\n",
    "    \"\"\"\n",
    "    Extract an underscore- or hyphen-separated list of integers after a key.\n",
    "    Examples:\n",
    "      \"__pos_3__\"          -> [3]\n",
    "      \"__neg_0_1_2_\"       -> [0,1,2]\n",
    "      \"-pos-10-11\"         -> [10,11]\n",
    "    \"\"\"\n",
    "    m = re.search(rf\"(?:^|[_-]){key}((?:[_-]\\d+)+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if not m:\n",
    "        m1 = re.search(rf\"(?:^|[_-]){key}[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "        if m1:\n",
    "            return [int(m1.group(1))]\n",
    "        return None\n",
    "    parts = re.findall(r\"\\d+\", m.group(1))\n",
    "    return [int(x) for x in parts] if parts else None\n",
    "\n",
    "\n",
    "def parse_metadata_from_filename(path: str | Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract bin, experiments/controls (IDs + counts), regulation, replicate, source_file.\n",
    "    Regulation token is taken as the LAST occurrence among (negabs|posabs|neg|pos).\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    name = p.stem\n",
    "\n",
    "    meta = {\n",
    "        \"bin\": None,\n",
    "        \"experiments\": None,\n",
    "        \"controls\": None,\n",
    "        \"experiments_ids\": None,\n",
    "        \"controls_ids\": None,\n",
    "        \"experiments_n\": None,\n",
    "        \"controls_n\": None,\n",
    "        \"regulation\": None,\n",
    "        \"replicate\": None,\n",
    "        \"source_file\": p.name,\n",
    "    }\n",
    "\n",
    "    # bin: \"bin5\"/\"bin_5\" or a leading number \"75__pos_...\"\n",
    "    m = re.search(r\"(?:^|[_-])bin[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if m:\n",
    "        meta[\"bin\"] = int(m.group(1))\n",
    "    else:\n",
    "        m2 = re.match(r\"^(\\d+)(?=[_-])\", name)\n",
    "        if m2:\n",
    "            meta[\"bin\"] = int(m2.group(1))\n",
    "\n",
    "    exp_ids = _extract_id_list(name, \"pos\")\n",
    "    ctl_ids = _extract_id_list(name, \"neg\")\n",
    "    if exp_ids is not None:\n",
    "        meta[\"experiments_ids\"] = \",\".join(str(x) for x in exp_ids)\n",
    "        meta[\"experiments_n\"] = len(exp_ids)\n",
    "        meta[\"experiments\"] = len(exp_ids)\n",
    "    if ctl_ids is not None:\n",
    "        meta[\"controls_ids\"] = \",\".join(str(x) for x in ctl_ids)\n",
    "        meta[\"controls_n\"] = len(ctl_ids)\n",
    "        meta[\"controls\"] = len(ctl_ids)\n",
    "\n",
    "    reg_tokens = [m.group(1).lower() for m in re.finditer(\n",
    "        r\"(?:^|[_-])(negabs|posabs|neg|pos)(?=[_-]|$)\", name, flags=re.I\n",
    "    )]\n",
    "    if reg_tokens:\n",
    "        token = reg_tokens[-1]\n",
    "        reg_map = {\"negabs\": \"downregulated\", \"neg\": \"downregulated\",\n",
    "                   \"posabs\": \"upregulated\", \"pos\": \"upregulated\"}\n",
    "        meta[\"regulation\"] = reg_map.get(token)\n",
    "\n",
    "    m = re.search(r\"(?:^|[_-])run([A-Za-z])(?=[_-]|$)\", name)\n",
    "    if m:\n",
    "        meta[\"replicate\"] = m.group(1).upper()\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def load_space_separated(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    # whitespace-delimited by default; fallback to CSV\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", header=None,\n",
    "                         names=[\"mass\", \"intensity\"], comment=\"#\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"mass\", \"intensity\"]\n",
    "        else:\n",
    "            raise ValueError(\"Deconvoluted file must have at least two columns: mass intensity\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_signals(\n",
    "    df: pd.DataFrame,\n",
    "    params: PeakFindingParams = PeakFindingParams()\n",
    ") -> pd.DataFrame:\n",
    "    # normalize columns\n",
    "    if not {\"mass\", \"intensity\"}.issubset(df.columns):\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.copy()\n",
    "            df.columns = [\"mass\", \"intensity\"] + [f\"col{i}\" for i in range(2, df.shape[1])]\n",
    "        else:\n",
    "            raise ValueError(\"Input DataFrame must have columns ['mass','intensity'].\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"mass\", \"intensity\"])\n",
    "    df = df.sort_values(\"mass\").reset_index(drop=True)\n",
    "\n",
    "    x = df[\"mass\"].to_numpy(float)\n",
    "    y = df[\"intensity\"].to_numpy(float)\n",
    "\n",
    "    y_proc = _smooth(y, params.smooth_window)\n",
    "\n",
    "    sigma = _mad_sigma(y_proc)\n",
    "    ymax = float(np.max(y_proc)) if y_proc.size else 0.0\n",
    "\n",
    "    min_prom = params.min_prominence or max(6.0 * sigma, 0.001 * ymax)\n",
    "    min_h    = params.min_height     or max(4.0 * sigma, 0.0005 * ymax)\n",
    "\n",
    "    peaks, props = find_peaks(\n",
    "        y_proc,\n",
    "        prominence=min_prom,\n",
    "        height=min_h,\n",
    "        distance=max(1, int(params.min_distance_pts))\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mass\": x[peaks],\n",
    "        \"intensity\": y[peaks],\n",
    "        \"prominence\": props.get(\"prominences\", np.full(peaks.shape, np.nan)),\n",
    "        \"left_base_idx\": props.get(\"left_bases\", np.full(peaks.shape, -1)),\n",
    "        \"right_base_idx\": props.get(\"right_bases\", np.full(peaks.shape, -1)),\n",
    "    })\n",
    "\n",
    "    # SNR estimate and filter\n",
    "    snr_den = sigma if sigma > 0 else (np.std(y_proc) if y_proc.size else 1.0)\n",
    "    snr_den = snr_den if snr_den > 0 else 1.0\n",
    "    out[\"snr\"] = out[\"intensity\"] / snr_den\n",
    "\n",
    "    if params.min_snr > 0:\n",
    "        out = out[out[\"snr\"] >= params.min_snr].reset_index(drop=True)\n",
    "\n",
    "    return out.sort_values(\"intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def plot_spectrum_with_peaks(\n",
    "    df: pd.DataFrame,\n",
    "    peaks_df: pd.DataFrame,\n",
    "    out_png: str | Path | None = None,\n",
    "    title: str = \"Detected Neutral-Mass Signals\"\n",
    ") -> None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"mass\"].to_numpy(), df[\"intensity\"].to_numpy(), linewidth=1)\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        plt.scatter(\n",
    "            peaks_df[\"mass\"].to_numpy(),\n",
    "            peaks_df[\"intensity\"].to_numpy(),  # use peaks‚Äô intensities\n",
    "            s=18\n",
    "        )\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# -----------------  CHARGE-SERIES MATCHING  -----------------\n",
    "# ============================================================\n",
    "\n",
    "# Matching parameters (tweak as needed)\n",
    "PROTON_MASS = 1.007276466812  # Da\n",
    "\n",
    "Z_MIN, Z_MAX = 5, 50\n",
    "PPM_TOL = 1000.0              # ppm window for m/z match\n",
    "ABS_DA_TOL = 1.0              # absolute Da floor (used with ppm)\n",
    "MIN_MATCHED_CHARGE_STATES = 4 # require ‚â• N charge-state hits to accept a protein\n",
    "\n",
    "\n",
    "def _read_raw_ms1(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly read raw MS1 CSV. Expected two columns (m/z, intensity), with or without headers.\n",
    "    If more columns exist, pick the best 'mz' and 'intensity' columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "\n",
    "    if df.shape[1] == 2:\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "    else:\n",
    "        cols_lower = [str(c).lower() for c in df.columns]\n",
    "        mz_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                         if (\"mz\" in c) or (\"m/z\" in c) or (\"mass/charge\" in c) or (c.strip() == \"m z\")]\n",
    "        int_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                          if (\"int\" in c) or (\"abund\" in c) or (\"height\" in c) or (\"signal\" in c)]\n",
    "        if not mz_candidates:\n",
    "            mz_candidates = [0]\n",
    "        if not int_candidates:\n",
    "            int_candidates = [1 if df.shape[1] > 1 else 0]\n",
    "        df = df.iloc[:, [mz_candidates[0], int_candidates[0]]].copy()\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df = df[df[\"intensity\"] > 0].copy()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"mz\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ppm_window(target_mz: float, ppm: float, abs_da: float) -> tuple[float, float]:\n",
    "    da = target_mz * ppm * 1e-6\n",
    "    tol = max(da, abs_da)\n",
    "    return target_mz - tol, target_mz + tol\n",
    "\n",
    "\n",
    "def _match_targets(sorted_mz: np.ndarray, targets: np.ndarray,\n",
    "                   ppm: float, abs_da: float, available_mask: np.ndarray) -> dict[int, int | None]:\n",
    "    results: dict[int, int | None] = {}\n",
    "    for ti, t in enumerate(targets):\n",
    "        lo, hi = _ppm_window(t, ppm, abs_da)\n",
    "        j = bisect_left(sorted_mz, t)\n",
    "        best_idx = None\n",
    "        best_delta = float(\"inf\")\n",
    "        for k in (j, j-1, j+1, j-2, j+2, j-3, j+3):\n",
    "            if 0 <= k < len(sorted_mz):\n",
    "                mz_k = sorted_mz[k]\n",
    "                if available_mask[k] and (lo <= mz_k <= hi):\n",
    "                    delta = abs(mz_k - t)\n",
    "                    if delta < best_delta:\n",
    "                        best_delta = delta\n",
    "                        best_idx = k\n",
    "        results[ti] = best_idx\n",
    "    return results\n",
    "\n",
    "\n",
    "def _generate_charge_series(neutral_mass: float, z_min: int, z_max: int) -> pd.DataFrame:\n",
    "    z = np.arange(z_min, z_max + 1, dtype=int)\n",
    "    mz = (neutral_mass + z * PROTON_MASS) / z\n",
    "    return pd.DataFrame({\"z\": z, \"target_mz\": mz})\n",
    "\n",
    "\n",
    "def assign_ms1_peaks(raw_df: pd.DataFrame, deconv_peaks_df: pd.DataFrame,\n",
    "                     meta: dict | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assign raw MS1 peaks to detected neutral masses (from deconvoluted spectrum) by charge-series matching.\n",
    "\n",
    "    Returns:\n",
    "      assigned_raw: raw_df with columns [assigned_mass, assigned_charge, is_assigned]\n",
    "      assignments_summary: one row per accepted neutral mass with metadata, includes SNR\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.sort_values(\"mz\").reset_index(drop=True)\n",
    "    mz_arr = raw_df[\"mz\"].to_numpy()\n",
    "    inten_arr = raw_df[\"intensity\"].to_numpy()\n",
    "    available = np.ones(len(raw_df), dtype=bool)\n",
    "\n",
    "    assigned_mass = np.full(len(raw_df), np.nan)\n",
    "    assigned_z    = np.full(len(raw_df), np.nan)\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for r in deconv_peaks_df.itertuples(index=False):\n",
    "        mass = float(r.mass)\n",
    "        mass_intensity = float(r.intensity)\n",
    "        mass_snr = float(getattr(r, \"snr\", np.nan))  # <-- carry SNR into the summary\n",
    "\n",
    "        series = _generate_charge_series(mass, Z_MIN, Z_MAX)\n",
    "        targets = series[\"target_mz\"].to_numpy()\n",
    "\n",
    "        matches = _match_targets(mz_arr, targets, PPM_TOL, ABS_DA_TOL, available_mask=available)\n",
    "\n",
    "        matched_indices = []\n",
    "        matched_z_list  = []\n",
    "        matched_mz_list = []\n",
    "\n",
    "        for ti, k in matches.items():\n",
    "            if k is not None:\n",
    "                matched_indices.append(k)\n",
    "                matched_z_list.append(int(series.iloc[ti][\"z\"]))\n",
    "                matched_mz_list.append(mz_arr[k])\n",
    "\n",
    "        if len(matched_indices) >= MIN_MATCHED_CHARGE_STATES:\n",
    "            # accept and mark assigned\n",
    "            for idx, z_val in zip(matched_indices, matched_z_list):\n",
    "                if available[idx]:\n",
    "                    available[idx] = False\n",
    "                    assigned_mass[idx] = mass\n",
    "                    assigned_z[idx] = z_val\n",
    "\n",
    "            frac_intensity_removed = (\n",
    "                float(np.sum(inten_arr[matched_indices])) / float(np.sum(inten_arr))\n",
    "                if inten_arr.sum() > 0 else 0.0\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"neutral_mass\": mass,\n",
    "                \"deconv_intensity\": mass_intensity,\n",
    "                \"snr\": mass_snr,  # <-- new column in assignments_summary\n",
    "                \"n_matches\": len(matched_indices),\n",
    "                \"matched_z_list\": json.dumps(matched_z_list),\n",
    "                \"matched_mz_list\": json.dumps([round(float(x), 1) for x in matched_mz_list]),\n",
    "                \"ppm_tol\": PPM_TOL,\n",
    "                \"abs_da_tol\": ABS_DA_TOL,\n",
    "                \"fraction_total_intensity_captured\": frac_intensity_removed\n",
    "            }\n",
    "            # attach filename metadata if available\n",
    "            if meta:\n",
    "                row.update({\n",
    "                    \"bin\": meta.get(\"bin\"),\n",
    "                    \"experiments_ids\": meta.get(\"experiments_ids\"),\n",
    "                    \"controls_ids\": meta.get(\"controls_ids\"),\n",
    "                    \"regulation\": meta.get(\"regulation\"),\n",
    "                    \"replicate\": meta.get(\"replicate\"),\n",
    "                    \"source_file\": meta.get(\"source_file\"),\n",
    "                })\n",
    "            summary_rows.append(row)\n",
    "\n",
    "    assigned_raw = raw_df.copy()\n",
    "    assigned_raw[\"assigned_mass\"] = assigned_mass\n",
    "    assigned_raw[\"assigned_charge\"] = assigned_z\n",
    "    assigned_raw[\"is_assigned\"] = ~np.isnan(assigned_mass)\n",
    "\n",
    "    assignments_summary = pd.DataFrame(summary_rows).sort_values(\n",
    "        \"deconv_intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # if meta present, also add bin to assigned_raw (useful downstream)\n",
    "    if meta and \"bin\" in meta:\n",
    "        assigned_raw[\"bin\"] = meta[\"bin\"]\n",
    "\n",
    "    return assigned_raw, assignments_summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  PLOTTING  --------------------------\n",
    "# ============================================================\n",
    "\n",
    "def plot_neutral_mass_spectrum(deconv_peaks_df: pd.DataFrame, out_dir: str,\n",
    "                               filename: str = \"neutral_mass_spectrum.png\"):\n",
    "    if deconv_peaks_df.empty:\n",
    "        print(\"No neutral masses to plot.\")\n",
    "        return\n",
    "    masses = deconv_peaks_df[\"mass\"].to_numpy()\n",
    "    intens = deconv_peaks_df[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(9, 4.5))\n",
    "    plt.vlines(masses, 0, intens, linewidth=1)\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Neutral Mass Spectrum (detected peaks)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                  filename: str = \"mirror_assigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    assigned_mask = assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    assigned_int = np.where(assigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                 # Top: total\n",
    "    plt.vlines(mz[assigned_mask], 0, -assigned_int[assigned_mask], linewidth=0.8)  # Bottom: assigned (neg)\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -assigned_int.max() if assigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_unassigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                    filename: str = \"mirror_unassigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    unassigned_mask = ~assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    unassigned_int = np.where(unassigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                     # Top: total\n",
    "    plt.vlines(mz[unassigned_mask], 0, -unassigned_int[unassigned_mask], linewidth=0.8)  # Bottom: unassigned\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -unassigned_int.max() if unassigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Non-assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_by_protein_vs_total(\n",
    "    assigned_raw: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    filename: str = \"mirror_assigned_by_protein_vs_total.png\",\n",
    "    max_legend_items: int = 20\n",
    "):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(11, 5.6))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.5)  # top: total\n",
    "\n",
    "    df_assigned_only = assigned_raw[assigned_raw[\"is_assigned\"]].copy()\n",
    "    if df_assigned_only.empty:\n",
    "        plt.xlabel(\"m/z\")\n",
    "        plt.ylabel(\"Intensity (arb.)\")\n",
    "        plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {out_path}\")\n",
    "        return\n",
    "\n",
    "    counts = (\n",
    "        df_assigned_only.groupby(\"assigned_mass\", dropna=True)[\"is_assigned\"]\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    proteins_in_order = counts.index.tolist()\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get(\n",
    "        'color', ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "    )\n",
    "\n",
    "    legend_entries = 0\n",
    "    for i, mass in enumerate(proteins_in_order):\n",
    "        mask = (assigned_raw[\"assigned_mass\"] == mass)\n",
    "        mz_i = assigned_raw.loc[mask, \"mz\"].to_numpy()\n",
    "        inten_i = assigned_raw.loc[mask, \"intensity\"].to_numpy()\n",
    "\n",
    "        label = None\n",
    "        if legend_entries < max_legend_items:\n",
    "            label = f\"{mass/1000:.2f} kDa (n={len(mz_i)})\"\n",
    "            legend_entries += 1\n",
    "\n",
    "        plt.vlines(\n",
    "            mz_i, 0, -inten_i,\n",
    "            linewidth=0.8,\n",
    "            color=color_cycle[i % len(color_cycle)],\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -df_assigned_only[\"intensity\"].max() if len(df_assigned_only) else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    if legend_entries:\n",
    "        plt.legend(title=\"Assigned proteins\", loc=\"upper right\", fontsize=8, ncol=1)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  MAIN SCRIPT  -----------------------\n",
    "# ============================================================\n",
    "\n",
    "# ---- User-configurable paths ----\n",
    "RAW_MS1_CSV = r\"F:/test/5__pos_1__neg_0_pos_runA.csv\"        # raw MS1 (m/z, intensity)\n",
    "DECONV_TXT  = r\"F:/test/5__pos_1__neg_0_pos_runA_mass.txt\"   # deconvoluted neutral masses (mass intensity)\n",
    "OUT_DIR     = r\"F:/new\"\n",
    "\n",
    "# ---- Neutral-mass peak detection parameters ----\n",
    "DECONV_DETECT_PARAMS = PeakFindingParams(\n",
    "    min_distance_pts=20,   # decon masses can be coarse; 2 is a good start\n",
    "    min_snr=10,          # enforce minimum SNR\n",
    "    smooth_window=0,      # set to 5/7 if your decon spectrum is very noisy\n",
    "    # min_prominence=None, min_height=None  # auto from MAD if None\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- 1) Load deconvoluted spectrum & detect neutral-mass peaks ---\n",
    "    meta = parse_metadata_from_filename(DECONV_TXT)\n",
    "    deconv_raw = load_space_separated(DECONV_TXT)\n",
    "    deconv_peaks = detect_signals(deconv_raw, params=DECONV_DETECT_PARAMS)\n",
    "\n",
    "    # attach filename metadata to neutral-mass peaks table for traceability\n",
    "    deconv_peaks = deconv_peaks.assign(\n",
    "        bin=meta.get(\"bin\"),\n",
    "        experiments_ids=meta.get(\"experiments_ids\"),\n",
    "        controls_ids=meta.get(\"controls_ids\"),\n",
    "        regulation=meta.get(\"regulation\"),\n",
    "        replicate=meta.get(\"replicate\"),\n",
    "        source_file=meta.get(\"source_file\"),\n",
    "    )\n",
    "\n",
    "    # Save + plot neutral-mass detections alongside the deconv file\n",
    "    in_path = Path(DECONV_TXT)\n",
    "    out_detect_csv = str(in_path.with_name(in_path.stem + \"_detected_signals.csv\"))\n",
    "    out_detect_png = str(in_path.with_name(in_path.stem + \"_detected_signals.png\"))\n",
    "    deconv_peaks.to_csv(out_detect_csv, index=False)\n",
    "    plot_spectrum_with_peaks(deconv_raw, deconv_peaks, out_png=out_detect_png)\n",
    "    print(f\"[Neutral-mass detection] {len(deconv_peaks)} peaks ‚Üí {out_detect_csv}\")\n",
    "    print(f\"[Neutral-mass detection] Plot saved ‚Üí {out_detect_png}\")\n",
    "    print(\"Parsed filename metadata:\", meta)\n",
    "\n",
    "    # --- 2) Read raw MS1 and assign charge-series to detected masses ---\n",
    "    raw_df = _read_raw_ms1(RAW_MS1_CSV)\n",
    "\n",
    "    assigned_raw, summary = assign_ms1_peaks(raw_df, deconv_peaks, meta=meta)\n",
    "\n",
    "    out_assigned = os.path.join(OUT_DIR, \"assigned_ms1_with_peaks.csv\")\n",
    "    out_summary  = os.path.join(OUT_DIR, \"assignments_summary.csv\")\n",
    "    assigned_raw.to_csv(out_assigned, index=False)\n",
    "    summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # --- 3) Plots on assignments ---\n",
    "    plot_neutral_mass_spectrum(deconv_peaks, OUT_DIR, filename=\"neutral_mass_spectrum.png\")\n",
    "    plot_mirror_assigned_vs_total(assigned_raw, OUT_DIR, filename=\"mirror_assigned_vs_total.png\")\n",
    "    plot_mirror_unassigned_vs_total(assigned_raw, OUT_DIR, filename=\"mirror_unassigned_vs_total.png\")\n",
    "    plot_mirror_assigned_by_protein_vs_total(\n",
    "        assigned_raw, OUT_DIR, filename=\"mirror_assigned_by_protein_vs_total.png\", max_legend_items=20\n",
    "    )\n",
    "\n",
    "    # --- 4) Console report ---\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Raw MS1 peaks (rows): {len(raw_df):,}\")\n",
    "    print(f\"Detected neutral-mass peaks: {len(deconv_peaks):,}\")\n",
    "    print(f\"Assigned raw peaks: {int(assigned_raw['is_assigned'].sum()):,}\")\n",
    "    print(f\"Non-assigned raw peaks: {int((~assigned_raw['is_assigned']).sum()):,}\")\n",
    "    print(f\"Saved: {out_assigned}\")\n",
    "    print(f\"Saved: {out_summary}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cd554",
   "metadata": {},
   "source": [
    "Identification of proteoforms against tdportal report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613c9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with formatted best_match column ‚Üí F:/new/assignments_with_best_matches.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combine charge-assignment summary with best matches from a databank.\n",
    "\n",
    "- Reads:\n",
    "    df1: assignments_summary (must have: neutral_mass, bin (or 'bin '), matched_mz_list)\n",
    "    df2: databank_with_ids (must have: rt_aligned, precursor_mz, MASS, Accession)\n",
    "\n",
    "- For each row in df1, for each m/z in matched_mz_list:\n",
    "    find the single best df2 row where ALL hold:\n",
    "        |rt_aligned - bin|    <= rt_window\n",
    "        |precursor_mz - m/z|  <= mz_tol\n",
    "        |MASS - neutral_mass| <= mass_tol\n",
    "  Then format: \"<mz>: <Accession>, <neutral_mass>\"\n",
    "\n",
    "- Outputs:\n",
    "    df1 with an added \"best_match\" column ‚Üí CSV\n",
    "\n",
    "Edit the 3 PATHS below before running.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, List\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG: edit these paths\n",
    "# ----------------------------\n",
    "CHARGE_FILE_PATH = r\"F:/new/assignments_summary.csv\"\n",
    "DATABANK_PATH    = r\"F:/test/databank_with_ids.csv\"\n",
    "OUTPUT_PATH      = r\"F:/new/assignments_with_best_matches.csv\"\n",
    "\n",
    "# Matching tolerances\n",
    "RT_WINDOW = 10.0     # minutes (or your RT unit)\n",
    "MZ_TOL    = 2.0      # Da\n",
    "MASS_TOL  = 20.0     # Da\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to numeric, invalid ‚Üí NaN.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    \"\"\"Convert a string-repr list into a Python list of floats safely.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, required: List[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Core search\n",
    "# ----------------------------\n",
    "def search_best(\n",
    "    df: pd.DataFrame,\n",
    "    rt_query: float,\n",
    "    mz_query: float,\n",
    "    mass_query: float,\n",
    "    rt_window: float = RT_WINDOW,\n",
    "    mz_tol: float = MZ_TOL,\n",
    "    mass_tol: float = MASS_TOL\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Return the single best match (row as dict) if ALL three criteria match:\n",
    "      |rt - rt_query| <= rt_window\n",
    "      |mz - mz_query| <= mz_tol\n",
    "      |mass - mass_query| <= mass_tol\n",
    "    Otherwise returns None.\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "\n",
    "    # Make sure columns exist; if not, create as NaN\n",
    "    for col in (\"rt_aligned\", \"precursor_mz\", \"MASS\"):\n",
    "        if col not in work.columns:\n",
    "            work[col] = np.nan\n",
    "\n",
    "    # Coerce to numeric\n",
    "    work[\"rt_aligned\"]   = _num(work[\"rt_aligned\"])\n",
    "    work[\"precursor_mz\"] = _num(work[\"precursor_mz\"])\n",
    "    work[\"MASS\"]         = _num(work[\"MASS\"])\n",
    "\n",
    "    # Distances\n",
    "    d_rt   = (work[\"rt_aligned\"] - float(rt_query)).abs()\n",
    "    d_mz   = (work[\"precursor_mz\"] - float(mz_query)).abs()\n",
    "    d_mass = (work[\"MASS\"] - float(mass_query)).abs()\n",
    "\n",
    "    # All three criteria must pass\n",
    "    mask = (d_rt <= rt_window) & (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "    cand = work.loc[mask].copy()\n",
    "    if cand.empty:\n",
    "        return None\n",
    "\n",
    "    # Composite score: smaller is better\n",
    "    cand[\"score\"] = (\n",
    "        d_rt.loc[cand.index] / rt_window +\n",
    "        d_mz.loc[cand.index] / mz_tol +\n",
    "        d_mass.loc[cand.index] / mass_tol\n",
    "    )\n",
    "    best_row = cand.sort_values(\"score\", kind=\"mergesort\").iloc[0]\n",
    "    return best_row.to_dict()\n",
    "\n",
    "\n",
    "def best_match_formatter(row: pd.Series, df2: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    For each m/z in row['matched_mz_list'], search df2 for best match against\n",
    "    (rt=bin, mz=m/z, mass=neutral_mass). Return a compact formatted string.\n",
    "    \"\"\"\n",
    "    neutral_mass   = row.get(\"neutral_mass\", np.nan)\n",
    "    retention_time = row.get(\"bin\", row.get(\"bin \", np.nan))  # tolerate 'bin ' too\n",
    "    mz_list        = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "\n",
    "    # If any key value missing, nothing to do\n",
    "    if pd.isna(neutral_mass) or pd.isna(retention_time) or not mz_list:\n",
    "        return None\n",
    "\n",
    "    formatted = []\n",
    "    for mz_value in mz_list:\n",
    "        res = search_best(\n",
    "            df2,\n",
    "            rt_query=float(retention_time),\n",
    "            mz_query=float(mz_value),\n",
    "            mass_query=float(neutral_mass),\n",
    "        )\n",
    "        if res is not None:\n",
    "            uniprot_id = res.get(\"Accession\", \"NA\")\n",
    "            formatted.append(f\"{mz_value}: {uniprot_id}, {neutral_mass}\")\n",
    "\n",
    "    return \"[\" + \", \".join(formatted) + \"]\" if formatted else None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load CSVs\n",
    "    if not os.path.exists(CHARGE_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {CHARGE_FILE_PATH}\")\n",
    "    if not os.path.exists(DATABANK_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {DATABANK_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(CHARGE_FILE_PATH)\n",
    "    df2 = pd.read_csv(DATABANK_PATH)\n",
    "\n",
    "    # Normalize df1 column names to handle accidental trailing spaces, capitalization, etc.\n",
    "    df1.columns = [c.strip() for c in df1.columns]\n",
    "\n",
    "    # Ensure required columns in both tables (with tolerant check for 'bin' / 'bin ')\n",
    "    # For df1, accept either 'bin' or 'bin '.\n",
    "    need_df1 = [\"neutral_mass\", \"matched_mz_list\"]\n",
    "    _ensure_columns(df1, need_df1)\n",
    "    if \"bin\" not in df1.columns and \"bin \" not in df1.columns:\n",
    "        raise KeyError(\"df1 must contain 'bin' (or 'bin ').\")\n",
    "\n",
    "    # Ensure essential df2 columns\n",
    "    _ensure_columns(df2, [\"rt_aligned\", \"precursor_mz\", \"MASS\", \"Accession\"])\n",
    "\n",
    "    # If df1 had 'bin ' originally, create 'bin' as an alias\n",
    "    if \"bin\" not in df1.columns and \"bin \" in df1.columns:\n",
    "        df1[\"bin\"] = df1[\"bin \"]\n",
    "\n",
    "    # Build best_match column (use list comprehension for reliability/speed)\n",
    "    df1[\"best_match\"] = [\n",
    "        best_match_formatter(row, df2)\n",
    "        for _, row in df1.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Save\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"Saved with formatted best_match column ‚Üí {OUTPUT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45e0d",
   "metadata": {},
   "source": [
    "Quantification of all proteoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15558a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\new\\assignments_with_quant_sums.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Config (edit paths)\n",
    "# --------------------\n",
    "DATASET_RT_PATH = r\"F:\\casts\\databank\\csv_files\\dataset_rt.csv\"          # wide matrix with cast_* columns\n",
    "ASSIGNMENTS_PATH = r\"F:\\new\\assignments_with_best_matches.csv\"          # has 'bin' and 'matched_mz_list'\n",
    "OUT_PATH = os.path.join(\n",
    "    os.path.dirname(ASSIGNMENTS_PATH) or \".\",\n",
    "    \"assignments_with_quant_sums.csv\"\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def to_cast_col(n: float) -> str:\n",
    "    \"\"\"Map an m/z to its cast_* column name: int((mz-600)*10), zero-padded.\"\"\"\n",
    "    col_num = int((float(n) - 600.0) * 10.0)\n",
    "    return \"cast_\" + str(col_num).zfill(5)\n",
    "\n",
    "def parse_mz_list(val):\n",
    "    \"\"\"Safely parse matched_mz_list cells that look like '[864.9, 865.2, ...]'.\"\"\"\n",
    "    try:\n",
    "        out = ast.literal_eval(str(val))\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return [float(x) for x in out]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df_rt = pd.read_csv(DATASET_RT_PATH)\n",
    "df_asn = pd.read_csv(ASSIGNMENTS_PATH)\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"bin\", \"target\"]:\n",
    "    if col not in df_rt.columns:\n",
    "        raise KeyError(f\"'{col}' column is required in dataset_rt.csv\")\n",
    "\n",
    "if \"bin\" not in df_asn.columns or \"matched_mz_list\" not in df_asn.columns:\n",
    "    raise KeyError(\"assignments CSV must contain 'bin' and 'matched_mz_list' columns\")\n",
    "\n",
    "# NEW columns to be added to assignments\n",
    "new_cols = [\"group_0_sum\", \"group_1_sum\", \"group_2_sum\", \"group_3_sum\",\n",
    "            \"n_mz_used\", \"n_mz_found\", \"missing_cast_columns\"]\n",
    "for c in new_cols:\n",
    "    if c in df_asn.columns:\n",
    "        # avoid accidental overwrite\n",
    "        df_asn.drop(columns=[c], inplace=True)\n",
    "\n",
    "# --------------------\n",
    "# Row-wise quantification\n",
    "# --------------------\n",
    "results = []\n",
    "for idx, row in df_asn.iterrows():\n",
    "    bin_value = float(row[\"bin\"])\n",
    "    mz_list = parse_mz_list(row[\"matched_mz_list\"])\n",
    "    cast_cols = [to_cast_col(mz) for mz in mz_list]\n",
    "\n",
    "    # Filter dataset_rt to this bin\n",
    "    df_bin = df_rt[df_rt[\"bin\"] == bin_value]\n",
    "    if df_bin.empty:\n",
    "        res = dict(\n",
    "            group_0_sum=float(\"nan\"),\n",
    "            group_1_sum=float(\"nan\"),\n",
    "            group_2_sum=float(\"nan\"),\n",
    "            group_3_sum=float(\"nan\"),\n",
    "            n_mz_used=len(cast_cols),\n",
    "            n_mz_found=0,\n",
    "            missing_cast_columns=\", \".join(cast_cols) if cast_cols else \"\"\n",
    "        )\n",
    "        results.append(res)\n",
    "        continue\n",
    "\n",
    "    # Ensure target present\n",
    "    if \"target\" not in df_bin.columns:\n",
    "        raise KeyError(\"Column 'target' not found in dataset_rt.csv\")\n",
    "\n",
    "    existing = [c for c in cast_cols if c in df_bin.columns]\n",
    "    missing = [c for c in cast_cols if c not in df_bin.columns]\n",
    "\n",
    "    if not existing:\n",
    "        sums = {0: float(\"nan\"), 1: float(\"nan\"), 2: float(\"nan\"), 3: float(\"nan\")}\n",
    "    else:\n",
    "        # Sum intensities across all selected cast_* columns per target\n",
    "        grouped = df_bin.groupby(\"target\")[existing].sum()\n",
    "        total_per_target = grouped.sum(axis=1)  # sum across those cast_* columns\n",
    "        sums = {t: float(total_per_target.get(t, float(\"nan\"))) for t in [0, 1, 2, 3]}\n",
    "\n",
    "    res = dict(\n",
    "        group_0_sum=sums[0],\n",
    "        group_1_sum=sums[1],\n",
    "        group_2_sum=sums[2],\n",
    "        group_3_sum=sums[3],\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "# Attach results\n",
    "df_quant = pd.DataFrame(results, index=df_asn.index)\n",
    "df_asn_out = pd.concat([df_asn, df_quant], axis=1)\n",
    "\n",
    "# --------------------\n",
    "# Save updated CSV\n",
    "# --------------------\n",
    "df_asn_out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Saved: {OUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
