{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: comment out if you don't need seaborn\n",
    "import seaborn as sns  # noqa: F401\n",
    "\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Repro defaults (each repeat gets its own seed below; this just fixes\n",
    "# library-internal nondeterminism as much as feasible)\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# (Optional) make TF less memory hungry on GPU\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers you had (kept here; safe imports inside fn so file runs w/o fisher_py)\n",
    "# ---------------------------------------------------------------------\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def MS1Casting(folder_path, file_path):\n",
    "    try:\n",
    "        from fisher_py.raw_file import RawFile\n",
    "        from fisher_py.scan import Scan\n",
    "    except Exception:\n",
    "        raise ImportError(\"fisher_py is required for MS1Casting\")\n",
    "    os.chdir(folder_path)\n",
    "    raw = RawFile(file_path)\n",
    "    data_intensities = [0]*1369\n",
    "    for i in tqdm(range(1, raw.number_of_scans)):\n",
    "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "        if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "            scan_masses = raw_scan.preferred_masses\n",
    "            scan_intensities = raw_scan.preferred_intensities\n",
    "            for j in range(len(scan_masses)):\n",
    "                index = round(scan_masses[j])\n",
    "                if 600 < index < 1969:\n",
    "                    data_intensities[index-600] += scan_intensities[j]\n",
    "    return data_intensities\n",
    "\n",
    "def MS1Casting_highres(folder_path, file_path):\n",
    "    try:\n",
    "        from fisher_py.raw_file import RawFile\n",
    "        from fisher_py.scan import Scan\n",
    "    except Exception:\n",
    "        raise ImportError(\"fisher_py is required for MS1Casting_highres\")\n",
    "    os.chdir(folder_path)\n",
    "    raw = RawFile(file_path)\n",
    "    data_intensities = [0]*13690\n",
    "    for i in tqdm(range(1, raw.number_of_scans)):\n",
    "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "        if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "            scan_masses = raw_scan.preferred_masses\n",
    "            scan_intensities = raw_scan.preferred_intensities\n",
    "            for j in range(len(scan_masses)):\n",
    "                index = int((round(scan_masses[j], 1))*10)\n",
    "                if 6000 < index < 19690:\n",
    "                    data_intensities[index-6000] += scan_intensities[j]\n",
    "    return data_intensities\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Gradient averaging helper (models-then-samples mean of log-odds gradient)\n",
    "# ---------------------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _log_odds_grad_for_model(x1, model, class_a, class_b, eps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        log_odds = tf.math.log(p[:, class_a] + eps) - tf.math.log(p[:, class_b] + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_logodds_gradient_for_pair(X: tf.Tensor, models: list, class_a: int, class_b: int, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Average input gradient of log p(class_a|x) - log p(class_b|x) across samples and models.\n",
    "    Returns (D,) tensor.\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _log_odds_grad_for_model(x_i, m, class_a, class_b, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Simple deconvolution + plotting (unchanged)\n",
    "# ---------------------------------------------------------------------\n",
    "def charge_state_deconvolution(x_values, y_values, max_charge=50, intensity_threshold=0.0005):\n",
    "    mass_range = np.linspace(10000, 20000, len(x_values))\n",
    "    deconvoluted_spectrum = np.zeros_like(mass_range)\n",
    "    charge_mapping = {}\n",
    "    peak_list = []\n",
    "\n",
    "    for charge in range(1, max_charge + 1):\n",
    "        neutral_masses = x_values * charge\n",
    "        for i, neutral_mass in enumerate(neutral_masses):\n",
    "            if 10000 <= neutral_mass <= 20000 and y_values[i] > intensity_threshold:\n",
    "                idx = np.searchsorted(mass_range, neutral_mass)\n",
    "                if idx < len(deconvoluted_spectrum):\n",
    "                    deconvoluted_spectrum[idx] += y_values[i]\n",
    "                    charge_mapping[x_values[i]] = charge\n",
    "                    peak_list.append((neutral_mass, deconvoluted_spectrum[idx]))\n",
    "\n",
    "    top_peak = max(peak_list, key=lambda x: x[1]) if peak_list else (None, None)\n",
    "    return mass_range, deconvoluted_spectrum, charge_mapping, top_peak\n",
    "\n",
    "def plot_raw_spectrum(x_values, observed_spectrum, charge_mapping):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x_values, observed_spectrum, label='Observed Spectrum', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Mass/Charge (m/z)')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.title('')\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data load & prep (bin == 45; normalize each column by (max+1))\n",
    "# ---------------------------------------------------------------------\n",
    "CSV_PATH = r'F:/casts/dataset_rt.csv'   # <- adjust if needed\n",
    "BIN_VALUE = 45\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "filtered_df = df[df['bin'] == BIN_VALUE].copy()\n",
    "\n",
    "cols_to_normalize = filtered_df.columns.difference(['bin', 'target'])\n",
    "filtered_df[cols_to_normalize] = filtered_df[cols_to_normalize].apply(lambda x: x / (x.max() + 1.0))\n",
    "filtered_df = filtered_df.drop(columns=['bin'])\n",
    "\n",
    "X = filtered_df.copy()\n",
    "Y = X.pop(\"target\")\n",
    "X_train = np.nan_to_num(np.array(X), copy=False)\n",
    "y_train = np.nan_to_num(np.array(Y), copy=False)\n",
    "\n",
    "input_dim = X_train.shape[1]          # expected 13690\n",
    "num_classes = int(np.max(y_train)) + 1\n",
    "assert input_dim >= 10000, f\"Expected >=10000 features, got {input_dim}\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Model builder\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Nested training: 5-fold CV × 10 random inits within each fold = 50 models\n",
    "# ---------------------------------------------------------------------\n",
    "k = 5\n",
    "N_REPEATS = 10\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "rand_int = 100\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "all_models = []   # will hold 50 models\n",
    "fold_histories = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):\n",
    "    print(f\"\\n=== Fold {fold}/{k} ===\")\n",
    "    X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n",
    "    X_va, y_va = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    for r in range(N_REPEATS):\n",
    "        seed = rand_int * fold + r\n",
    "        tf.keras.utils.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        m = build_model(input_dim, num_classes)\n",
    "        hist = m.fit(\n",
    "            X_tr, y_tr,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_va, y_va),\n",
    "            verbose=0\n",
    "        )\n",
    "        all_models.append(m)\n",
    "        fold_histories.append(hist.history)\n",
    "        print(f\"  Trained model {r+1}/{N_REPEATS} for fold {fold} (seed={seed})\")\n",
    "\n",
    "print(f\"\\nTotal models trained: {len(all_models)} (expected 50)\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Average gradients across ALL 50 models\n",
    "# ---------------------------------------------------------------------\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "# Example: compute gradients for two pairs (adjust indices as needed)\n",
    "avg_grad_all_1_vs_0 = compute_avg_logodds_gradient_for_pair(\n",
    "    X_train_tensor, all_models, class_a=1, class_b=0\n",
    ")\n",
    "avg_grad_all_2_vs_0 = compute_avg_logodds_gradient_for_pair(\n",
    "    X_train_tensor, all_models, class_a=2, class_b=0\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Persist averaged gradients\n",
    "# ---------------------------------------------------------------------\n",
    "out_dir = \"./avg_grads_5x10\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "np.save(os.path.join(out_dir, \"avg_grad_all_1_vs_0.npy\"), avg_grad_all_1_vs_0.numpy())\n",
    "np.save(os.path.join(out_dir, \"avg_grad_all_2_vs_0.npy\"), avg_grad_all_2_vs_0.numpy())\n",
    "print(f\"Saved averaged gradients to: {out_dir}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Map first 10,000 features to x-grid (600..1600 by 0.1) and visualize\n",
    "# ---------------------------------------------------------------------\n",
    "x = np.arange(600, 1600, 0.1)  # length 10000\n",
    "\n",
    "y_1_vs_0 = avg_grad_all_1_vs_0.numpy().flatten()[:10000]\n",
    "y_2_vs_0 = avg_grad_all_2_vs_0.numpy().flatten()[:10000]\n",
    "\n",
    "# Split pos/neg for 2 vs 0 example\n",
    "x_pos_2v0 = x[y_2_vs_0 > 0]\n",
    "y_pos_2v0 = y_2_vs_0[y_2_vs_0 > 0]\n",
    "x_neg_2v0 = x[y_2_vs_0 < 0]\n",
    "y_neg_2v0 = y_2_vs_0[y_2_vs_0 < 0]\n",
    "\n",
    "# Deconvolution + plots (negative grads as positive by multiplying -1)\n",
    "y_values = -1.0 * y_neg_2v0\n",
    "x_values = x_neg_2v0\n",
    "mass_values, deconvoluted_spectrum, charge_mapping, top_peak = charge_state_deconvolution(x_values, y_values)\n",
    "print(\"Top Neutral Mass (neg grads, ALL 50 models):\", top_peak)\n",
    "plot_raw_spectrum(x_values, y_values, charge_mapping)\n",
    "\n",
    "y_values = y_pos_2v0\n",
    "x_values = x_pos_2v0\n",
    "mass_values, deconvoluted_spectrum, charge_mapping, top_peak = charge_state_deconvolution(x_values, y_values)\n",
    "print(\"Top Neutral Mass (pos grads, ALL 50 models):\", top_peak)\n",
    "plot_raw_spectrum(x_values, y_values, charge_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695ef5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 100] Fold 1/5 trained 10 models (total so far: 10)\n",
      "[Seed base 100] Fold 2/5 trained 10 models (total so far: 20)\n",
      "[Seed base 100] Fold 3/5 trained 10 models (total so far: 30)\n",
      "[Seed base 100] Fold 4/5 trained 10 models (total so far: 40)\n",
      "[Seed base 100] Fold 5/5 trained 10 models (total so far: 50)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _log_odds_grad_for_model at 0x0000017DEBA394E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _log_odds_grad_for_model at 0x0000017DEBA394E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total so far: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total so far: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total so far: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total so far: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total so far: 50)\n",
      "\n",
      "=== Cosine similarities (Run A vs Run B, class 2 vs 0) ===\n",
      "Full (pos + |neg|): 0.972631\n",
      "Pos-only          : 0.974154\n",
      "Neg-only (|neg|)  : 0.838420\n",
      "\n",
      "Saved plots -> ./two_run_compare_5x10\\plots\n",
      "Saved vectors -> ./two_run_compare_5x10\\run_comparison_vectors.csv\n",
      "Saved cosine summary -> ./two_run_compare_5x10\\cosine_summary.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional\n",
    "import seaborn as sns  # noqa: F401\n",
    "\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Repro defaults\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Your helpers (kept for completeness; unused in this script’s flow)\n",
    "# ---------------------------------------------------------------------\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def MS1Casting(folder_path, file_path):\n",
    "    try:\n",
    "        from fisher_py.raw_file import RawFile\n",
    "        from fisher_py.scan import Scan\n",
    "    except Exception:\n",
    "        raise ImportError(\"fisher_py is required for MS1Casting\")\n",
    "    os.chdir(folder_path)\n",
    "    raw = RawFile(file_path)\n",
    "    data_intensities = [0]*1369\n",
    "    for i in tqdm(range(1, raw.number_of_scans)):\n",
    "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "        if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "            scan_masses = raw_scan.preferred_masses\n",
    "            scan_intensities = raw_scan.preferred_intensities\n",
    "            for j in range(len(scan_masses)):\n",
    "                index = round(scan_masses[j])\n",
    "                if 600 < index < 1969:\n",
    "                    data_intensities[index-600] += scan_intensities[j]\n",
    "    return data_intensities\n",
    "\n",
    "def MS1Casting_highres(folder_path, file_path):\n",
    "    try:\n",
    "        from fisher_py.raw_file import RawFile\n",
    "        from fisher_py.scan import Scan\n",
    "    except Exception:\n",
    "        raise ImportError(\"fisher_py is required for MS1Casting_highres\")\n",
    "    os.chdir(folder_path)\n",
    "    raw = RawFile(file_path)\n",
    "    data_intensities = [0]*13690\n",
    "    for i in tqdm(range(1, raw.number_of_scans)):\n",
    "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "        if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "            scan_masses = raw_scan.preferred_masses\n",
    "            scan_intensities = raw_scan.preferred_intensities\n",
    "            for j in range(len(scan_masses)):\n",
    "                index = int((round(scan_masses[j], 1))*10)\n",
    "                if 6000 < index < 19690:\n",
    "                    data_intensities[index-6000] += scan_intensities[j]\n",
    "    return data_intensities\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Gradient helper\n",
    "# ---------------------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _log_odds_grad_for_model(x1, model, class_a, class_b, eps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        log_odds = tf.math.log(p[:, class_a] + eps) - tf.math.log(p[:, class_b] + eps)\n",
    "    g = tape.gradient(log_odds, x1)    # (1, D)\n",
    "    return tf.squeeze(g, axis=0)       # (D,)\n",
    "\n",
    "def compute_avg_logodds_gradient_for_pair(X: tf.Tensor, models: list, class_a: int, class_b: int, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Average input gradient of log p(class_a|x) - log p(class_b|x) across samples and models.\n",
    "    Returns (D,) tensor.\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _log_odds_grad_for_model(x_i, m, class_a, class_b, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Simple deconvolution (kept from your code; not used in comparisons)\n",
    "# ---------------------------------------------------------------------\n",
    "def charge_state_deconvolution(x_values, y_values, max_charge=50, intensity_threshold=0.0005):\n",
    "    mass_range = np.linspace(10000, 20000, len(x_values))\n",
    "    deconvoluted_spectrum = np.zeros_like(mass_range)\n",
    "    charge_mapping = {}\n",
    "    peak_list = []\n",
    "\n",
    "    for charge in range(1, max_charge + 1):\n",
    "        neutral_masses = x_values * charge\n",
    "        for i, neutral_mass in enumerate(neutral_masses):\n",
    "            if 10000 <= neutral_mass <= 20000 and y_values[i] > intensity_threshold:\n",
    "                idx = np.searchsorted(mass_range, neutral_mass)\n",
    "                if idx < len(deconvoluted_spectrum):\n",
    "                    deconvoluted_spectrum[idx] += y_values[i]\n",
    "                    charge_mapping[x_values[i]] = charge\n",
    "                    peak_list.append((neutral_mass, deconvoluted_spectrum[idx]))\n",
    "\n",
    "    top_peak = max(peak_list, key=lambda x: x[1]) if peak_list else (None, None)\n",
    "    return mass_range, deconvoluted_spectrum, charge_mapping, top_peak\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utils: model builder, cosine, mirror plot\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y shown above baseline, bottom_y mirrored below (as negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, color=\"k\", linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (600..1600 at 0.1 step)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config / data\n",
    "# ---------------------------------------------------------------------\n",
    "CSV_PATH = r'F:/casts/dataset_rt.csv'   # <- set appropriately\n",
    "BIN_VALUE = 45\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "k = 5\n",
    "N_REPEATS = 10\n",
    "\n",
    "# Two independent training baselines:\n",
    "RAND_INTS = [100, 777]   # <— you asked for two different rand_int values\n",
    "\n",
    "OUT_DIR = \"./two_run_compare_5x10\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load + normalize (per your pattern): drop bin, normalize each feature col by (max+1)\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "filtered_df = df[df['bin'] == BIN_VALUE].copy()\n",
    "\n",
    "cols_to_normalize = filtered_df.columns.difference(['bin', 'target'])\n",
    "filtered_df[cols_to_normalize] = filtered_df[cols_to_normalize].apply(lambda x: x / (x.max() + 1.0))\n",
    "filtered_df = filtered_df.drop(columns=['bin'])\n",
    "\n",
    "X_df = filtered_df.copy()\n",
    "Y = X_df.pop(\"target\").to_numpy()\n",
    "X = np.nan_to_num(X_df.to_numpy(), copy=False)\n",
    "input_dim = X.shape[1]\n",
    "num_classes = int(np.max(Y)) + 1\n",
    "assert input_dim >= 10000, f\"Expected >=10000 features, got {input_dim}\"\n",
    "\n",
    "# Map first 10,000 features to x-grid (600..1600 by 0.1)\n",
    "x_grid = np.arange(600, 1600, 0.1)  # length 10000\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Function: train full 5x10 and return averaged grad for class 2 vs 0\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_avg_grad(rand_int_base: int) -> np.ndarray:\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[train_idx], Y[train_idx]\n",
    "        X_va, y_va = X[val_idx], Y[val_idx]\n",
    "\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = rand_int_base * fold + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            m = build_model(input_dim, num_classes)\n",
    "            m.fit(\n",
    "                X_tr, y_tr,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_va, y_va),\n",
    "                verbose=0\n",
    "            )\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {rand_int_base}] Fold {fold}/{k} trained {N_REPEATS} models (total so far: {len(all_models)})\")\n",
    "\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    avg_grad_2_vs_0 = compute_avg_logodds_gradient_for_pair(X_tensor, all_models, class_a=2, class_b=0).numpy()\n",
    "    return avg_grad_2_vs_0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run A and Run B\n",
    "# ---------------------------------------------------------------------\n",
    "avg_A = train_and_avg_grad(RAND_INTS[0])\n",
    "avg_B = train_and_avg_grad(RAND_INTS[1])\n",
    "\n",
    "# Keep only the first 10k matching the x_grid\n",
    "yA_full = avg_A.flatten()[:10000]\n",
    "yB_full = avg_B.flatten()[:10000]\n",
    "\n",
    "# Build “channels”\n",
    "#   Pos-only  : keep positives, zeros elsewhere\n",
    "#   Neg-only  : keep absolute value of negatives, zeros elsewhere\n",
    "yA_pos = np.where(yA_full > 0, yA_full, 0.0)\n",
    "yB_pos = np.where(yB_full > 0, yB_full, 0.0)\n",
    "yA_neg = np.where(yA_full < 0, -yA_full, 0.0)  # abs of neg\n",
    "yB_neg = np.where(yB_full < 0, -yB_full, 0.0)\n",
    "\n",
    "# Cosine similarities\n",
    "cos_full = cosine_sim(yA_pos + yA_neg, yB_pos + yB_neg)\n",
    "cos_pos  = cosine_sim(yA_pos, yB_pos)\n",
    "cos_neg  = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "print(\"\\n=== Cosine similarities (Run A vs Run B, class 2 vs 0) ===\")\n",
    "print(f\"Full (pos + |neg|): {cos_full:.6f}\")\n",
    "print(f\"Pos-only          : {cos_pos:.6f}\")\n",
    "print(f\"Neg-only (|neg|)  : {cos_neg:.6f}\")\n",
    "\n",
    "# Save CSV of vectors + similarities\n",
    "csv_out = os.path.join(OUT_DIR, \"run_comparison_vectors.csv\")\n",
    "pd.DataFrame({\n",
    "    \"m/z\": x_grid,\n",
    "    \"yA_pos\": yA_pos,\n",
    "    \"yA_neg_abs\": yA_neg,\n",
    "    \"yB_pos\": yB_pos,\n",
    "    \"yB_neg_abs\": yB_neg,\n",
    "}).to_csv(csv_out, index=False)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"cosine_summary.json\"), \"w\") as f:\n",
    "    json.dump({\"cos_full\": cos_full, \"cos_pos\": cos_pos, \"cos_neg\": cos_neg}, f, indent=2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Mirror plots (Run A on top; Run B mirrored on bottom)\n",
    "# ---------------------------------------------------------------------\n",
    "mirror_plot(x_grid, yA_pos + yA_neg, yB_pos + yB_neg,\n",
    "            title=\"Mirror Plot — Full (pos + |neg|), class 2 vs 0\",\n",
    "            outfile=os.path.join(PLOT_DIR, \"mirror_full_2v0.png\"))\n",
    "\n",
    "mirror_plot(x_grid, yA_pos, yB_pos,\n",
    "            title=\"Mirror Plot — Positive only, class 2 vs 0\",\n",
    "            outfile=os.path.join(PLOT_DIR, \"mirror_pos_2v0.png\"))\n",
    "\n",
    "mirror_plot(x_grid, yA_neg, yB_neg,\n",
    "            title=\"Mirror Plot — Negative only (|neg|), class 2 vs 0\",\n",
    "            outfile=os.path.join(PLOT_DIR, \"mirror_negabs_2v0.png\"))\n",
    "\n",
    "print(f\"\\nSaved plots -> {PLOT_DIR}\")\n",
    "print(f\"Saved vectors -> {csv_out}\")\n",
    "print(f\"Saved cosine summary -> {os.path.join(OUT_DIR, 'cosine_summary.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df157355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 100] Fold 1/5 trained 10 models (total so far: 10)\n",
      "[Seed base 100] Fold 2/5 trained 10 models (total so far: 20)\n",
      "[Seed base 100] Fold 3/5 trained 10 models (total so far: 30)\n",
      "[Seed base 100] Fold 4/5 trained 10 models (total so far: 40)\n",
      "[Seed base 100] Fold 5/5 trained 10 models (total so far: 50)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _log_odds_grad_for_model at 0x0000017DF7D7FBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _log_odds_grad_for_model at 0x0000017DF7D7FBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total so far: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total so far: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total so far: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total so far: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total so far: 50)\n",
      "\n",
      "=== 1v0: Cosine similarities (Run A vs Run B) ===\n",
      "Full (pos + |neg|): 0.971782\n",
      "Pos-only          : 0.973655\n",
      "Neg-only (|neg|)  : 0.848608\n",
      "\n",
      "=== 2v0: Cosine similarities (Run A vs Run B) ===\n",
      "Full (pos + |neg|): 0.972631\n",
      "Pos-only          : 0.974154\n",
      "Neg-only (|neg|)  : 0.838420\n",
      "\n",
      "=== 3v0: Cosine similarities (Run A vs Run B) ===\n",
      "Full (pos + |neg|): 0.937225\n",
      "Pos-only          : 0.699618\n",
      "Neg-only (|neg|)  : 0.938870\n",
      "\n",
      "Outputs saved under: ./two_run_compare_5x10_multi\n",
      "Plots saved under:   ./two_run_compare_5x10_multi\\plots\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional\n",
    "import seaborn as sns  # noqa: F401\n",
    "\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Repro defaults\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# (Kept for completeness; unused in main flow)\n",
    "# ---------------------------------------------------------------------\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Gradient helper\n",
    "# ---------------------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _log_odds_grad_for_model(x1, model, class_a, class_b, eps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        log_odds = tf.math.log(p[:, class_a] + eps) - tf.math.log(p[:, class_b] + eps)\n",
    "    g = tape.gradient(log_odds, x1)    # (1, D)\n",
    "    return tf.squeeze(g, axis=0)       # (D,)\n",
    "\n",
    "def compute_avg_logodds_gradient_for_pair(X: tf.Tensor, models: list, class_a: int, class_b: int, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Average input gradient of log p(class_a|x) - log p(class_b|x) across samples and models.\n",
    "    Returns (D,) tensor.\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _log_odds_grad_for_model(x_i, m, class_a, class_b, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utils: model builder, cosine, mirror plot\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y shown above baseline, bottom_y mirrored below (as negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, color=\"k\", linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (600..1600 at 0.1 step)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config / data\n",
    "# ---------------------------------------------------------------------\n",
    "CSV_PATH = r'F:/casts/dataset_rt.csv'   # <- set appropriately\n",
    "BIN_VALUE = 45\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "k = 5\n",
    "N_REPEATS = 10\n",
    "\n",
    "# Two independent training baselines:\n",
    "RAND_INTS = [100, 777]   # two distinct rand_int bases\n",
    "\n",
    "OUT_DIR = \"./two_run_compare_5x10_multi\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Class pairs to analyze\n",
    "CLASS_PAIRS = [(1, 0), (2, 0), (3, 0)]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load + normalize (drop bin; per-column x/(max+1))\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "filtered_df = df[df['bin'] == BIN_VALUE].copy()\n",
    "\n",
    "cols_to_normalize = filtered_df.columns.difference(['bin', 'target'])\n",
    "filtered_df[cols_to_normalize] = filtered_df[cols_to_normalize].apply(lambda x: x / (x.max() + 1.0))\n",
    "filtered_df = filtered_df.drop(columns=['bin'])\n",
    "\n",
    "X_df = filtered_df.copy()\n",
    "Y = X_df.pop(\"target\").to_numpy()\n",
    "X = np.nan_to_num(X_df.to_numpy(), copy=False)\n",
    "input_dim = X.shape[1]\n",
    "num_classes = int(np.max(Y)) + 1\n",
    "assert input_dim >= 10000, f\"Expected >=10000 features, got {input_dim}\"\n",
    "\n",
    "# x-grid for first 10k features\n",
    "x_grid = np.arange(600, 1600, 0.1)  # len 10000\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Train a full 5x10 run and get averaged grads for requested pairs\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_avg_grads(rand_int_base: int, pairs: list):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[train_idx], Y[train_idx]\n",
    "        X_va, y_va = X[val_idx], Y[val_idx]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = rand_int_base * fold + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(input_dim, num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {rand_int_base}] Fold {fold}/{k} trained {N_REPEATS} models (total so far: {len(all_models)})\")\n",
    "\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    grads = {}\n",
    "    for (a, b) in pairs:\n",
    "        if a >= num_classes or b >= num_classes:\n",
    "            print(f\"Skipping pair ({a} vs {b}) — class index out of range (num_classes={num_classes}).\")\n",
    "            continue\n",
    "        grads[(a, b)] = compute_avg_logodds_gradient_for_pair(X_tensor, all_models, class_a=a, class_b=b).numpy()\n",
    "    return grads\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run A and Run B (each returns dict {(a,b): grad_vec})\n",
    "# ---------------------------------------------------------------------\n",
    "avg_A = train_and_avg_grads(RAND_INTS[0], CLASS_PAIRS)\n",
    "avg_B = train_and_avg_grads(RAND_INTS[1], CLASS_PAIRS)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# For each pair: build channels, compute cosine, save vectors, make plots\n",
    "# ---------------------------------------------------------------------\n",
    "overall_summary = {}\n",
    "for (a, b) in CLASS_PAIRS:\n",
    "    key = f\"{a}v{b}\"\n",
    "    if (a, b) not in avg_A or (a, b) not in avg_B:\n",
    "        continue\n",
    "\n",
    "    yA_full = avg_A[(a, b)].flatten()[:10000]\n",
    "    yB_full = avg_B[(a, b)].flatten()[:10000]\n",
    "\n",
    "    # Pos-only (keep positives), Neg-only (abs of negatives)\n",
    "    yA_pos = np.where(yA_full > 0, yA_full, 0.0)\n",
    "    yB_pos = np.where(yB_full > 0, yB_full, 0.0)\n",
    "    yA_neg = np.where(yA_full < 0, -yA_full, 0.0)\n",
    "    yB_neg = np.where(yB_full < 0, -yB_full, 0.0)\n",
    "\n",
    "    # Cosines\n",
    "    cos_full = cosine_sim(yA_pos + yA_neg, yB_pos + yB_neg)\n",
    "    cos_pos  = cosine_sim(yA_pos, yB_pos)\n",
    "    cos_neg  = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "    print(f\"\\n=== {key}: Cosine similarities (Run A vs Run B) ===\")\n",
    "    print(f\"Full (pos + |neg|): {cos_full:.6f}\")\n",
    "    print(f\"Pos-only          : {cos_pos:.6f}\")\n",
    "    print(f\"Neg-only (|neg|)  : {cos_neg:.6f}\")\n",
    "\n",
    "    # Save vectors\n",
    "    pair_dir = os.path.join(OUT_DIR, key)\n",
    "    pair_plot_dir = os.path.join(PLOT_DIR, key)\n",
    "    os.makedirs(pair_dir, exist_ok=True)\n",
    "    os.makedirs(pair_plot_dir, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"m/z\": x_grid,\n",
    "        \"yA_pos\": yA_pos,\n",
    "        \"yA_neg_abs\": yA_neg,\n",
    "        \"yB_pos\": yB_pos,\n",
    "        \"yB_neg_abs\": yB_neg,\n",
    "    }).to_csv(os.path.join(pair_dir, f\"run_vectors_{key}.csv\"), index=False)\n",
    "\n",
    "    # Save cosine summary for the pair\n",
    "    pair_summary = {\"cos_full\": cos_full, \"cos_pos\": cos_pos, \"cos_neg\": cos_neg}\n",
    "    with open(os.path.join(pair_dir, f\"cosine_summary_{key}.json\"), \"w\") as f:\n",
    "        json.dump(pair_summary, f, indent=2)\n",
    "    overall_summary[key] = pair_summary\n",
    "\n",
    "    # Mirror plots\n",
    "    mirror_plot(x_grid, yA_pos + yA_neg, yB_pos + yB_neg,\n",
    "                title=f\"Mirror — Full (pos + |neg|), class {a} vs {b}\",\n",
    "                outfile=os.path.join(pair_plot_dir, f\"mirror_full_{key}.png\"))\n",
    "    mirror_plot(x_grid, yA_pos, yB_pos,\n",
    "                title=f\"Mirror — Positive only, class {a} vs {b}\",\n",
    "                outfile=os.path.join(pair_plot_dir, f\"mirror_pos_{key}.png\"))\n",
    "    mirror_plot(x_grid, yA_neg, yB_neg,\n",
    "                title=f\"Mirror — Negative only (|neg|), class {a} vs {b}\",\n",
    "                outfile=os.path.join(pair_plot_dir, f\"mirror_negabs_{key}.png\"))\n",
    "\n",
    "# Save an overall JSON summary across all pairs\n",
    "with open(os.path.join(OUT_DIR, \"cosine_summary_all_pairs.json\"), \"w\") as f:\n",
    "    json.dump(overall_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nOutputs saved under: {OUT_DIR}\")\n",
    "print(f\"Plots saved under:   {PLOT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb009d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Repro defaults (library nondeterminism tamed as much as feasible)\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal helper kept for completeness (unused in main flow)\n",
    "# ---------------------------------------------------------------------\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Gradient helpers\n",
    "# ---------------------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _log_odds_grad_for_model(x1, model, class_a, class_b, eps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        log_odds = tf.math.log(p[:, class_a] + eps) - tf.math.log(p[:, class_b] + eps)\n",
    "    g = tape.gradient(log_odds, x1)    # (1, D)\n",
    "    return tf.squeeze(g, axis=0)       # (D,)\n",
    "\n",
    "def compute_avg_logodds_gradient_for_pair(X: tf.Tensor, models: list, class_a: int, class_b: int, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Average input gradient of log p(class_a|x) - log p(class_b|x) across samples and models.\n",
    "    Returns (D,) tensor (float32 NumPy when .numpy()).\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _log_odds_grad_for_model(x_i, m, class_a, class_b, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utils: model, cosine, mirror plotting\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y drawn above baseline; bottom_y mirrored below (negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, color=\"k\", linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (600..1600 at 0.1 step)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "CSV_PATH   = r\"F:/casts/dataset_half.csv\"  # <- adjust\n",
    "EPOCHS     = 50\n",
    "BATCH_SIZE = 32\n",
    "K_SPLITS   = 5\n",
    "N_REPEATS  = 10\n",
    "\n",
    "# Two independent 5x10 runs per bin:\n",
    "RAND_INTS  = [100, 777]\n",
    "\n",
    "# Class pairs you wanted:\n",
    "CLASS_PAIRS = [(1, 0), (2, 0), (3, 0)]\n",
    "\n",
    "# Optionally limit how many bins to process (None = all)\n",
    "BINS_LIMIT = None  # e.g., set to 3 for a quick smoke test\n",
    "\n",
    "OUT_DIR  = \"./all_bins_two_run_compare_5x10\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load once; enumerate bin values\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "all_bins = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "\n",
    "if BINS_LIMIT is not None:\n",
    "    all_bins = all_bins[:int(BINS_LIMIT)]\n",
    "\n",
    "print(f\"Discovered {len(all_bins)} bin(s): {all_bins}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-run trainer that returns averaged grads for requested pairs\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_avg_grads(X: np.ndarray, Y: np.ndarray, rand_int_base: int, pairs: list, num_classes: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[train_idx], Y[train_idx]\n",
    "        X_va, y_va = X[val_idx], Y[val_idx]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = rand_int_base * fold + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {rand_int_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total so far: {len(all_models)})\")\n",
    "\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    grads = {}\n",
    "    for (a, b) in pairs:\n",
    "        if a >= num_classes or b >= num_classes:\n",
    "            print(f\"  Skipping pair ({a} vs {b}) — class index out of range (num_classes={num_classes}).\")\n",
    "            continue\n",
    "        grads[(a, b)] = compute_avg_logodds_gradient_for_pair(X_tensor, all_models, class_a=a, class_b=b).numpy()\n",
    "    return grads\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main loop over bins\n",
    "# ---------------------------------------------------------------------\n",
    "overall_summary = {}\n",
    "\n",
    "for bin_value in all_bins:\n",
    "    print(f\"\\n================= BIN {bin_value} =================\")\n",
    "    # Filter this bin and normalize columns except ['bin','target'] by (max+1)\n",
    "    fdf = df[df[\"bin\"] == bin_value].copy()\n",
    "    if fdf.empty:\n",
    "        print(f\"  Bin {bin_value}: no rows — skipping.\")\n",
    "        continue\n",
    "\n",
    "    cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "    fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "    fdf = fdf.drop(columns=['bin'])\n",
    "\n",
    "    X_df = fdf.copy()\n",
    "    Y = X_df.pop(\"target\").to_numpy()\n",
    "    X = np.nan_to_num(X_df.to_numpy(), copy=False)\n",
    "\n",
    "    input_dim   = X.shape[1]\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "\n",
    "    if input_dim < 1 or X.shape[0] < 2:\n",
    "        print(f\"  Bin {bin_value}: insufficient data (samples={X.shape[0]}, dim={input_dim}) — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Build the x-grid up to min(10000, input_dim)\n",
    "    n_grid = min(10000, input_dim)\n",
    "    x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)  # len n_grid\n",
    "\n",
    "    # Two independent runs (each returns {(a,b): grad_vec})\n",
    "    avg_A = train_and_avg_grads(X, Y, RAND_INTS[0], CLASS_PAIRS, num_classes)\n",
    "    avg_B = train_and_avg_grads(X, Y, RAND_INTS[1], CLASS_PAIRS, num_classes)\n",
    "\n",
    "    bin_summary = {}\n",
    "    bin_dir      = os.path.join(OUT_DIR, f\"bin_{bin_value}\")\n",
    "    bin_plot_dir = os.path.join(PLOT_DIR, f\"bin_{bin_value}\")\n",
    "    os.makedirs(bin_dir, exist_ok=True)\n",
    "    os.makedirs(bin_plot_dir, exist_ok=True)\n",
    "\n",
    "    for (a, b) in CLASS_PAIRS:\n",
    "        key = f\"{a}v{b}\"\n",
    "        if (a, b) not in avg_A or (a, b) not in avg_B:\n",
    "            print(f\"  Bin {bin_value}: pair {key} missing — skipped.\")\n",
    "            continue\n",
    "\n",
    "        yA_full = avg_A[(a, b)].flatten()[:n_grid]\n",
    "        yB_full = avg_B[(a, b)].flatten()[:n_grid]\n",
    "\n",
    "        # Channels: pos keep; neg -> absolute value\n",
    "        yA_pos = np.where(yA_full > 0, yA_full, 0.0)\n",
    "        yB_pos = np.where(yB_full > 0, yB_full, 0.0)\n",
    "        yA_neg = np.where(yA_full < 0, -yA_full, 0.0)\n",
    "        yB_neg = np.where(yB_full < 0, -yB_full, 0.0)\n",
    "\n",
    "        # Cosines\n",
    "        cos_full = cosine_sim(yA_pos + yA_neg, yB_pos + yB_neg)\n",
    "        cos_pos  = cosine_sim(yA_pos, yB_pos)\n",
    "        cos_neg  = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "        print(f\"  Bin {bin_value} — {key}: cos_full={cos_full:.6f}  cos_pos={cos_pos:.6f}  cos_neg={cos_neg:.6f}\")\n",
    "\n",
    "        # Save vectors for this bin/pair\n",
    "        pair_dir = os.path.join(bin_dir, key)\n",
    "        pair_plot_dir = os.path.join(bin_plot_dir, key)\n",
    "        os.makedirs(pair_dir, exist_ok=True)\n",
    "        os.makedirs(pair_plot_dir, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"m/z\": x_grid,\n",
    "            \"yA_pos\": yA_pos,\n",
    "            \"yA_neg_abs\": yA_neg,\n",
    "            \"yB_pos\": yB_pos,\n",
    "            \"yB_neg_abs\": yB_neg,\n",
    "        }).to_csv(os.path.join(pair_dir, f\"run_vectors_{key}.csv\"), index=False)\n",
    "\n",
    "        # Pair-level JSON\n",
    "        pair_summary = {\"cos_full\": cos_full, \"cos_pos\": cos_pos, \"cos_neg\": cos_neg}\n",
    "        with open(os.path.join(pair_dir, f\"cosine_summary_{key}.json\"), \"w\") as f:\n",
    "            json.dump(pair_summary, f, indent=2)\n",
    "\n",
    "        # Plots\n",
    "        mirror_plot(x_grid, yA_pos + yA_neg, yB_pos + yB_neg,\n",
    "                    title=f\"BIN {bin_value} — Mirror Full (pos+|neg|) {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_full_{key}.png\"))\n",
    "        mirror_plot(x_grid, yA_pos, yB_pos,\n",
    "                    title=f\"BIN {bin_value} — Mirror Positive {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_pos_{key}.png\"))\n",
    "        mirror_plot(x_grid, yA_neg, yB_neg,\n",
    "                    title=f\"BIN {bin_value} — Mirror Negative (|neg|) {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_negabs_{key}.png\"))\n",
    "\n",
    "        bin_summary[key] = pair_summary\n",
    "\n",
    "    # Save per-bin summary\n",
    "    with open(os.path.join(bin_dir, \"cosine_summary_all_pairs.json\"), \"w\") as f:\n",
    "        json.dump(bin_summary, f, indent=2)\n",
    "\n",
    "    overall_summary[str(bin_value)] = bin_summary\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Save the full multi-bin summary\n",
    "# ---------------------------------------------------------------------\n",
    "with open(os.path.join(OUT_DIR, \"ALL_BINS_cosine_summary.json\"), \"w\") as f:\n",
    "    json.dump(overall_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(f\"Outputs rooted at: {OUT_DIR}\")\n",
    "print(f\"Plots rooted at:   {PLOT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb07f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Repro defaults (library nondeterminism tamed as much as feasible)\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal helper kept for completeness (unused in main flow)\n",
    "# ---------------------------------------------------------------------\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Gradient helpers\n",
    "# ---------------------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _log_odds_grad_for_model(x1, model, class_a, class_b, eps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        log_odds = tf.math.log(p[:, class_a] + eps) - tf.math.log(p[:, class_b] + eps)\n",
    "    g = tape.gradient(log_odds, x1)    # (1, D)\n",
    "    return tf.squeeze(g, axis=0)       # (D,)\n",
    "\n",
    "def compute_avg_logodds_gradient_for_pair(X: tf.Tensor, models: list, class_a: int, class_b: int, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Average input gradient of log p(class_a|x) - log p(class_b|x) across samples and models.\n",
    "    Returns (D,) tensor (float32 NumPy when .numpy()).\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _log_odds_grad_for_model(x_i, m, class_a, class_b, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utils: model, cosine, mirror plotting\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y drawn above baseline; bottom_y mirrored below (negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, color=\"k\", linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (600..1600 at 0.1 step)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "CSV_PATH   = r\"F:/casts/dataset_half.csv\"  # <- adjust\n",
    "EPOCHS     = 50\n",
    "BATCH_SIZE = 32\n",
    "K_SPLITS   = 5\n",
    "N_REPEATS  = 10\n",
    "\n",
    "# Two independent 5x10 runs per bin:\n",
    "RAND_INTS  = [100, 777]\n",
    "\n",
    "# Class pairs you wanted:\n",
    "CLASS_PAIRS = [(1, 0), (2, 0), (3, 0)]\n",
    "\n",
    "# Optionally limit how many bins to process (None = all)\n",
    "BINS_LIMIT = None  # e.g., set to 3 for a quick smoke test\n",
    "\n",
    "OUT_DIR  = \"./all_bins_two_run_compare_5x10_batches\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load once; enumerate bin values\n",
    "# ---------------------------------------------------------------------\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "# all_bins = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "\n",
    "# if BINS_LIMIT is not None:\n",
    "#     all_bins = all_bins[:int(BINS_LIMIT)]\n",
    "\n",
    "# print(f\"Discovered {len(all_bins)} bin(s): {all_bins}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "all_bins = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])[-6:]  # last 6 only\n",
    "print(f\"Discovered {df['bin'].nunique()} unique bin(s) total; running last {len(all_bins)}: {all_bins}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-run trainer that returns averaged grads for requested pairs\n",
    "#  - Cleans up models/graph/tensors BEFORE returning to free memory\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_avg_grads(X: np.ndarray, Y: np.ndarray, rand_int_base: int, pairs: list, num_classes: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[train_idx], Y[train_idx]\n",
    "        X_va, y_va = X[val_idx], Y[val_idx]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = rand_int_base * fold + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {rand_int_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total so far: {len(all_models)})\")\n",
    "\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    grads = {}\n",
    "    for (a, b) in pairs:\n",
    "        if a >= num_classes or b >= num_classes:\n",
    "            print(f\"  Skipping pair ({a} vs {b}) — class index out of range (num_classes={num_classes}).\")\n",
    "            continue\n",
    "        grads[(a, b)] = compute_avg_logodds_gradient_for_pair(X_tensor, all_models, class_a=a, class_b=b).numpy()\n",
    "\n",
    "    # ---------- Aggressive cleanup of models/graph ----------\n",
    "    try:\n",
    "        for m in all_models:\n",
    "            del m\n",
    "    except Exception:\n",
    "        pass\n",
    "    del all_models, X_tensor\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    return grads\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-bin processing\n",
    "# ---------------------------------------------------------------------\n",
    "def process_single_bin(bin_value, overall_summary):\n",
    "    print(f\"\\n================= BIN {bin_value} =================\")\n",
    "    # Filter this bin and normalize columns except ['bin','target'] by (max+1)\n",
    "    fdf = df[df[\"bin\"] == bin_value].copy()\n",
    "    if fdf.empty:\n",
    "        print(f\"  Bin {bin_value}: no rows — skipping.\")\n",
    "        return\n",
    "\n",
    "    cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "    fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "    fdf = fdf.drop(columns=['bin'])\n",
    "\n",
    "    X_df = fdf.copy()\n",
    "    Y = X_df.pop(\"target\").to_numpy()\n",
    "    X = np.nan_to_num(X_df.to_numpy(), copy=False)\n",
    "\n",
    "    input_dim   = X.shape[1]\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "\n",
    "    if input_dim < 1 or X.shape[0] < 2:\n",
    "        print(f\"  Bin {bin_value}: insufficient data (samples={X.shape[0]}, dim={input_dim}) — skipping.\")\n",
    "        return\n",
    "\n",
    "    # Build the x-grid up to min(10000, input_dim)\n",
    "    n_grid = min(10000, input_dim)\n",
    "    x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)  # len n_grid\n",
    "\n",
    "    # Two independent runs (each returns {(a,b): grad_vec})\n",
    "    avg_A = train_and_avg_grads(X, Y, RAND_INTS[0], CLASS_PAIRS, num_classes)\n",
    "    avg_B = train_and_avg_grads(X, Y, RAND_INTS[1], CLASS_PAIRS, num_classes)\n",
    "\n",
    "    bin_summary = {}\n",
    "    bin_dir      = os.path.join(OUT_DIR, f\"bin_{bin_value}\")\n",
    "    bin_plot_dir = os.path.join(PLOT_DIR, f\"bin_{bin_value}\")\n",
    "    os.makedirs(bin_dir, exist_ok=True)\n",
    "    os.makedirs(bin_plot_dir, exist_ok=True)\n",
    "\n",
    "    for (a, b) in CLASS_PAIRS:\n",
    "        key = f\"{a}v{b}\"\n",
    "        if (a, b) not in avg_A or (a, b) not in avg_B:\n",
    "            print(f\"  Bin {bin_value}: pair {key} missing — skipped.\")\n",
    "            continue\n",
    "\n",
    "        yA_full = avg_A[(a, b)].flatten()[:n_grid]\n",
    "        yB_full = avg_B[(a, b)].flatten()[:n_grid]\n",
    "\n",
    "        # Channels: pos keep; neg -> absolute value\n",
    "        yA_pos = np.where(yA_full > 0, yA_full, 0.0)\n",
    "        yB_pos = np.where(yB_full > 0, yB_full, 0.0)\n",
    "        yA_neg = np.where(yA_full < 0, -yA_full, 0.0)\n",
    "        yB_neg = np.where(yB_full < 0, -yB_full, 0.0)\n",
    "\n",
    "        # Cosines\n",
    "        cos_full = cosine_sim(yA_pos + yA_neg, yB_pos + yB_neg)\n",
    "        cos_pos  = cosine_sim(yA_pos, yB_pos)\n",
    "        cos_neg  = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "        print(f\"  Bin {bin_value} — {key}: cos_full={cos_full:.6f}  cos_pos={cos_pos:.6f}  cos_neg={cos_neg:.6f}\")\n",
    "\n",
    "        # Save vectors for this bin/pair\n",
    "        pair_dir = os.path.join(bin_dir, key)\n",
    "        pair_plot_dir = os.path.join(bin_plot_dir, key)\n",
    "        os.makedirs(pair_dir, exist_ok=True)\n",
    "        os.makedirs(pair_plot_dir, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"m/z\": x_grid,\n",
    "            \"yA_pos\": yA_pos,\n",
    "            \"yA_neg_abs\": yA_neg,\n",
    "            \"yB_pos\": yB_pos,\n",
    "            \"yB_neg_abs\": yB_neg,\n",
    "        }).to_csv(os.path.join(pair_dir, f\"run_vectors_{key}.csv\"), index=False)\n",
    "\n",
    "        # Pair-level JSON\n",
    "        pair_summary = {\"cos_full\": cos_full, \"cos_pos\": cos_pos, \"cos_neg\": cos_neg}\n",
    "        with open(os.path.join(pair_dir, f\"cosine_summary_{key}.json\"), \"w\") as f:\n",
    "            json.dump(pair_summary, f, indent=2)\n",
    "\n",
    "        # Plots\n",
    "        mirror_plot(x_grid, yA_pos + yA_neg, yB_pos + yB_neg,\n",
    "                    title=f\"BIN {bin_value} — Mirror Full (pos+|neg|) {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_full_{key}.png\"))\n",
    "        mirror_plot(x_grid, yA_pos, yB_pos,\n",
    "                    title=f\"BIN {bin_value} — Mirror Positive {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_pos_{key}.png\"))\n",
    "        mirror_plot(x_grid, yA_neg, yB_neg,\n",
    "                    title=f\"BIN {bin_value} — Mirror Negative (|neg|) {key}\",\n",
    "                    outfile=os.path.join(pair_plot_dir, f\"mirror_negabs_{key}.png\"))\n",
    "\n",
    "        bin_summary[key] = pair_summary\n",
    "\n",
    "    # Save per-bin summary\n",
    "    with open(os.path.join(bin_dir, \"cosine_summary_all_pairs.json\"), \"w\") as f:\n",
    "        json.dump(bin_summary, f, indent=2)\n",
    "\n",
    "    overall_summary[str(bin_value)] = bin_summary\n",
    "\n",
    "    # -------- Per-bin cleanup (free CPU/GPU/TF graph memory) --------\n",
    "    try:\n",
    "        del fdf, X_df, X, Y, x_grid, yA_full, yB_full, yA_pos, yB_pos, yA_neg, yB_neg, avg_A, avg_B\n",
    "    except Exception:\n",
    "        pass\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Batch runner: process bins in groups of 4 with cleanup between batches\n",
    "# ---------------------------------------------------------------------\n",
    "def process_bins_in_batches(bins, batch_size=4):\n",
    "    overall_summary = {}\n",
    "    n = len(bins)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = bins[i:i+batch_size]\n",
    "        print(\"\\n\" + \"=\"*68)\n",
    "        print(f\"Processing bin batch {i//batch_size + 1} of {(n + batch_size - 1)//batch_size}: {batch}\")\n",
    "        print(\"=\"*68)\n",
    "\n",
    "        for b in batch:\n",
    "            process_single_bin(b, overall_summary)\n",
    "\n",
    "        # -------- Between-batch cleanup --------\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        # ---------------------------------------\n",
    "\n",
    "    # Save the full multi-bin summary\n",
    "    with open(os.path.join(OUT_DIR, \"ALL_BINS_cosine_summary.json\"), \"w\") as f:\n",
    "        json.dump(overall_summary, f, indent=2)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Outputs rooted at: {OUT_DIR}\")\n",
    "    print(f\"Plots rooted at:   {PLOT_DIR}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Entry point\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    process_bins_in_batches(all_bins, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fb8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 45: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _group_logodds_grad_for_model at 0x0000027AF7F36520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _group_logodds_grad_for_model at 0x0000027AF7F36520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "\n",
      "Saved:\n",
      "  grads_csv: ./bin45_group_compare_two_runs\\bin45_grads_AB.csv\n",
      "  grad_runA_npy: ./bin45_group_compare_two_runs\\bin45_grad_runA.npy\n",
      "  grad_runB_npy: ./bin45_group_compare_two_runs\\bin45_grad_runB.npy\n",
      "  mirror_pos_png: ./bin45_group_compare_two_runs\\plots\\bin45_mirror_pos.png\n",
      "  mirror_negabs_png: ./bin45_group_compare_two_runs\\plots\\bin45_mirror_negabs.png\n",
      "\n",
      "Cosine similarities — positive: 0.760823   negative(|abs|): 0.910192\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient (keep model as-is, multi-class)\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    \"\"\"\n",
    "    Gradient wrt inputs of log(sum_{i in pos_ids} p_i) - log(sum_{j in neg_ids} p_j).\n",
    "    x1: (1, D)\n",
    "    \"\"\"\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(\n",
    "    X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Average input gradient across samples and models.\n",
    "    Returns (D,) np.ndarray.\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model (unchanged)\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Cosine & plotting helpers\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y drawn above baseline; bottom_y mirrored below (negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Config (adjust path as needed)\n",
    "# ----------------------------\n",
    "CSV_PATH   = r\"F:/casts/dataset_rt.csv\"  # <- adjust\n",
    "BIN_VALUE  = 45\n",
    "EPOCHS     = 50\n",
    "BATCH_SIZE = 32\n",
    "K_SPLITS   = 5\n",
    "N_REPEATS  = 10\n",
    "\n",
    "# Two independent runs\n",
    "SEED_BASES = [111, 777]\n",
    "\n",
    "OUT_DIR  = \"./bin45_group_compare_two_runs\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Training (multi-class) and gradient extraction for one run\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "def run_once_get_gradient(X: np.ndarray, Y: np.ndarray, seed_base: int) -> np.ndarray:\n",
    "    models = train_kfold_repeats(X, Y, seed_base)\n",
    "    avg_grad = compute_avg_group_logodds_gradient(X, models, pos_ids=(2,3), neg_ids=(0,1), eps=1e-8)\n",
    "    # cleanup\n",
    "    try:\n",
    "        for m in models: del m\n",
    "    except Exception:\n",
    "        pass\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    return avg_grad  # (D,)\n",
    "\n",
    "# ----------------------------\n",
    "# Main: filter bin=45, train twice, plot mirrors\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load and filter\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "    if fdf.empty:\n",
    "        raise ValueError(f\"No rows found for bin == {BIN_VALUE} in {CSV_PATH}\")\n",
    "\n",
    "    # Normalize all features except ['bin','target'] by (max+1)\n",
    "    cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "    fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "    # Keep only 0..3\n",
    "    fdf = fdf[fdf[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "    Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "    X = np.nan_to_num(fdf.drop(columns=['bin', 'target']).to_numpy(), copy=False)\n",
    "\n",
    "    if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "        raise ValueError(f\"Insufficient data (samples={X.shape[0]}, dim={X.shape[1]}).\")\n",
    "\n",
    "    print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "          f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "    # Two independent runs\n",
    "    grad_A = run_once_get_gradient(X, Y, seed_base=SEED_BASES[0])\n",
    "    grad_B = run_once_get_gradient(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "    # Save raw gradients\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    np.save(os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grad_runA.npy\"), grad_A)\n",
    "    np.save(os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grad_runB.npy\"), grad_B)\n",
    "    # CSV with truncated grid for convenience\n",
    "    n_grid = min(10000, min(grad_A.size, grad_B.size))\n",
    "    x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "    pd.DataFrame({\"m/z\": x_grid, \"grad_runA\": grad_A[:n_grid], \"grad_runB\": grad_B[:n_grid]}).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grads_AB.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Split into pos / neg(abs)\n",
    "    yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "    yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "    yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "    yA_neg = np.where(yA < 0, -yA, 0.0)  # absolute value\n",
    "    yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "    # Cosine similarities (for titles/logs)\n",
    "    cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "    cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "    # Mirror plots (positive / negative)\n",
    "    pos_title = f\"Bin {BIN_VALUE} — Mirror Positive Gradients  (cos={cos_pos:.4f})\"\n",
    "    neg_title = f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs|  (cos={cos_neg:.4f})\"\n",
    "\n",
    "    mirror_plot(\n",
    "        x_grid, yA_pos, yB_pos,\n",
    "        title=pos_title,\n",
    "        outfile=os.path.join(PLOT_DIR, f\"bin{BIN_VALUE}_mirror_pos.png\")\n",
    "    )\n",
    "    mirror_plot(\n",
    "        x_grid, yA_neg, yB_neg,\n",
    "        title=neg_title,\n",
    "        outfile=os.path.join(PLOT_DIR, f\"bin{BIN_VALUE}_mirror_negabs.png\")\n",
    "    )\n",
    "\n",
    "    # Save small JSON summary\n",
    "    summary = {\n",
    "        \"bin\": BIN_VALUE,\n",
    "        \"comparison\": \"log(p2+p3) - log(p0+p1)\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"k_splits\": K_SPLITS,\n",
    "        \"n_repeats\": N_REPEATS,\n",
    "        \"seed_bases\": SEED_BASES,\n",
    "        \"cosine_pos\": cos_pos,\n",
    "        \"cosine_neg_abs\": cos_neg,\n",
    "        \"paths\": {\n",
    "            \"grads_csv\": os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grads_AB.csv\"),\n",
    "            \"grad_runA_npy\": os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grad_runA.npy\"),\n",
    "            \"grad_runB_npy\": os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_grad_runB.npy\"),\n",
    "            \"mirror_pos_png\": os.path.join(PLOT_DIR, f\"bin{BIN_VALUE}_mirror_pos.png\"),\n",
    "            \"mirror_negabs_png\": os.path.join(PLOT_DIR, f\"bin{BIN_VALUE}_mirror_negabs.png\"),\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, f\"bin{BIN_VALUE}_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    for k, v in summary[\"paths\"].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nCosine similarities — positive: {cos_pos:.6f}   negative(|abs|): {cos_neg:.6f}\")\n",
    "\n",
    "    # final cleanup\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
