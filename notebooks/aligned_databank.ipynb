{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1663b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def _decode_bytes_inplace(df: pd.DataFrame) -> None:\n",
    "    for c in df.columns:\n",
    "        dt = df[c].dtype\n",
    "        if dt == object or str(dt).startswith(\"|S\"):\n",
    "            df[c] = df[c].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x)\n",
    "\n",
    "def _pick(df: pd.DataFrame, *cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of {cands} found in {df.columns.tolist()}\")\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame) -> None:\n",
    "    # sample_name\n",
    "    if \"sample_name\" not in df.columns:\n",
    "        s = _pick(df, \"sample_name\", \"file_name\", \"raw_name\", \"run_name\")\n",
    "        df[\"sample_name\"] = df[s].astype(str)\n",
    "    # group_name (optional)\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "    # retention time (keep original spelling for compatibility with your code)\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"retention_time\"].astype(float)\n",
    "        elif {\"rt_min\",\"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (df[\"rt_min\"].astype(float)+df[\"rt_max\"].astype(float))/2.0\n",
    "        elif \"rt_min\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt_min\"].astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt\"].astype(float)\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' in metadata\")\n",
    "\n",
    "def _build_align_functions_from_drift_csv(drift_csv_path: str):\n",
    "    \"\"\"\n",
    "    Returns dict: run_name -> f(rt) giving drift (min) at given RT.\n",
    "    Expects columns: target_name, bin_center_min, avg_rt_drift\n",
    "    Edge behavior: hold first/last values constant.\n",
    "    \"\"\"\n",
    "    dt = pd.read_csv(drift_csv_path)\n",
    "    # tolerate different casings\n",
    "    rename_map = {}\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in dt.columns:\n",
    "            for c in dt.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename_map[c] = need\n",
    "    if rename_map:\n",
    "        dt = dt.rename(columns=rename_map)\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in dt.columns:\n",
    "            raise KeyError(f\"Drift CSV missing '{need}'\")\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in dt.groupby(\"target_name\"):\n",
    "        x = np.asarray(grp[\"bin_center_min\"], dtype=float)\n",
    "        y = np.asarray(grp[\"avg_rt_drift\"], dtype=float)\n",
    "        if x.size == 0:\n",
    "            continue\n",
    "        order = np.argsort(x)\n",
    "        x = x[order]; y = y[order]\n",
    "        if x.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            # bind x,y to the closure now\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, dtype=float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(x, y)\n",
    "    return fns\n",
    "\n",
    "def _safe_stem(path: str) -> str:\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]+', \"_\", stem)\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def bin_ms1_npz_with_alignment(\n",
    "    npz_path: str,\n",
    "    drift_csv_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Load an NPZ (e.g., TreatmentA.ms1.npz) with:\n",
    "      - ms1_matrix: (N_scans, N_bins)\n",
    "      - metadata arrays per column (same length N_scans)\n",
    "    Apply alignment using drift CSV (per-run drift vs reference),\n",
    "    then aggregate into fixed aligned-RT bins with overlap.\n",
    "\n",
    "    Output: one CSV with 8 rows (one per 10-min bin), columns:\n",
    "      ['group_name','rt_start_min','rt_end_min','expanded_start_min','expanded_end_min',\n",
    "       'rt_center_min','n_scans', 'cast_00000'..]\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "    if not os.path.exists(drift_csv_path):\n",
    "        raise FileNotFoundError(drift_csv_path)\n",
    "\n",
    "    # ---- Load NPZ ----\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]           # (N, L)\n",
    "    meta_keys = [k for k in z.files if k != \"ms1_matrix\"]\n",
    "    metadata = pd.DataFrame({k: z[k] for k in meta_keys})\n",
    "    _decode_bytes_inplace(metadata)\n",
    "    _ensure_cols(metadata)\n",
    "\n",
    "    if len(metadata) != MS1.shape[0]:\n",
    "        raise ValueError(f\"Row mismatch: metadata={len(metadata)} vs ms1_matrix={MS1.shape[0]}\")\n",
    "\n",
    "    N_BINS = MS1.shape[1]\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(N_BINS)]\n",
    "\n",
    "    # ---- Build alignment functions from drift table ----\n",
    "    align_fns = _build_align_functions_from_drift_csv(drift_csv_path)\n",
    "    zero = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "\n",
    "    # ---- Compute aligned RT per scan ----\n",
    "    rt_raw   = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    samples  = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "\n",
    "    rt_corr = np.zeros_like(rt_raw, dtype=float)\n",
    "    for run in np.unique(samples):\n",
    "        f = align_fns.get(run, zero)  # if ref or unknown -> zero drift\n",
    "        m = (samples == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # ---- Fixed bins: 8 Ã— 10 min over [0, 80)\n",
    "    starts = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends   = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    # clip expanded windows to data bounds for safety\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    rows = []\n",
    "    for t0, t1, mid in zip(starts, ends, centers):\n",
    "        win_start = max(t0 - overlap, rt_min)\n",
    "        win_end   = min(t1 + overlap, rt_max)\n",
    "\n",
    "        mask = (rt_aligned >= win_start) & (rt_aligned < win_end)\n",
    "        n_scans = int(mask.sum())\n",
    "\n",
    "        if n_scans > 0:\n",
    "            # Sum with higher precision then cast down\n",
    "            vec = MS1[mask].sum(axis=0, dtype=np.float64).astype(np.float32, copy=False)\n",
    "            rt_obs_min = float(rt_aligned[mask].min())\n",
    "            rt_obs_max = float(rt_aligned[mask].max())\n",
    "        else:\n",
    "            vec = np.zeros(N_BINS, dtype=np.float32)\n",
    "            rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "        group_name = str(metadata.get(\"group_name\", \"Unknown\").iloc[0]) if len(metadata) else \"Unknown\"\n",
    "        meta = [group_name, t0, t1, win_start, win_end, mid, n_scans, rt_obs_min, rt_obs_max]\n",
    "        rows.append(meta + vec.tolist())\n",
    "\n",
    "    # ---- Write CSV ----\n",
    "    col_meta = [\n",
    "        \"group_name\",\n",
    "        \"rt_start_min\", \"rt_end_min\",\n",
    "        \"expanded_start_min\", \"expanded_end_min\",\n",
    "        \"rt_center_min\", \"n_scans\",\n",
    "        \"rt_aligned_min_obs\", \"rt_aligned_max_obs\",\n",
    "    ]\n",
    "    out_df = pd.DataFrame(rows, columns=col_meta + cast_cols)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f76c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _resolve_csv(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    if not os.path.splitext(path)[1] and os.path.exists(path + \".csv\"):\n",
    "        return path + \".csv\"\n",
    "    raise FileNotFoundError(f\"Drift file not found: {path} (also tried {path+'.csv'})\")\n",
    "\n",
    "def _decode_bytes_inplace(df: pd.DataFrame) -> None:\n",
    "    for c in df.columns:\n",
    "        dt = df[c].dtype\n",
    "        if dt == object or str(dt).startswith(\"|S\"):\n",
    "            df[c] = df[c].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x)\n",
    "\n",
    "def _pick(df: pd.DataFrame, *cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of {cands} found in {df.columns.tolist()}\")\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame) -> None:\n",
    "    if \"sample_name\" not in df.columns:\n",
    "        s = _pick(df, \"sample_name\", \"file_name\", \"raw_name\", \"run_name\")\n",
    "        df[\"sample_name\"] = df[s].astype(str)\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"retention_time\"].astype(float)\n",
    "        elif {\"rt_min\",\"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (df[\"rt_min\"].astype(float) + df[\"rt_max\"].astype(float)) / 2.0\n",
    "        elif \"rt_min\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt_min\"].astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt\"].astype(float)\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' from metadata\")\n",
    "\n",
    "def _build_align_functions_from_drift(drift_path: str):\n",
    "    \"\"\"\n",
    "    Supports TWO formats:\n",
    "\n",
    "    1) Long table with columns: target_name, bin_center_min, avg_rt_drift\n",
    "    2) Wide matrix: rows=runs (index), columns=bins (bin centers as headers)\n",
    "\n",
    "    Returns dict: run_name -> f(rt) => drift (minutes)\n",
    "    \"\"\"\n",
    "    p = _resolve_csv(drift_path)\n",
    "\n",
    "    # try wide matrix first (index=run names)\n",
    "    try:\n",
    "        wide = pd.read_csv(p, index_col=0)\n",
    "        # columns should be numeric bin centers (or strings convertible to float)\n",
    "        cols_numeric = []\n",
    "        ok = True\n",
    "        for c in wide.columns:\n",
    "            try:\n",
    "                cols_numeric.append(float(c))\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and wide.shape[1] > 0:\n",
    "            col_vals = np.array(cols_numeric, dtype=float)\n",
    "            fns = {}\n",
    "            for run, row in wide.iterrows():\n",
    "                y = row.to_numpy(dtype=float)\n",
    "                # drop NaNs where possible\n",
    "                mask = ~np.isnan(col_vals) & ~np.isnan(y)\n",
    "                x = col_vals[mask]\n",
    "                y = y[mask]\n",
    "                if x.size == 0:\n",
    "                    continue\n",
    "                order = np.argsort(x)\n",
    "                x = x[order]; y = y[order]\n",
    "                if x.size == 1:\n",
    "                    c = float(y[0])\n",
    "                    fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "                else:\n",
    "                    def make_f(xv, yv):\n",
    "                        def f(rt):\n",
    "                            rt = np.asarray(rt, dtype=float)\n",
    "                            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                        return f\n",
    "                    fns[str(run)] = make_f(x, y)\n",
    "            if fns:\n",
    "                return fns\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: long format\n",
    "    long = pd.read_csv(p)\n",
    "    rename = {}\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            for c in long.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename[c] = need\n",
    "    if rename:\n",
    "        long = long.rename(columns=rename)\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            raise KeyError(f\"Drift file missing column '{need}' (after trying wide+long formats).\")\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in long.groupby(\"target_name\"):\n",
    "        x = np.asarray(grp[\"bin_center_min\"], dtype=float)\n",
    "        y = np.asarray(grp[\"avg_rt_drift\"], dtype=float)\n",
    "        order = np.argsort(x)\n",
    "        x = x[order]; y = y[order]\n",
    "        if x.size == 0:\n",
    "            continue\n",
    "        if x.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, dtype=float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(x, y)\n",
    "    return fns\n",
    "\n",
    "def _safe_stem(path: str) -> str:\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]+', \"_\", stem)\n",
    "\n",
    "# ---------- main ----------\n",
    "def bin_ms1_npz_with_alignment(\n",
    "    npz_path: str,\n",
    "    drift_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sum MS1 spectra within aligned RT windows (8 bins Ã— 10 min, Â±2.5 min overlap).\n",
    "    - npz_path must contain: 'ms1_matrix' + metadata arrays (same length)\n",
    "    - drift_path: wide runsÃ—bins matrix OR long drift table\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]  # shape: (N, L)\n",
    "\n",
    "    # metadata\n",
    "    meta_keys = [k for k in z.files if k != \"ms1_matrix\"]\n",
    "    metadata = pd.DataFrame({k: z[k] for k in meta_keys})\n",
    "    _decode_bytes_inplace(metadata)\n",
    "    _ensure_cols(metadata)\n",
    "\n",
    "    if len(metadata) != MS1.shape[0]:\n",
    "        raise ValueError(f\"Row mismatch: metadata={len(metadata)} vs ms1_matrix={MS1.shape[0]}\")\n",
    "\n",
    "    N_BINS = MS1.shape[1]\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(N_BINS)]\n",
    "\n",
    "    # alignment functions (per run)\n",
    "    align_fns = _build_align_functions_from_drift(drift_path)\n",
    "    zero = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "\n",
    "    # aligned RT per scan\n",
    "    rt_raw  = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    runs    = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "    rt_corr = np.zeros_like(rt_raw, dtype=float)\n",
    "\n",
    "    for run in np.unique(runs):\n",
    "        f = align_fns.get(run, zero)  # ref or unknown -> zero\n",
    "        m = (runs == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # fixed 8 bins: [0,80) with Â±2.5 min overlap\n",
    "    starts  = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends    = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    rows = []\n",
    "    for t0, t1, mid in zip(starts, ends, centers):\n",
    "        win_start = max(t0 - overlap, rt_min)\n",
    "        win_end   = min(t1 + overlap, rt_max)\n",
    "        mask = (rt_aligned >= win_start) & (rt_aligned < win_end)\n",
    "        n_scans = int(mask.sum())\n",
    "\n",
    "        if n_scans > 0:\n",
    "            vec = MS1[mask].sum(axis=0, dtype=np.float64).astype(np.float32, copy=False)\n",
    "            rt_obs_min = float(rt_aligned[mask].min())\n",
    "            rt_obs_max = float(rt_aligned[mask].max())\n",
    "        else:\n",
    "            vec = np.zeros(N_BINS, dtype=np.float32)\n",
    "            rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "        group_name = str(metadata.get(\"group_name\", \"Unknown\").iloc[0]) if len(metadata) else \"Unknown\"\n",
    "        rows.append([group_name, t0, t1, win_start, win_end, mid, n_scans, rt_obs_min, rt_obs_max] + vec.tolist())\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"group_name\",\n",
    "            \"rt_start_min\",\"rt_end_min\",\n",
    "            \"expanded_start_min\",\"expanded_end_min\",\n",
    "            \"rt_center_min\",\"n_scans\",\n",
    "            \"rt_aligned_min_obs\",\"rt_aligned_max_obs\"\n",
    "        ] + cast_cols\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n",
    "\n",
    "# ---------- run with your paths ----------\n",
    "if __name__ == \"__main__\":\n",
    "    npz_path   = r\"F:\\casts\\databank\\TreatmentA.ms1.npz\"\n",
    "    drift_path = r\"F:\\casts\\databank\\rt_drifts_matrix\"   # will auto-try .csv\n",
    "    out_csv    = r\"F:\\casts\\databank\\TreatmentA_aligned_bins.csv\"\n",
    "\n",
    "    wrote = bin_ms1_npz_with_alignment(\n",
    "        npz_path=npz_path,\n",
    "        drift_path=drift_path,\n",
    "        out_csv_path=out_csv,\n",
    "        bin_width=10.0,\n",
    "        overlap=2.5,\n",
    "        num_bins=8\n",
    "    )\n",
    "    print(\"Saved:\", wrote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9772236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _resolve_csv(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    root, ext = os.path.splitext(path)\n",
    "    if not ext and os.path.exists(path + \".csv\"):\n",
    "        return path + \".csv\"\n",
    "    raise FileNotFoundError(f\"Drift file not found: {path} (also tried {path+'.csv'})\")\n",
    "\n",
    "def _decode_bytes_inplace(df: pd.DataFrame) -> None:\n",
    "    for c in df.columns:\n",
    "        dt = df[c].dtype\n",
    "        if dt == object or str(dt).startswith(\"|S\"):\n",
    "            df[c] = df[c].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x)\n",
    "\n",
    "def _pick(df: pd.DataFrame, *cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of {cands} found in {df.columns.tolist()}\")\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame) -> None:\n",
    "    if \"sample_name\" not in df.columns:\n",
    "        s = _pick(df, \"sample_name\", \"file_name\", \"raw_name\", \"run_name\")\n",
    "        df[\"sample_name\"] = df[s].astype(str)\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"retention_time\"].astype(float)\n",
    "        elif {\"rt_min\",\"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (df[\"rt_min\"].astype(float)+df[\"rt_max\"].astype(float))/2.0\n",
    "        elif \"rt_min\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt_min\"].astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt\"].astype(float)\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' from metadata\")\n",
    "\n",
    "def _build_align_functions_from_drift(drift_path: str):\n",
    "    p = _resolve_csv(drift_path)\n",
    "\n",
    "    # Try wide matrix first (rows=runs, cols=bins as numbers)\n",
    "    try:\n",
    "        wide = pd.read_csv(p, index_col=0)\n",
    "        cols = []\n",
    "        ok = True\n",
    "        for c in wide.columns:\n",
    "            try:\n",
    "                cols.append(float(c))\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and wide.shape[1] > 0:\n",
    "            x_all = np.asarray(cols, float)\n",
    "            fns = {}\n",
    "            for run, row in wide.iterrows():\n",
    "                y = row.to_numpy(dtype=float)\n",
    "                mask = ~np.isnan(x_all) & ~np.isnan(y)\n",
    "                x = x_all[mask]; y = y[mask]\n",
    "                if x.size == 0:\n",
    "                    continue\n",
    "                order = np.argsort(x)\n",
    "                x = x[order]; y = y[order]\n",
    "                if x.size == 1:\n",
    "                    c = float(y[0])\n",
    "                    fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "                else:\n",
    "                    def make_f(xv, yv):\n",
    "                        def f(rt):\n",
    "                            rt = np.asarray(rt, float)\n",
    "                            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                        return f\n",
    "                    fns[str(run)] = make_f(x, y)\n",
    "            if fns:\n",
    "                return fns\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: long format\n",
    "    long = pd.read_csv(p)\n",
    "    rename = {}\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            for c in long.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename[c] = need\n",
    "    if rename:\n",
    "        long = long.rename(columns=rename)\n",
    "    for need in (\"target_name\",\"bin_center_min\",\"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            raise KeyError(f\"Drift file missing column '{need}' (after wide+long tries).\")\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in long.groupby(\"target_name\"):\n",
    "        x = np.asarray(grp[\"bin_center_min\"], float)\n",
    "        y = np.asarray(grp[\"avg_rt_drift\"], float)\n",
    "        order = np.argsort(x)\n",
    "        x = x[order]; y = y[order]\n",
    "        if x.size == 0:\n",
    "            continue\n",
    "        if x.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(x, y)\n",
    "    return fns\n",
    "\n",
    "def _safe_metadata_from_npz(z: np.lib.npyio.NpzFile, n_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame from NPZ where some arrays may be scalars or wrong length.\n",
    "    - Keep only 1D arrays of length n_rows\n",
    "    - Broadcast 0D / length-1 arrays\n",
    "    - Skip others (with a print)\n",
    "    \"\"\"\n",
    "    cols = {}\n",
    "    for k in z.files:\n",
    "        if k == \"ms1_matrix\":\n",
    "            continue\n",
    "        arr = z[k]\n",
    "        a = np.asarray(arr, dtype=object)  # don't force numeric yet\n",
    "        if a.ndim == 0:\n",
    "            cols[k] = np.repeat(a.item(), n_rows)\n",
    "        elif a.ndim == 1:\n",
    "            if a.shape[0] == n_rows:\n",
    "                cols[k] = a\n",
    "            elif a.shape[0] == 1:\n",
    "                cols[k] = np.repeat(a[0], n_rows)\n",
    "            else:\n",
    "                print(f\"[skip] '{k}' length {a.shape[0]} != {n_rows}\")\n",
    "        else:\n",
    "            # 2D+ (per-scan vectors etc.) â€” skip for metadata\n",
    "            print(f\"[skip] '{k}' shape {a.shape} not 1D/scalar\")\n",
    "    df = pd.DataFrame(cols)\n",
    "    _decode_bytes_inplace(df)\n",
    "    _ensure_cols(df)\n",
    "    return df\n",
    "\n",
    "def bin_ms1_npz_with_alignment(\n",
    "    npz_path: str,\n",
    "    drift_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8\n",
    ") -> str:\n",
    "    # Load NPZ\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]  # (N, L)\n",
    "    N, L = MS1.shape\n",
    "\n",
    "    # SAFE metadata build\n",
    "    metadata = _safe_metadata_from_npz(z, n_rows=N)\n",
    "\n",
    "    # Alignment functions\n",
    "    align_fns = _build_align_functions_from_drift(drift_path)\n",
    "    zero = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "\n",
    "    # Aligned RT per scan\n",
    "    rt_raw = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    runs   = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "    rt_corr = np.zeros_like(rt_raw, float)\n",
    "    for run in np.unique(runs):\n",
    "        f = align_fns.get(run, zero)\n",
    "        m = (runs == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # Fixed bins: 8 Ã— 10 min with Â±2.5 min overlap\n",
    "    starts  = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends    = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(L)]\n",
    "    rows = []\n",
    "    for t0, t1, mid in zip(starts, ends, centers):\n",
    "        win_start = max(t0 - overlap, rt_min)\n",
    "        win_end   = min(t1 + overlap, rt_max)\n",
    "        mask = (rt_aligned >= win_start) & (rt_aligned < win_end)\n",
    "        n_scans = int(mask.sum())\n",
    "\n",
    "        if n_scans > 0:\n",
    "            vec = MS1[mask].sum(axis=0, dtype=np.float64).astype(np.float32, copy=False)\n",
    "            rt_obs_min = float(rt_aligned[mask].min())\n",
    "            rt_obs_max = float(rt_aligned[mask].max())\n",
    "        else:\n",
    "            vec = np.zeros(L, dtype=np.float32)\n",
    "            rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "        group_name = str(metadata.get(\"group_name\", \"Unknown\").iloc[0]) if len(metadata) else \"Unknown\"\n",
    "        rows.append([group_name, t0, t1, win_start, win_end, mid, n_scans, rt_obs_min, rt_obs_max] + vec.tolist())\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"group_name\",\n",
    "            \"rt_start_min\",\"rt_end_min\",\n",
    "            \"expanded_start_min\",\"expanded_end_min\",\n",
    "            \"rt_center_min\",\"n_scans\",\n",
    "            \"rt_aligned_min_obs\",\"rt_aligned_max_obs\"\n",
    "        ] + cast_cols\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91bf54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] 'ms2_scan' length 336425 != 147057\n",
      "[skip] 'ms2_rt' length 336425 != 147057\n",
      "[skip] 'ms2_precursor_mz' length 336425 != 147057\n",
      "[skip] 'ms2_file_id' length 336425 != 147057\n",
      "[skip] 'file_names_lookup' length 33 != 147057\n",
      "[skip] 'file_paths_lookup' length 33 != 147057\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ('sample_name', 'file_name', 'raw_name', 'run_name') found in ['group_name', 'ms1_scan', 'ms1_rt', 'ms1_file_id']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m drift_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcasts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatabank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrt_drifts_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# auto-tries .csv\u001b[39;00m\n\u001b[0;32m      3\u001b[0m out_csv    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcasts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatabank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTreatmentA_aligned_bins.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m wrote \u001b[38;5;241m=\u001b[39m \u001b[43mbin_ms1_npz_with_alignment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnpz_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpz_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrift_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrift_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved:\u001b[39m\u001b[38;5;124m\"\u001b[39m, wrote)\n",
      "Cell \u001b[1;32mIn[9], line 165\u001b[0m, in \u001b[0;36mbin_ms1_npz_with_alignment\u001b[1;34m(npz_path, drift_path, out_csv_path, bin_width, overlap, num_bins)\u001b[0m\n\u001b[0;32m    162\u001b[0m N, L \u001b[38;5;241m=\u001b[39m MS1\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# SAFE metadata build\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_metadata_from_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Alignment functions\u001b[39;00m\n\u001b[0;32m    168\u001b[0m align_fns \u001b[38;5;241m=\u001b[39m _build_align_functions_from_drift(drift_path)\n",
      "Cell \u001b[1;32mIn[9], line 144\u001b[0m, in \u001b[0;36m_safe_metadata_from_npz\u001b[1;34m(z, n_rows)\u001b[0m\n\u001b[0;32m    142\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cols)\n\u001b[0;32m    143\u001b[0m _decode_bytes_inplace(df)\n\u001b[1;32m--> 144\u001b[0m \u001b[43m_ensure_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m, in \u001b[0;36m_ensure_cols\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cols\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 27\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43m_pick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[s]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36m_pick\u001b[1;34m(df, *cands)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m c\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcands\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ('sample_name', 'file_name', 'raw_name', 'run_name') found in ['group_name', 'ms1_scan', 'ms1_rt', 'ms1_file_id']\""
     ]
    }
   ],
   "source": [
    "npz_path   = r\"F:\\casts\\databank\\TreatmentA.ms1.npz\"\n",
    "drift_path = r\"F:\\casts\\databank\\rt_drifts_matrix\"  # auto-tries .csv\n",
    "out_csv    = r\"F:\\casts\\databank\\TreatmentA_aligned_bins.csv\"\n",
    "\n",
    "wrote = bin_ms1_npz_with_alignment(\n",
    "    npz_path=npz_path,\n",
    "    drift_path=drift_path,\n",
    "    out_csv_path=out_csv,\n",
    "    bin_width=10.0,\n",
    "    overlap=2.5,\n",
    "    num_bins=8\n",
    ")\n",
    "print(\"Saved:\", wrote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926d75ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: this will wipe *everything* you defined in the current session!\n",
    "for var in list(globals().keys()):\n",
    "    if var[0] != \"_\":  # keep built-ins like __name__, __doc__, etc.\n",
    "        del globals()[var]\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44119bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = np.load('F:/casts/databank/TreatmentA.ms1.npz')\n",
    "z.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _resolve_csv(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    root, ext = os.path.splitext(path)\n",
    "    if not ext and os.path.exists(path + \".csv\"):\n",
    "        return path + \".csv\"\n",
    "    raise FileNotFoundError(f\"Drift file not found: {path} (also tried {path+'.csv'})\")\n",
    "\n",
    "def _decode_bytes_arr(a):\n",
    "    if isinstance(a, np.ndarray) and (a.dtype.kind in (\"S\", \"O\")):\n",
    "        out = []\n",
    "        for x in a:\n",
    "            if isinstance(x, (bytes, bytearray)):\n",
    "                try:\n",
    "                    out.append(x.decode(\"utf-8\"))\n",
    "                except Exception:\n",
    "                    out.append(str(x))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return np.array(out, dtype=object)\n",
    "    return a\n",
    "\n",
    "def _safe_metadata_from_npz_with_lut(z: np.lib.npyio.NpzFile, n_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build metadata DataFrame robustly:\n",
    "      - keep 1D arrays with length n_rows\n",
    "      - broadcast 0D/len-1 arrays\n",
    "      - skip others\n",
    "      - create sample_name using file_names_lookup[ms1_file_id] if possible\n",
    "      - create 'retntion time' from ms1_rt\n",
    "    \"\"\"\n",
    "    cols = {}\n",
    "    for k in z.files:\n",
    "        if k == \"ms1_matrix\":\n",
    "            continue\n",
    "        arr = z[k]\n",
    "        # keep raw array for special handling of lookups later\n",
    "        if k in (\"file_names_lookup\", \"file_paths_lookup\"):\n",
    "            cols[k] = arr  # store for later\n",
    "            continue\n",
    "\n",
    "        a = np.asarray(arr)\n",
    "        if a.ndim == 0:\n",
    "            cols[k] = np.repeat(a.item(), n_rows)\n",
    "        elif a.ndim == 1:\n",
    "            if a.shape[0] == n_rows:\n",
    "                cols[k] = a\n",
    "            elif a.shape[0] == 1:\n",
    "                cols[k] = np.repeat(a[0], n_rows)\n",
    "            else:\n",
    "                # skip non-matching lengths (e.g., ms2 arrays)\n",
    "                # print(f\"[skip] '{k}' length {a.shape[0]} != {n_rows}\")\n",
    "                pass\n",
    "        else:\n",
    "            # skip 2D+ (e.g., per-row vectors)\n",
    "            # print(f\"[skip] '{k}' shape {a.shape} not 1D/scalar\")\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame({k: cols[k] for k in cols if k not in (\"file_names_lookup\", \"file_paths_lookup\")})\n",
    "\n",
    "    # decode potential byte columns\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object or str(df[c].dtype).startswith(\"|S\"):\n",
    "            df[c] = pd.Series([x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x for x in df[c]])\n",
    "\n",
    "    # ---- sample_name via lookup if available ----\n",
    "    if \"ms1_file_id\" in df.columns and \"file_names_lookup\" in cols:\n",
    "        fid = pd.Series(df[\"ms1_file_id\"]).astype(int).to_numpy()\n",
    "        names_lut = _decode_bytes_arr(cols[\"file_names_lookup\"])\n",
    "        names_lut = np.asarray(names_lut, dtype=object)\n",
    "        # safe mapping with fallback 'fid_<id>'\n",
    "        fallback = np.array([f\"fid_{i}\" for i in fid], dtype=object)\n",
    "        ok = (fid >= 0) & (fid < names_lut.shape[0])\n",
    "        mapped = fallback.copy()\n",
    "        mapped[ok] = names_lut[fid[ok]]\n",
    "        df[\"sample_name\"] = mapped.astype(str)\n",
    "    else:\n",
    "        # fallbacks: file_name/raw_name/run_name; else synthesize\n",
    "        if \"file_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"file_name\"].astype(str)\n",
    "        elif \"raw_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"raw_name\"].astype(str)\n",
    "        elif \"run_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"run_name\"].astype(str)\n",
    "        elif \"ms1_file_id\" in df.columns:\n",
    "            df[\"sample_name\"] = (\"fid_\" + pd.Series(df[\"ms1_file_id\"]).astype(int).astype(str)).astype(str)\n",
    "        else:\n",
    "            df[\"sample_name\"] = \"UnknownRun\"\n",
    "\n",
    "    # ---- group_name default ----\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "\n",
    "    # ---- retention time (legacy spelling for compatibility) ----\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"ms1_rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"ms1_rt\"]).astype(float)\n",
    "        elif \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"retention_time\"]).astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"rt\"]).astype(float)\n",
    "        elif {\"rt_min\", \"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (pd.Series(df[\"rt_min\"]).astype(float) + pd.Series(df[\"rt_max\"]).astype(float)) / 2.0\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' (looked for ms1_rt, retention_time, rt, rt_min/rt_max).\")\n",
    "\n",
    "    # Ensure string types\n",
    "    df[\"sample_name\"] = df[\"sample_name\"].astype(str)\n",
    "    df[\"group_name\"]  = df[\"group_name\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def _build_align_functions_from_drift(drift_path: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      1) Wide matrix: index=runs, columns=bins (bin centers numeric)\n",
    "      2) Long table : target_name, bin_center_min, avg_rt_drift\n",
    "\n",
    "    Returns (fns, default_fn)\n",
    "    \"\"\"\n",
    "    p = _resolve_csv(drift_path)\n",
    "\n",
    "    # try wide first\n",
    "    try:\n",
    "        wide = pd.read_csv(p, index_col=0)\n",
    "        # columns as numeric bin centers\n",
    "        x_all = []\n",
    "        ok = True\n",
    "        for c in wide.columns:\n",
    "            try:\n",
    "                x_all.append(float(c))\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and len(x_all) > 0:\n",
    "            x_all = np.asarray(x_all, float)\n",
    "            fns = {}\n",
    "            for run, row in wide.iterrows():\n",
    "                y = row.to_numpy(dtype=float)\n",
    "                mask = ~np.isnan(x_all) & ~np.isnan(y)\n",
    "                x = x_all[mask]; y2 = y[mask]\n",
    "                if x.size == 0:\n",
    "                    continue\n",
    "                order = np.argsort(x)\n",
    "                x = x[order]; y2 = y2[order]\n",
    "                if x.size == 1:\n",
    "                    c = float(y2[0])\n",
    "                    fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "                else:\n",
    "                    def make_f(xv, yv):\n",
    "                        def f(rt):\n",
    "                            rt = np.asarray(rt, float)\n",
    "                            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                        return f\n",
    "                    fns[str(run)] = make_f(x, y2)\n",
    "            # default = median across runs\n",
    "            y_med = np.nanmedian(wide.to_numpy(dtype=float), axis=0)\n",
    "            maskm = ~np.isnan(x_all) & ~np.isnan(y_med)\n",
    "            if np.any(maskm):\n",
    "                xm = x_all[maskm]; ym = y_med[maskm]\n",
    "                order = np.argsort(xm); xm = xm[order]; ym = ym[order]\n",
    "                if xm.size == 1:\n",
    "                    default_fn = (lambda c=float(ym[0]): (lambda rt: np.full_like(np.asarray(rt, float), c)))()\n",
    "                else:\n",
    "                    def default_fn(rt, xv=xm, yv=ym):\n",
    "                        rt = np.asarray(rt, float)\n",
    "                        return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "            else:\n",
    "                default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "            return fns, default_fn\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # long format fallback\n",
    "    long = pd.read_csv(p)\n",
    "    rename = {}\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            for c in long.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename[c] = need\n",
    "    if rename:\n",
    "        long = long.rename(columns=rename)\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            raise KeyError(f\"Drift file missing column '{need}'\")\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in long.groupby(\"target_name\"):\n",
    "        x = np.asarray(grp[\"bin_center_min\"], float)\n",
    "        y = np.asarray(grp[\"avg_rt_drift\"], float)\n",
    "        order = np.argsort(x); x = x[order]; y = y[order]\n",
    "        if x.size == 0:\n",
    "            continue\n",
    "        if x.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(x, y)\n",
    "\n",
    "    # default = median across runs per bin center\n",
    "    med = long.groupby(\"bin_center_min\", as_index=False)[\"avg_rt_drift\"].median()\n",
    "    x = med[\"bin_center_min\"].to_numpy(float)\n",
    "    y = med[\"avg_rt_drift\"].to_numpy(float)\n",
    "    order = np.argsort(x); x = x[order]; y = y[order]\n",
    "    if x.size == 0:\n",
    "        default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "    elif x.size == 1:\n",
    "        default_fn = (lambda c=float(y[0]): (lambda rt: np.full_like(np.asarray(rt, float), c)))()\n",
    "    else:\n",
    "        def default_fn(rt, xv=x, yv=y):\n",
    "            rt = np.asarray(rt, float)\n",
    "            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "    return fns, default_fn\n",
    "\n",
    "# --------------- main ---------------\n",
    "def bin_ms1_npz_with_alignment(\n",
    "    npz_path: str,\n",
    "    drift_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8\n",
    ") -> str:\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]\n",
    "    N, L = MS1.shape\n",
    "\n",
    "    metadata = _safe_metadata_from_npz_with_lut(z, n_rows=N)\n",
    "\n",
    "    # build per-run alignment functions + default\n",
    "    align_fns, default_fn = _build_align_functions_from_drift(drift_path)\n",
    "\n",
    "    # aligned RT per scan\n",
    "    rt_raw = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    runs   = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "    rt_corr = np.zeros_like(rt_raw, float)\n",
    "    for run in np.unique(runs):\n",
    "        f = align_fns.get(run, default_fn)  # fallback to median drift curve if run missing\n",
    "        m = (runs == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # fixed bins: [0, 80) with Â±2.5 overlap\n",
    "    starts  = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends    = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    # aggregate\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(L)]\n",
    "    rows = []\n",
    "    for t0, t1, mid in zip(starts, ends, centers):\n",
    "        win_start = max(t0 - overlap, rt_min)\n",
    "        win_end   = min(t1 + overlap, rt_max)\n",
    "        mask = (rt_aligned >= win_start) & (rt_aligned < win_end)\n",
    "        n_scans = int(mask.sum())\n",
    "\n",
    "        if n_scans > 0:\n",
    "            vec = MS1[mask].sum(axis=0, dtype=np.float64).astype(np.float32, copy=False)\n",
    "            rt_obs_min = float(rt_aligned[mask].min())\n",
    "            rt_obs_max = float(rt_aligned[mask].max())\n",
    "        else:\n",
    "            vec = np.zeros(L, dtype=np.float32)\n",
    "            rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "        group_name = str(metadata.get(\"group_name\", \"Unknown\").iloc[0]) if len(metadata) else \"Unknown\"\n",
    "        rows.append([group_name, t0, t1, win_start, win_end, mid, n_scans, rt_obs_min, rt_obs_max] + vec.tolist())\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"group_name\",\n",
    "            \"rt_start_min\",\"rt_end_min\",\n",
    "            \"expanded_start_min\",\"expanded_end_min\",\n",
    "            \"rt_center_min\",\"n_scans\",\n",
    "            \"rt_aligned_min_obs\",\"rt_aligned_max_obs\"\n",
    "        ] + cast_cols\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n",
    "\n",
    "# --------------- run with your paths ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    npz_path   = r\"F:\\casts\\databank\\TreatmentA.ms1.npz\"\n",
    "    drift_path = r\"F:\\casts\\databank\\rt_drifts_matrix\"  # auto-tries .csv\n",
    "    out_csv    = r\"F:\\casts\\databank\\TreatmentA_aligned_bins.csv\"\n",
    "\n",
    "    wrote = bin_ms1_npz_with_alignment(\n",
    "        npz_path=npz_path,\n",
    "        drift_path=drift_path,\n",
    "        out_csv_path=out_csv,\n",
    "        bin_width=10.0,\n",
    "        overlap=2.5,\n",
    "        num_bins=8\n",
    "    )\n",
    "    print(\"Saved:\", wrote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f68fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\casts\\databank\\TreatmentA_aligned_bins_per_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "\n",
    "def _resolve_csv(path: str) -> str:\n",
    "    \"\"\"Return path (or path.csv) if exists; else raise.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    root, ext = os.path.splitext(path)\n",
    "    if not ext and os.path.exists(path + \".csv\"):\n",
    "        return path + \".csv\"\n",
    "    raise FileNotFoundError(f\"Drift file not found: {path}  (also tried {path+'.csv'})\")\n",
    "\n",
    "def _decode_bytes_arr(a):\n",
    "    \"\"\"Decode a 1D array of bytes/objects to str objects.\"\"\"\n",
    "    if isinstance(a, np.ndarray) and (a.dtype.kind in (\"S\", \"O\")):\n",
    "        out = []\n",
    "        for x in a:\n",
    "            if isinstance(x, (bytes, bytearray)):\n",
    "                try:\n",
    "                    out.append(x.decode(\"utf-8\"))\n",
    "                except Exception:\n",
    "                    out.append(str(x))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return np.array(out, dtype=object)\n",
    "    return a\n",
    "\n",
    "def _safe_metadata_from_npz_with_lut(z: np.lib.npyio.NpzFile, n_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust metadata builder:\n",
    "      - keep 1D arrays of length n_rows\n",
    "      - broadcast 0D or length-1 arrays\n",
    "      - skip other shapes/lengths (e.g., ms2_* if mismatched)\n",
    "      - build sample_name from file_names_lookup[ms1_file_id] when available\n",
    "      - set retntion time from ms1_rt\n",
    "    \"\"\"\n",
    "    cols = {}\n",
    "    for k in z.files:\n",
    "        if k == \"ms1_matrix\":\n",
    "            continue\n",
    "        arr = z[k]\n",
    "        # Keep lookups for later mapping\n",
    "        if k in (\"file_names_lookup\", \"file_paths_lookup\"):\n",
    "            cols[k] = arr\n",
    "            continue\n",
    "\n",
    "        a = np.asarray(arr)\n",
    "        if a.ndim == 0:\n",
    "            cols[k] = np.repeat(a.item(), n_rows)\n",
    "        elif a.ndim == 1:\n",
    "            if a.shape[0] == n_rows:\n",
    "                cols[k] = a\n",
    "            elif a.shape[0] == 1:\n",
    "                cols[k] = np.repeat(a[0], n_rows)\n",
    "            else:\n",
    "                # skip mismatched lengths\n",
    "                pass\n",
    "        else:\n",
    "            # skip 2D+\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame({k: cols[k] for k in cols if k not in (\"file_names_lookup\", \"file_paths_lookup\")})\n",
    "\n",
    "    # decode bytes in df columns\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object or str(df[c].dtype).startswith(\"|S\"):\n",
    "            df[c] = pd.Series([x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x for x in df[c]])\n",
    "\n",
    "    # sample_name via file_names_lookup[ms1_file_id] when possible\n",
    "    if \"ms1_file_id\" in df.columns and \"file_names_lookup\" in cols:\n",
    "        fid = pd.Series(df[\"ms1_file_id\"]).astype(int).to_numpy()\n",
    "        names_lut = _decode_bytes_arr(cols[\"file_names_lookup\"])\n",
    "        names_lut = np.asarray(names_lut, dtype=object)\n",
    "        fallback = np.array([f\"fid_{i}\" for i in fid], dtype=object)\n",
    "        ok = (fid >= 0) & (fid < names_lut.shape[0])\n",
    "        mapped = fallback.copy()\n",
    "        mapped[ok] = names_lut[fid[ok]]\n",
    "        df[\"sample_name\"] = mapped.astype(str)\n",
    "    else:\n",
    "        if \"file_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"file_name\"].astype(str)\n",
    "        elif \"raw_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"raw_name\"].astype(str)\n",
    "        elif \"run_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"run_name\"].astype(str)\n",
    "        elif \"ms1_file_id\" in df.columns:\n",
    "            df[\"sample_name\"] = (\"fid_\" + pd.Series(df[\"ms1_file_id\"]).astype(int).astype(str)).astype(str)\n",
    "        else:\n",
    "            df[\"sample_name\"] = \"UnknownRun\"\n",
    "\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "\n",
    "    # retention time (legacy spelling)\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"ms1_rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"ms1_rt\"]).astype(float)\n",
    "        elif \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"retention_time\"]).astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"rt\"]).astype(float)\n",
    "        elif {\"rt_min\", \"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (pd.Series(df[\"rt_min\"]).astype(float) + pd.Series(df[\"rt_max\"]).astype(float)) / 2.0\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' (looked for ms1_rt, retention_time, rt, rt_min/rt_max).\")\n",
    "\n",
    "    df[\"sample_name\"] = df[\"sample_name\"].astype(str)\n",
    "    df[\"group_name\"]  = df[\"group_name\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def _build_align_functions_from_drift(drift_path: str):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      1) Wide matrix CSV: index=runs, columns=bin centers (minutes)\n",
    "      2) Long table  CSV: target_name, bin_center_min, avg_rt_drift\n",
    "    All missing values are filled with 0.\n",
    "    Returns (fns, default_fn) where default_fn is the zero-curve.\n",
    "    \"\"\"\n",
    "    p = _resolve_csv(drift_path)\n",
    "\n",
    "    # Try wide matrix first\n",
    "    try:\n",
    "        wide = pd.read_csv(p, index_col=0)\n",
    "        # convert column names to numeric bin centers\n",
    "        bin_centers = []\n",
    "        ok = True\n",
    "        for c in wide.columns:\n",
    "            try:\n",
    "                bin_centers.append(float(c))\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and len(bin_centers) > 0:\n",
    "            order = np.argsort(bin_centers)\n",
    "            cols_sorted = [wide.columns[i] for i in order]\n",
    "            wide = wide.loc[:, cols_sorted]\n",
    "            x_all = np.array([float(c) for c in cols_sorted], dtype=float)\n",
    "\n",
    "            # fill all missing with 0\n",
    "            wide = wide.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            fns = {}\n",
    "            for run, row in wide.iterrows():\n",
    "                y = row.to_numpy(dtype=float)  # NaNs already 0\n",
    "                if x_all.size == 1:\n",
    "                    c = float(y[0])\n",
    "                    fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "                else:\n",
    "                    def make_f(xv, yv):\n",
    "                        def f(rt):\n",
    "                            rt = np.asarray(rt, float)\n",
    "                            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                        return f\n",
    "                    fns[str(run)] = make_f(x_all, y)\n",
    "\n",
    "            default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "            return fns, default_fn\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: long table\n",
    "    long = pd.read_csv(p)\n",
    "    # normalize headers\n",
    "    rename = {}\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            for c in long.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename[c] = need\n",
    "    if rename:\n",
    "        long = long.rename(columns=rename)\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            raise KeyError(f\"Drift file missing column '{need}'\")\n",
    "\n",
    "    # full grid of bin centers\n",
    "    all_bins = np.sort(long[\"bin_center_min\"].astype(float).unique())\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in long.groupby(\"target_name\"):\n",
    "        # initialize y as zeros (missing -> 0)\n",
    "        y = np.zeros_like(all_bins, dtype=float)\n",
    "        x_run = grp[\"bin_center_min\"].astype(float).to_numpy()\n",
    "        y_run = grp[\"avg_rt_drift\"].astype(float).to_numpy()\n",
    "        # map provided points\n",
    "        idx_map = {bx: i for i, bx in enumerate(all_bins)}\n",
    "        for xr, yr in zip(x_run, y_run):\n",
    "            i = idx_map.get(xr, None)\n",
    "            if i is not None and np.isfinite(yr):\n",
    "                y[i] = yr  # others remain 0\n",
    "\n",
    "        if all_bins.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(all_bins, y)\n",
    "\n",
    "    default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "    return fns, default_fn\n",
    "\n",
    "def _sum_rows_chunked(M, idxs, chunk_rows=1024, out_dtype=np.float32):\n",
    "    \"\"\"Memory-safe sum over selected rows.\"\"\"\n",
    "    if idxs.size == 0:\n",
    "        return np.zeros(M.shape[1], dtype=out_dtype)\n",
    "    acc = np.zeros(M.shape[1], dtype=np.float64)\n",
    "    for s in range(0, idxs.size, chunk_rows):\n",
    "        block = M[idxs[s:s+chunk_rows]]\n",
    "        acc += block.sum(axis=0, dtype=np.float64)\n",
    "    return acc.astype(out_dtype, copy=False)\n",
    "\n",
    "# ------------------ main (per-sample) ------------------\n",
    "\n",
    "def bin_ms1_npz_with_alignment_per_sample(\n",
    "    npz_path: str,\n",
    "    drift_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8,\n",
    "    chunk_rows: int = 1024\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Align per-scan RT using per-run drift curves, then for **each sample_name**\n",
    "    sum MS1 spectra into 8 bins (10 min) with Â±2.5 min overlap on aligned RT.\n",
    "    Missing drift values -> 0. Writes ONE CSV with 8 rows per sample.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]       # shape: (N, L)\n",
    "    N, L = MS1.shape\n",
    "\n",
    "    # metadata with sample_name, group_name, retntion time\n",
    "    metadata = _safe_metadata_from_npz_with_lut(z, n_rows=N)\n",
    "\n",
    "    # build align functions; default is zero-curve\n",
    "    align_fns, default_fn = _build_align_functions_from_drift(drift_path)\n",
    "\n",
    "    # per-scan aligned RT\n",
    "    rt_raw = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    runs   = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "    groups = metadata[\"group_name\"].astype(str).to_numpy()\n",
    "\n",
    "    rt_corr = np.zeros_like(rt_raw, dtype=float)\n",
    "    for run in np.unique(runs):\n",
    "        f = align_fns.get(run, default_fn)  # if run missing -> zero drift\n",
    "        m = (runs == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # fixed bins: [0, 80) stepped by 10, with Â±2.5 overlap on aligned RT\n",
    "    starts  = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends    = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    # Precompute for clipping\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(L)]\n",
    "    rows = []\n",
    "\n",
    "    # ---- PER-SAMPLE LOOP ----\n",
    "    unique_runs = np.unique(runs)\n",
    "    for run in unique_runs:\n",
    "        idx_run = np.flatnonzero(runs == run)\n",
    "        if idx_run.size == 0:\n",
    "            continue\n",
    "\n",
    "        # group label for this run (assume constant within run)\n",
    "        grp_vals = np.unique(groups[idx_run])\n",
    "        group_label = grp_vals[0] if grp_vals.size > 0 else \"Unknown\"\n",
    "\n",
    "        rt_run = rt_aligned[idx_run]\n",
    "\n",
    "        for t0, t1, mid in zip(starts, ends, centers):\n",
    "            win_start = max(t0 - overlap, rt_min)\n",
    "            win_end   = min(t1 + overlap, rt_max)\n",
    "\n",
    "            # indices of this run that fall in the window\n",
    "            mask_local = (rt_run >= win_start) & (rt_run < win_end)\n",
    "            idxs = idx_run[mask_local]\n",
    "            n_scans = int(idxs.size)\n",
    "\n",
    "            if n_scans > 0:\n",
    "                vec = _sum_rows_chunked(MS1, idxs, chunk_rows=chunk_rows, out_dtype=np.float32)\n",
    "                rt_obs_min = float(rt_run[mask_local].min())\n",
    "                rt_obs_max = float(rt_run[mask_local].max())\n",
    "            else:\n",
    "                vec = np.zeros(L, dtype=np.float32)\n",
    "                rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "            # include sample_name so output is per-sample\n",
    "            rows.append([\n",
    "                run, group_label,\n",
    "                t0, t1, win_start, win_end, mid,\n",
    "                n_scans, rt_obs_min, rt_obs_max\n",
    "            ] + vec.tolist())\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"sample_name\", \"group_name\",\n",
    "            \"rt_start_min\",\"rt_end_min\",\n",
    "            \"expanded_start_min\",\"expanded_end_min\",\n",
    "            \"rt_center_min\",\"n_scans\",\n",
    "            \"rt_aligned_min_obs\",\"rt_aligned_max_obs\"\n",
    "        ] + cast_cols\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n",
    "\n",
    "# ------------------ run ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    npz_path   = r\"F:\\casts\\databank\\TreatmentA.ms1.npz\"\n",
    "    drift_path = r\"F:\\casts\\databank\\rt_drifts_matrix\"  # auto-tries .csv\n",
    "    out_csv    = r\"F:\\casts\\databank\\TreatmentA_aligned_bins_per_sample.csv\"\n",
    "\n",
    "    wrote = bin_ms1_npz_with_alignment_per_sample(\n",
    "        npz_path=npz_path,\n",
    "        drift_path=drift_path,\n",
    "        out_csv_path=out_csv,\n",
    "        bin_width=10.0,\n",
    "        overlap=2.5,\n",
    "        num_bins=8,\n",
    "        chunk_rows=1024  # lower if you still see MemoryError\n",
    "    )\n",
    "    print(\"Saved:\", wrote)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
