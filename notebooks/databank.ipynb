{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94c968a",
   "metadata": {},
   "source": [
    "Functions to generate MS1 and MS2 matrixes plus metadata from rawfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99980ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config / binning\n",
    "# -----------------------------\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690   # 600.0 m/z * 10 .. 1935.9 (10 pts per m/z)\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600     # m/z 400..1999 (1 pt per m/z)\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "GROUPS = (\"TreatmentA\", \"TreatmentB\", \"TreatmentC\", \"TreatmentD\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _group_from_name(name: str) -> str:\n",
    "    for g in GROUPS:\n",
    "        if g in name:\n",
    "            return g\n",
    "    return \"Unknown\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _ensure_folder_list(paths):\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        return list(paths)\n",
    "    return [paths]\n",
    "\n",
    "def _gather_raw_files(folder_paths):\n",
    "    folder_list = _ensure_folder_list(folder_paths)\n",
    "    raw_files = []\n",
    "    for fp in folder_list:\n",
    "        fp_abs = os.path.abspath(fp)\n",
    "        if not os.path.isdir(fp_abs):\n",
    "            raise FileNotFoundError(f'Folder not found: \"{fp_abs}\"')\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.raw\")))\n",
    "        raw_files.extend(glob.glob(os.path.join(fp_abs, \"*.RAW\")))\n",
    "    raw_files = sorted(set(os.path.abspath(p) for p in raw_files))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\n",
    "            f'No \".raw\" files found in: {\", \".join(map(os.path.abspath, folder_list))}'\n",
    "        )\n",
    "    return raw_files\n",
    "\n",
    "def _sanitize_metadata_dict(md: dict) -> dict:\n",
    "    \"\"\"Ensure arrays are numeric or Unicode (never object dtype).\"\"\"\n",
    "    safe = {}\n",
    "    for k, v in md.items():\n",
    "        if isinstance(v, (int, float, np.number, np.bool_)):\n",
    "            safe[k] = np.array(v)\n",
    "            continue\n",
    "        if isinstance(v, (list, tuple, np.ndarray)):\n",
    "            arr = np.asarray(v)\n",
    "            if arr.dtype == object:\n",
    "                try:\n",
    "                    arr = arr.astype(np.float32)\n",
    "                except Exception:\n",
    "                    arr = arr.astype(\"U\")\n",
    "            if np.issubdtype(arr.dtype, np.character):\n",
    "                arr = arr.astype(\"U\")\n",
    "            safe[k] = arr\n",
    "            continue\n",
    "        if isinstance(v, str):\n",
    "            safe[k] = np.array(v, dtype=\"U\")\n",
    "            continue\n",
    "        safe[k] = np.array(str(v), dtype=\"U\")\n",
    "    return safe\n",
    "\n",
    "def _out_paths(out_dir: str, group: str):\n",
    "    base = os.path.join(os.path.abspath(out_dir), group)\n",
    "    return (f\"{base}.ms1.npz\", f\"{base}.ms2.npz\", f\"{base}.meta.npz\")\n",
    "\n",
    "# -----------------------------\n",
    "# Core: process one treatment group at a time\n",
    "# -----------------------------\n",
    "def _process_group(group: str, group_files: list, out_dir: str):\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "      - MS1 (float32, UNnormalized) stacked per MS1 scan for this group\n",
    "      - MS2 (float16, per-scan normalized) stacked per MS2 scan for this group\n",
    "      - METADATA aligned to the two matrices\n",
    "    Saves three NPZ files and frees RAM.\n",
    "    \"\"\"\n",
    "    if not group_files:\n",
    "        return None\n",
    "\n",
    "    # Guard: require fisher_py\n",
    "    try:\n",
    "        RawFile, Scan  # type: ignore # noqa\n",
    "    except NameError:\n",
    "        raise ImportError(\"fisher_py is required for RAW access. Uncomment the imports at the top.\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ms1_path, ms2_path, meta_path = _out_paths(out_dir, group)\n",
    "\n",
    "    # Per-group accumulators\n",
    "    file_basenames, file_abspaths = [], []\n",
    "    file_to_id = {}\n",
    "\n",
    "    # MS1\n",
    "    ms1_rows = []                               # list of vectors (float32)\n",
    "    ms1_scan, ms1_rt, ms1_file_id = [], [], []  # aligned to ms1_rows\n",
    "\n",
    "    # MS2\n",
    "    ms2_rows = []                               # list of vectors (float16)\n",
    "    ms2_scan, ms2_rt, ms2_prec_mz, ms2_file_id = [], [], [], []\n",
    "\n",
    "    # Iterate files in this group\n",
    "    for raw_abs in group_files:\n",
    "        raw_name = os.path.basename(raw_abs)\n",
    "        if raw_abs not in file_to_id:\n",
    "            file_to_id[raw_abs] = len(file_basenames)\n",
    "            file_basenames.append(raw_name)\n",
    "            file_abspaths.append(raw_abs)\n",
    "        f_id = file_to_id[raw_abs]\n",
    "\n",
    "        # open RAW\n",
    "        try:\n",
    "            raw = RawFile(raw_abs)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_abs} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for i in tqdm(range(1, total_scans + 1), desc=f\"[{group}] {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", i)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            masses = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            intens = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if masses.size == 0 or intens.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                # Build UNnormalized float32 MS1 row\n",
    "                # Bin at 0.1 m/z: index = round(m/z*10)\n",
    "                idx = np.rint(masses * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v32 = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS1_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                ms1_rows.append(v32)\n",
    "                ms1_scan.append(sc_num)\n",
    "                ms1_rt.append(rt)\n",
    "                ms1_file_id.append(f_id)\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                # Build per-scan normalized MS2 row (float16 for compact size)\n",
    "                # Bin at 1.0 m/z: index = round(m/z)\n",
    "                idx = np.rint(masses).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v32 = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v32, idx[mask] - MS2_MIN_IDX, intens[mask].astype(np.float32, copy=False))\n",
    "                vmax = float(v32.max())\n",
    "                if vmax > 0:\n",
    "                    v32 /= vmax\n",
    "                vec_ms2 = v32.astype(np.float16, copy=False)\n",
    "\n",
    "                # Precursor m/z (fallback to parsing scan_type text)\n",
    "                prec = np.nan\n",
    "                for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "                    if hasattr(raw_scan, attr):\n",
    "                        try:\n",
    "                            prec = float(getattr(raw_scan, attr))\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if np.isnan(prec):\n",
    "                    m = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "                    prec = float(m[1]) if len(m) > 1 else np.nan\n",
    "\n",
    "                ms2_rows.append(vec_ms2)\n",
    "                ms2_scan.append(sc_num)\n",
    "                ms2_rt.append(rt)\n",
    "                ms2_prec_mz.append(prec)\n",
    "                ms2_file_id.append(f_id)\n",
    "\n",
    "        # dispose RAW handle\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Build metadata (per-group) ----\n",
    "    # Note: IDs are per-group (0..n_files_in_group-1)\n",
    "    metadata_raw = dict(\n",
    "        group_name=np.array(group, dtype=\"U\"),\n",
    "\n",
    "        # MS1 row-aligned meta\n",
    "        ms1_scan=np.asarray(ms1_scan, dtype=np.int32),\n",
    "        ms1_rt=np.asarray(ms1_rt, dtype=np.float32),\n",
    "        ms1_file_id=np.asarray(ms1_file_id, dtype=np.int32),\n",
    "\n",
    "        # MS2 row-aligned meta\n",
    "        ms2_scan=np.asarray(ms2_scan, dtype=np.int32),\n",
    "        ms2_rt=np.asarray(ms2_rt, dtype=np.float32),\n",
    "        ms2_precursor_mz=np.asarray(ms2_prec_mz, dtype=np.float32),\n",
    "        ms2_file_id=np.asarray(ms2_file_id, dtype=np.int32),\n",
    "\n",
    "        # Lookups\n",
    "        file_names_lookup=np.asarray(file_basenames, dtype=\"U\"),\n",
    "        file_paths_lookup=np.asarray(file_abspaths, dtype=\"U\"),\n",
    "    )\n",
    "    metadata = _sanitize_metadata_dict(metadata_raw)\n",
    "\n",
    "    # ---- Stack & save (release RAM right after) ----\n",
    "    # MS1 (float32, UNnormalized)\n",
    "    if ms1_rows:\n",
    "        MS1 = np.vstack(ms1_rows).astype(np.float32, copy=False)\n",
    "    else:\n",
    "        MS1 = np.zeros((0, MS1_LEN), dtype=np.float32)\n",
    "    np.savez_compressed(ms1_path, ms1_matrix=MS1, **metadata)\n",
    "    print(f\"[{group}] Saved MS1: {ms1_path}  shape={MS1.shape}, dtype={MS1.dtype}\")\n",
    "    del MS1, ms1_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # MS2 (float16, normalized per scan)\n",
    "    if ms2_rows:\n",
    "        MS2 = np.vstack(ms2_rows).astype(np.float16, copy=False)\n",
    "    else:\n",
    "        MS2 = np.zeros((0, MS2_LEN), dtype=np.float16)\n",
    "    np.savez_compressed(ms2_path, ms2_matrix=MS2, **metadata)\n",
    "    print(f\"[{group}] Saved MS2: {ms2_path}  shape={MS2.shape}, dtype={MS2.dtype}\")\n",
    "    del MS2, ms2_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Save metadata standalone (useful if you want to load meta without matrices)\n",
    "    np.savez_compressed(meta_path, **metadata)\n",
    "    print(f\"[{group}] Saved META: {meta_path}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    del metadata, metadata_raw\n",
    "    gc.collect()\n",
    "\n",
    "    return {\"group\": group, \"ms1\": ms1_path, \"ms2\": ms2_path, \"meta\": meta_path}\n",
    "\n",
    "# -----------------------------\n",
    "# Public API\n",
    "# -----------------------------\n",
    "def wholeCasting_per_group(folder_paths, out_dir: str):\n",
    "    \"\"\"\n",
    "    Scans RAW files, partitions by TreatmentA/B/C/D (using filename contains),\n",
    "    and for each group writes:\n",
    "      <out_dir>/<Group>.ms1.npz  (float32, UNnormalized)\n",
    "      <out_dir>/<Group>.ms2.npz  (float16, per-scan normalized)\n",
    "      <out_dir>/<Group>.meta.npz\n",
    "\n",
    "    RAM is freed between groups.\n",
    "    Returns a dict of outputs keyed by group.\n",
    "    \"\"\"\n",
    "    raw_files = _gather_raw_files(folder_paths)\n",
    "    by_group = {g: [] for g in GROUPS}\n",
    "    for p in raw_files:\n",
    "        g = _group_from_name(os.path.basename(p))\n",
    "        if g in by_group:\n",
    "            by_group[g].append(p)\n",
    "\n",
    "    outputs = {}\n",
    "    for g in GROUPS:\n",
    "        paths = _process_group(g, by_group[g], out_dir)\n",
    "        outputs[g] = paths\n",
    "        # safety: ensure memory is really freed between groups\n",
    "        gc.collect()\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588164d",
   "metadata": {},
   "source": [
    "Calling the wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95399297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Warning: this will wipe *everything* you defined in the current session!\n",
    "for var in list(globals().keys()):\n",
    "    if var[0] != \"_\":  # keep built-ins like __name__, __doc__, etc.\n",
    "        del globals()[var]\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "wholeCasting_per_group([\"F:/TreatmentABC\", \"F:/TreatmentD\"], out_dir=\"F:/casts/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd828755",
   "metadata": {},
   "source": [
    "Combine MS2 matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa466f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "TreatmentA = \"F:/casts/databank/TreatmentA.ms2.npz\"\n",
    "TreatmentB = \"F:/casts/databank/TreatmentB.ms2.npz\"\n",
    "TreatmentC = \"F:/casts/databank/TreatmentC.ms2.npz\"\n",
    "TreatmentD = \"F:/casts/databank/TreatmentD.ms2.npz\"\n",
    "\n",
    "z = np.load(file=TreatmentD)\n",
    "\n",
    "# Mat + metadata (same row count/order)\n",
    "ms2_D = z[\"ms2_matrix\"]             # (n_rows, 13690), float32\n",
    "ms2_scan = z[\"ms2_scan\"]          # (n_rows,)\n",
    "ms2_rt   = z[\"ms2_rt\"]            # (n_rows,) minutes\n",
    "ms2_fid  = z[\"ms2_file_id\"]       # (n_rows,)\n",
    "fnames   = z[\"file_names_lookup\"] # (n_files,)\n",
    "group_name = z[\"group_name\"]\n",
    "precursor_mz = z[\"ms2_precursor_mz\"]\n",
    "\n",
    "# Optional: assemble a handy DataFrame aligned to ms1 rows\n",
    "ms2_meta_D = pd.DataFrame({\n",
    "    \"scan\": ms2_scan,\n",
    "    \"rt_min\": ms2_rt,\n",
    "    \"precursor_mz\": precursor_mz,\n",
    "    \"file_name\": fnames[ms2_fid],\n",
    "    'group_name': group_name\n",
    "})\n",
    "\n",
    "metadata = pd.concat([ms2_meta_A, ms2_meta_B, ms2_meta_C, ms2_meta_D], ignore_index=True)\n",
    "ms2_lib = np.vstack((ms2_A, ms2_B, ms2_C, ms2_D))\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(\"F:/casts/databank/ms2_dataset.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"ms2_lib\", data=ms2_lib, compression=\"gzip\")\n",
    "    for col in metadata.columns:\n",
    "        f.create_dataset(col, data=metadata[col].values.astype(\"S\") if metadata[col].dtype == object else metadata[col].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2127320",
   "metadata": {},
   "source": [
    "Upload the MS2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f13d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "with h5py.File(\"F:/casts/databank/ms2_dataset.h5\", \"r\") as f:\n",
    "    ms2_lib = f[\"ms2_lib\"][:]\n",
    "    metadata = pd.DataFrame({col: f[col][:] for col in f.keys() if col != \"ms2_lib\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca29b4",
   "metadata": {},
   "source": [
    "Generate the retention time drift table and updated retrntion times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Load HDF5 (ms2_dataset.h5) -> compute per-bin RT drifts vs first run -> align RTs\n",
    "Save:\n",
    "  - per-scan aligned metadata CSV (drops 'cast spectra')\n",
    "  - per-bin drift tables CSV\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================\n",
    "# Config\n",
    "# =====================\n",
    "H5_PATH = r\"F:/casts/databank/ms2_dataset.h5\"\n",
    "\n",
    "SIM_THRESHOLD  = 0.95\n",
    "MZ_WINDOW      = 1.0\n",
    "TARGET_N       = 50\n",
    "BIN_WIDTH      = 10.0\n",
    "OVERLAP_MIN    = 2.5\n",
    "FORCE_BIN_END_MIN = 80.0\n",
    "SAMPLE_WITH_REPLACEMENT_IF_NEEDED = False\n",
    "\n",
    "PLOT_DRIFT_CURVES = False     # set True if you want plots\n",
    "PLOT_SANITY_AFTER = False\n",
    "\n",
    "# CSV outputs\n",
    "SAVE_ALIGNED_CSV  = True\n",
    "CSV_OUT_PATH      = r\"F:/casts/databank/aligned_metadata1.csv\"\n",
    "\n",
    "SAVE_DRIFTS_CSV   = True\n",
    "DRIFTS_CSV_PATH   = r\"F:/casts/databank/rt_drifts1.csv\"\n",
    "\n",
    "# =========================================================\n",
    "# Helpers\n",
    "# =========================================================\n",
    "def _to_1d_float_array(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        arr = np.asarray(x, dtype=float)\n",
    "    else:\n",
    "        try:\n",
    "            arr = np.asarray(x, dtype=float).ravel()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return arr.ravel().astype(float, copy=False)\n",
    "\n",
    "def cosine(a, b):\n",
    "    va = _to_1d_float_array(a); vb = _to_1d_float_array(b)\n",
    "    if va is None or vb is None or va.size == 0 or vb.size == 0:\n",
    "        return -np.inf\n",
    "    if va.shape != vb.shape:\n",
    "        n = min(va.size, vb.size)\n",
    "        if n == 0:\n",
    "            return -np.inf\n",
    "        va, vb = va[:n], vb[:n]\n",
    "    denom = np.linalg.norm(va) * np.linalg.norm(vb)\n",
    "    if denom == 0:\n",
    "        return -np.inf\n",
    "    return float(np.dot(va, vb) / denom)\n",
    "\n",
    "def decode_bytes_inplace(df: pd.DataFrame) -> None:\n",
    "    for col in df.columns:\n",
    "        dt = df[col].dtype\n",
    "        if dt == object or str(dt).startswith(\"|S\"):\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x\n",
    "            )\n",
    "\n",
    "def pick_col(df: pd.DataFrame, *cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of {cands} found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "def harmonize_columns(df: pd.DataFrame) -> None:\n",
    "    # sample_name\n",
    "    if \"sample_name\" not in df.columns:\n",
    "        s_col = pick_col(df, \"sample_name\", \"file_name\", \"raw_name\", \"run_name\")\n",
    "        df[\"sample_name\"] = df[s_col].astype(str)\n",
    "\n",
    "    # m/z\n",
    "    if \"m/z\" not in df.columns:\n",
    "        mz_col = pick_col(df, \"m/z\", \"mz\", \"precursor_mz\")\n",
    "        df[\"m/z\"] = df[mz_col].astype(float)\n",
    "\n",
    "    # retntion time (keep original spelling for compatibility)\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"retention_time\"].astype(float)\n",
    "        elif {\"rt_min\", \"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (df[\"rt_min\"].astype(float) + df[\"rt_max\"].astype(float)) / 2.0\n",
    "        elif \"rt_min\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt_min\"].astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = df[\"rt\"].astype(float)\n",
    "        else:\n",
    "            raise KeyError(\"Could not infer 'retntion time' column from metadata.\")\n",
    "\n",
    "def load_h5_build_df(h5_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(h5_path):\n",
    "        raise FileNotFoundError(h5_path)\n",
    "\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        if \"ms2_lib\" not in f:\n",
    "            raise KeyError(\"HDF5 must contain 'ms2_lib' dataset.\")\n",
    "        ms2_lib = f[\"ms2_lib\"][:]  # (N, L)\n",
    "        meta = {k: f[k][:] for k in f.keys() if k != \"ms2_lib\"}\n",
    "\n",
    "    metadata = pd.DataFrame(meta)\n",
    "    decode_bytes_inplace(metadata)\n",
    "    harmonize_columns(metadata)\n",
    "\n",
    "    if len(metadata) != ms2_lib.shape[0]:\n",
    "        raise ValueError(f\"Row mismatch: metadata={len(metadata)} vs ms2_lib={ms2_lib.shape[0]}\")\n",
    "\n",
    "    metadata = metadata.copy()\n",
    "    metadata[\"cast spectra\"] = pd.Series(list(ms2_lib), index=metadata.index)\n",
    "    return metadata\n",
    "\n",
    "def build_bins_for_target(df_target: pd.DataFrame,\n",
    "                          bin_width: float,\n",
    "                          force_end_min):\n",
    "    if df_target.empty:\n",
    "        return [], np.nan, np.nan\n",
    "\n",
    "    rt_min = float(df_target[\"retntion time\"].min())\n",
    "    rt_max = float(df_target[\"retntion time\"].max())\n",
    "\n",
    "    start_edge = bin_width * floor(rt_min / bin_width)\n",
    "    end_edge   = bin_width * ceil(rt_max / bin_width)\n",
    "\n",
    "    if force_end_min is not None:\n",
    "        end_edge = float(force_end_min)\n",
    "        if end_edge <= start_edge:\n",
    "            raise ValueError(f\"FORCE_BIN_END_MIN ({force_end_min}) must be > start_edge ({start_edge}).\")\n",
    "\n",
    "    bins = []\n",
    "    t = start_edge\n",
    "    while t < end_edge:\n",
    "        bins.append((t, t + bin_width))\n",
    "        t += bin_width\n",
    "    return bins, rt_min, rt_max\n",
    "\n",
    "def collect_valid_drifts(bin_df: pd.DataFrame,\n",
    "                         mz_ref: np.ndarray,\n",
    "                         rt_ref: np.ndarray,\n",
    "                         cast_ref: np.ndarray,\n",
    "                         sim_threshold: float,\n",
    "                         mz_window: float,\n",
    "                         target_n: int,\n",
    "                         sample_with_replacement: bool) -> list:\n",
    "    if bin_df.empty:\n",
    "        return []\n",
    "\n",
    "    def drift_for_row(row):\n",
    "        mz_i   = float(row[\"m/z\"])\n",
    "        rt_i   = float(row[\"retntion time\"])\n",
    "        cast_i = row[\"cast spectra\"]\n",
    "\n",
    "        mask = np.abs(mz_ref - mz_i) < mz_window\n",
    "        idxs = np.where(mask)[0]\n",
    "        if idxs.size == 0:\n",
    "            return None\n",
    "\n",
    "        match_count = 0\n",
    "        rt_sum = 0.0\n",
    "        for j in idxs:\n",
    "            if cosine(cast_i, cast_ref[j]) > sim_threshold:\n",
    "                match_count += 1\n",
    "                rt_sum += rt_ref[j]\n",
    "        if match_count == 0:\n",
    "            return None\n",
    "        return rt_i - (rt_sum / match_count)\n",
    "\n",
    "    drifts = []\n",
    "    if sample_with_replacement:\n",
    "        tries = 0\n",
    "        max_tries = max(200, target_n * 20)\n",
    "        while len(drifts) < target_n and tries < max_tries:\n",
    "            row = bin_df.sample(n=1, replace=True).iloc[0]\n",
    "            tries += 1\n",
    "            d = drift_for_row(row)\n",
    "            if d is not None:\n",
    "                drifts.append(d)\n",
    "        return drifts\n",
    "\n",
    "    bin_df_shuf = bin_df.sample(frac=1.0, replace=False, random_state=42).reset_index(drop=True)\n",
    "    for _, row in bin_df_shuf.iterrows():\n",
    "        if len(drifts) >= target_n:\n",
    "            break\n",
    "        d = drift_for_row(row)\n",
    "        if d is not None:\n",
    "            drifts.append(d)\n",
    "    return drifts\n",
    "\n",
    "def compute_drift_table_for_target(df_target: pd.DataFrame,\n",
    "                                   mz_ref: np.ndarray,\n",
    "                                   rt_ref: np.ndarray,\n",
    "                                   cast_ref: np.ndarray) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    bins, rt_min, rt_max = build_bins_for_target(df_target, BIN_WIDTH, FORCE_BIN_END_MIN)\n",
    "    records = []\n",
    "\n",
    "    for (t0, t1) in bins:\n",
    "        win_start = max(t0 - OVERLAP_MIN, rt_min)\n",
    "        win_end   = min(t1 + OVERLAP_MIN, rt_max)\n",
    "\n",
    "        bin_df = df_target[(df_target[\"retntion time\"] >= win_start) &\n",
    "                           (df_target[\"retntion time\"] <  win_end)].copy()\n",
    "\n",
    "        drifts = collect_valid_drifts(\n",
    "            bin_df,\n",
    "            mz_ref=mz_ref, rt_ref=rt_ref, cast_ref=cast_ref,\n",
    "            sim_threshold=SIM_THRESHOLD,\n",
    "            mz_window=MZ_WINDOW,\n",
    "            target_n=TARGET_N,\n",
    "            sample_with_replacement=SAMPLE_WITH_REPLACEMENT_IF_NEEDED\n",
    "        )\n",
    "        n_valid = len(drifts)\n",
    "        avg_drift = float(np.mean(drifts)) if n_valid > 0 else float(\"nan\")\n",
    "\n",
    "        records.append({\n",
    "            \"bin_start_min\": t0,\n",
    "            \"bin_end_min\": t1,\n",
    "            \"expanded_start_min\": win_start,\n",
    "            \"expanded_end_min\": win_end,\n",
    "            \"n_in_expanded_window\": len(bin_df),\n",
    "            \"n_valid_used\": n_valid,\n",
    "            \"target_n\": TARGET_N,\n",
    "            \"avg_rt_drift\": avg_drift,\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame.from_records(records)\n",
    "    if result_df.empty:\n",
    "        return result_df, result_df\n",
    "\n",
    "    result_df[\"bin_center_min\"] = 0.5 * (result_df[\"bin_start_min\"] + result_df[\"bin_end_min\"])\n",
    "    plot_df_valid = result_df[\n",
    "        (~np.isnan(result_df[\"avg_rt_drift\"])) & (result_df[\"n_valid_used\"] > 0)\n",
    "    ].copy()\n",
    "    return result_df, plot_df_valid\n",
    "\n",
    "def build_alignment_function(plot_df_valid: pd.DataFrame):\n",
    "    if plot_df_valid is None or plot_df_valid.empty:\n",
    "        return lambda x: np.zeros_like(np.asarray(x, dtype=float))\n",
    "\n",
    "    x = plot_df_valid[\"bin_center_min\"].to_numpy()\n",
    "    y = plot_df_valid[\"avg_rt_drift\"].to_numpy()\n",
    "    order = np.argsort(x)\n",
    "    x = x[order]; y = y[order]\n",
    "\n",
    "    if x.size == 1:\n",
    "        c = float(y[0])\n",
    "        return lambda rt: np.full_like(np.asarray(rt, dtype=float), c)\n",
    "\n",
    "    def f(rt):\n",
    "        rt = np.asarray(rt, dtype=float)\n",
    "        return np.interp(rt, x, y, left=y[0], right=y[-1])\n",
    "    return f\n",
    "\n",
    "def align_runs_from_h5(h5_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      aligned_df   : per-scan DataFrame with rt_correction and rt_aligned\n",
    "      drift_table  : per-bin drift table for all targets\n",
    "    \"\"\"\n",
    "    df = load_h5_build_df(h5_path)\n",
    "\n",
    "    # reference & targets\n",
    "    sample_order = df[\"sample_name\"].dropna().unique().tolist()\n",
    "    if len(sample_order) < 2:\n",
    "        raise ValueError(f\"Need ≥2 samples to align; found {len(sample_order)}: {sample_order}\")\n",
    "    ref_name = sample_order[0]\n",
    "    target_names = sample_order[1:]\n",
    "\n",
    "    df_ref = df[df[\"sample_name\"] == ref_name].copy()\n",
    "    if df_ref.empty:\n",
    "        raise ValueError(f\"No reference rows found for '{ref_name}'.\")\n",
    "    mz_ref   = df_ref[\"m/z\"].to_numpy()\n",
    "    rt_ref   = df_ref[\"retntion time\"].to_numpy()\n",
    "    cast_ref = df_ref[\"cast spectra\"].to_numpy(object)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"rt_correction\"] = 0.0\n",
    "    df[\"rt_aligned\"] = df[\"retntion time\"].astype(float)\n",
    "\n",
    "    all_drifts = []  # collect per-target drift tables\n",
    "\n",
    "    if PLOT_DRIFT_CURVES:\n",
    "        plt.figure()\n",
    "        any_series = False\n",
    "\n",
    "    for tname in target_names:\n",
    "        dft = df[df[\"sample_name\"] == tname].copy()\n",
    "        if dft.empty:\n",
    "            print(f\"Warning: no rows for target '{tname}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        res_df, plot_df_valid = compute_drift_table_for_target(dft, mz_ref, rt_ref, cast_ref)\n",
    "\n",
    "        # add target name & collect drift table\n",
    "        res_df = res_df.copy()\n",
    "        res_df[\"target_name\"] = tname\n",
    "        all_drifts.append(res_df)\n",
    "\n",
    "        # optional: weighted avg summary\n",
    "        if not plot_df_valid.empty:\n",
    "            weights = plot_df_valid[\"n_valid_used\"].to_numpy()\n",
    "            vals    = plot_df_valid[\"avg_rt_drift\"].to_numpy()\n",
    "            wavg    = np.average(vals, weights=weights)\n",
    "            print(f\"{tname}: weighted overall avg drift = {wavg:.3f} min \"\n",
    "                  f\"(kept {plot_df_valid.shape[0]} bins with ≥1 valid match; TARGET_N={TARGET_N})\")\n",
    "        else:\n",
    "            print(f\"{tname}: no bins with ≥1 valid match.\")\n",
    "\n",
    "        if PLOT_DRIFT_CURVES and not plot_df_valid.empty:\n",
    "            plt.plot(plot_df_valid[\"bin_center_min\"], plot_df_valid[\"avg_rt_drift\"], marker=\"o\", label=tname)\n",
    "\n",
    "        # build & apply alignment\n",
    "        align_fn = build_alignment_function(plot_df_valid)\n",
    "        rt_vals = dft[\"retntion time\"].to_numpy(dtype=float)\n",
    "        corr = align_fn(rt_vals)\n",
    "        aligned = rt_vals - corr\n",
    "        df.loc[dft.index, \"rt_correction\"] = corr\n",
    "        df.loc[dft.index, \"rt_aligned\"] = aligned\n",
    "\n",
    "    # reference unchanged\n",
    "    df.loc[df[\"sample_name\"] == ref_name, \"rt_correction\"] = 0.0\n",
    "    df.loc[df[\"sample_name\"] == ref_name, \"rt_aligned\"] = df.loc[df[\"sample_name\"] == ref_name, \"retntion time\"].astype(float)\n",
    "\n",
    "    if PLOT_DRIFT_CURVES:\n",
    "        plt.axhline(0.0, linestyle=\"--\", color=\"gray\")\n",
    "        plt.axhline(5.0, linestyle=\"--\", alpha=0.6)\n",
    "        plt.axhline(-5.0, linestyle=\"--\", alpha=0.6)\n",
    "        plt.xlabel(\"Retention time (min, bin center)\")\n",
    "        plt.ylabel(\"Average RT drift vs ref (min)\")\n",
    "        # plt.legend(title=\"Target samples\", fontsize=9)\n",
    "        plt.grid(True, which=\"both\", linestyle=\":\", linewidth=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # combine drift tables\n",
    "    drift_table = pd.concat(all_drifts, ignore_index=True) if all_drifts else pd.DataFrame()\n",
    "\n",
    "    if PLOT_SANITY_AFTER:\n",
    "        plt.figure()\n",
    "        for name in df[\"sample_name\"].dropna().unique().tolist():\n",
    "            dfx = df[df[\"sample_name\"] == name]\n",
    "            tmp = dfx[[\"retntion time\", \"rt_correction\"]].copy()\n",
    "            tmp[\"bin\"] = (tmp[\"retntion time\"] // 2.0) * 2.0  # 2-min bins\n",
    "            grp = tmp.groupby(\"bin\", as_index=False)[\"rt_correction\"].median()\n",
    "            plt.plot(grp[\"bin\"], grp[\"rt_correction\"], marker=\".\", alpha=0.85, label=name)\n",
    "        plt.axhline(0.0, linestyle=\"--\", color=\"gray\")\n",
    "        plt.xlabel(\"Raw RT (min, 2-min bins)\")\n",
    "        plt.ylabel(\"Median applied correction (min)\")\n",
    "        # plt.legend(fontsize=8)\n",
    "        plt.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return df, drift_table\n",
    "\n",
    "# =====================\n",
    "# Run\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    aligned_df, drift_table = align_runs_from_h5(H5_PATH)\n",
    "\n",
    "    # Save aligned per-scan metadata (drop huge spectra)\n",
    "    if SAVE_ALIGNED_CSV:\n",
    "        os.makedirs(os.path.dirname(CSV_OUT_PATH), exist_ok=True)\n",
    "        aligned_df.drop(columns=[\"cast spectra\"], errors=\"ignore\").to_csv(CSV_OUT_PATH, index=False)\n",
    "        print(f\"Saved aligned metadata to: {CSV_OUT_PATH}\")\n",
    "\n",
    "    # Save per-bin drift table\n",
    "    if SAVE_DRIFTS_CSV:\n",
    "        os.makedirs(os.path.dirname(DRIFTS_CSV_PATH), exist_ok=True)\n",
    "        drift_table.to_csv(DRIFTS_CSV_PATH, index=False)\n",
    "        print(f\"Saved per-bin RT drifts to: {DRIFTS_CSV_PATH}\")\n",
    "\n",
    "    # Quick summary\n",
    "    for name in aligned_df[\"sample_name\"].dropna().unique().tolist():\n",
    "        dfx = aligned_df[aligned_df[\"sample_name\"] == name]\n",
    "        med_corr = float(np.nanmedian(dfx[\"rt_correction\"])) if len(dfx) else np.nan\n",
    "        print(f\"{name:30s} median correction: {med_corr: .3f} min\")\n",
    "\n",
    "    print(\"\\nColumns in aligned_df:\")\n",
    "    print(\"  sample_name, m/z, retntion time, rt_correction, rt_aligned, cast spectra\")\n",
    "    if not drift_table.empty:\n",
    "        print(\"\\nDrift table columns:\")\n",
    "        print(drift_table.columns.tolist())\n",
    "\n",
    "# ---- Make runs × bins drift matrix and save to CSV ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "SAVE_DRIFT_MATRIX_CSV = True\n",
    "DRIFT_MATRIX_CSV_PATH = r\"F:/casts/databank/rt_drifts_matrix1.csv\"\n",
    "\n",
    "if not drift_table.empty and SAVE_DRIFT_MATRIX_CSV:\n",
    "    dt = drift_table.copy()\n",
    "\n",
    "    # Use bin centers as columns (minutes). Round to 2 decimals for clean headers.\n",
    "    dt[\"bin_center_min\"] = dt[\"bin_center_min\"].astype(float).round(2)\n",
    "\n",
    "    # Pivot: rows=runs (target_name), cols=bins, values=avg drift\n",
    "    drift_matrix = (\n",
    "        dt.pivot_table(\n",
    "            index=\"target_name\",\n",
    "            columns=\"bin_center_min\",\n",
    "            values=\"avg_rt_drift\",\n",
    "            aggfunc=\"mean\"  # safe if duplicates ever appear\n",
    "        )\n",
    "        .sort_index(axis=1)  # sort bins left→right\n",
    "    )\n",
    "\n",
    "    # Optional: include the reference run as a zero row if you want it in the matrix\n",
    "    try:\n",
    "        ref_name = aligned_df[\"sample_name\"].dropna().unique().tolist()[0]\n",
    "        if ref_name not in drift_matrix.index:\n",
    "            # add zero drift across all bins for the reference\n",
    "            drift_matrix.loc[ref_name] = 0.0\n",
    "            drift_matrix = drift_matrix.sort_index()\n",
    "    except Exception:\n",
    "        pass  # skip if aligned_df is not available\n",
    "\n",
    "    # (Optional) prettier column labels like \"t00-10\", else keep numeric centers:\n",
    "    # dt2 = drift_table.copy()\n",
    "    # dt2[\"bin_label\"] = dt2[\"bin_start_min\"].astype(int).astype(str) + \"-\" + dt2[\"bin_end_min\"].astype(int).astype(str)\n",
    "    # drift_matrix = (dt2.pivot_table(index=\"target_name\", columns=\"bin_label\", values=\"avg_rt_drift\").sort_index(axis=1))\n",
    "\n",
    "    os.makedirs(os.path.dirname(DRIFT_MATRIX_CSV_PATH), exist_ok=True)\n",
    "    drift_matrix.to_csv(DRIFT_MATRIX_CSV_PATH, float_format=\"%.5f\")\n",
    "    print(f\"Saved drift matrix (runs × bins) to: {DRIFT_MATRIX_CSV_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc63be8",
   "metadata": {},
   "source": [
    "Using the drift table do the actual quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this will wipe *everything* you defined in the current session!\n",
    "for var in list(globals().keys()):\n",
    "    if var[0] != \"_\":  # keep built-ins like __name__, __doc__, etc.\n",
    "        del globals()[var]\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "\n",
    "def _resolve_csv(path: str) -> str:\n",
    "    \"\"\"Return path (or path.csv) if exists; else raise.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    root, ext = os.path.splitext(path)\n",
    "    if not ext and os.path.exists(path + \".csv\"):\n",
    "        return path + \".csv\"\n",
    "    raise FileNotFoundError(f\"Drift file not found: {path}  (also tried {path+'.csv'})\")\n",
    "\n",
    "def _decode_bytes_arr(a):\n",
    "    \"\"\"Decode a 1D array of bytes/objects to str objects.\"\"\"\n",
    "    if isinstance(a, np.ndarray) and (a.dtype.kind in (\"S\", \"O\")):\n",
    "        out = []\n",
    "        for x in a:\n",
    "            if isinstance(x, (bytes, bytearray)):\n",
    "                try:\n",
    "                    out.append(x.decode(\"utf-8\"))\n",
    "                except Exception:\n",
    "                    out.append(str(x))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return np.array(out, dtype=object)\n",
    "    return a\n",
    "\n",
    "def _safe_metadata_from_npz_with_lut(z: np.lib.npyio.NpzFile, n_rows: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust metadata builder:\n",
    "      - keep 1D arrays of length n_rows\n",
    "      - broadcast 0D or length-1 arrays\n",
    "      - skip other shapes/lengths (e.g., ms2_* if mismatched)\n",
    "      - build sample_name from file_names_lookup[ms1_file_id] when available\n",
    "      - set retntion time from ms1_rt\n",
    "    \"\"\"\n",
    "    cols = {}\n",
    "    for k in z.files:\n",
    "        if k == \"ms1_matrix\":\n",
    "            continue\n",
    "        arr = z[k]\n",
    "        # Keep lookups for later mapping\n",
    "        if k in (\"file_names_lookup\", \"file_paths_lookup\"):\n",
    "            cols[k] = arr\n",
    "            continue\n",
    "\n",
    "        a = np.asarray(arr)\n",
    "        if a.ndim == 0:\n",
    "            cols[k] = np.repeat(a.item(), n_rows)\n",
    "        elif a.ndim == 1:\n",
    "            if a.shape[0] == n_rows:\n",
    "                cols[k] = a\n",
    "            elif a.shape[0] == 1:\n",
    "                cols[k] = np.repeat(a[0], n_rows)\n",
    "            else:\n",
    "                # skip mismatched lengths\n",
    "                pass\n",
    "        else:\n",
    "            # skip 2D+\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame({k: cols[k] for k in cols if k not in (\"file_names_lookup\", \"file_paths_lookup\")})\n",
    "\n",
    "    # decode bytes in df columns\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object or str(df[c].dtype).startswith(\"|S\"):\n",
    "            df[c] = pd.Series([x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x for x in df[c]])\n",
    "\n",
    "    # sample_name via file_names_lookup[ms1_file_id] when possible\n",
    "    if \"ms1_file_id\" in df.columns and \"file_names_lookup\" in cols:\n",
    "        fid = pd.Series(df[\"ms1_file_id\"]).astype(int).to_numpy()\n",
    "        names_lut = _decode_bytes_arr(cols[\"file_names_lookup\"])\n",
    "        names_lut = np.asarray(names_lut, dtype=object)\n",
    "        fallback = np.array([f\"fid_{i}\" for i in fid], dtype=object)\n",
    "        ok = (fid >= 0) & (fid < names_lut.shape[0])\n",
    "        mapped = fallback.copy()\n",
    "        mapped[ok] = names_lut[fid[ok]]\n",
    "        df[\"sample_name\"] = mapped.astype(str)\n",
    "    else:\n",
    "        if \"file_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"file_name\"].astype(str)\n",
    "        elif \"raw_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"raw_name\"].astype(str)\n",
    "        elif \"run_name\" in df.columns:\n",
    "            df[\"sample_name\"] = df[\"run_name\"].astype(str)\n",
    "        elif \"ms1_file_id\" in df.columns:\n",
    "            df[\"sample_name\"] = (\"fid_\" + pd.Series(df[\"ms1_file_id\"]).astype(int).astype(str)).astype(str)\n",
    "        else:\n",
    "            df[\"sample_name\"] = \"UnknownRun\"\n",
    "\n",
    "    if \"group_name\" not in df.columns:\n",
    "        df[\"group_name\"] = \"Unknown\"\n",
    "\n",
    "    # retention time (legacy spelling)\n",
    "    if \"retntion time\" not in df.columns:\n",
    "        if \"ms1_rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"ms1_rt\"]).astype(float)\n",
    "        elif \"retention_time\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"retention_time\"]).astype(float)\n",
    "        elif \"rt\" in df.columns:\n",
    "            df[\"retntion time\"] = pd.Series(df[\"rt\"]).astype(float)\n",
    "        elif {\"rt_min\", \"rt_max\"}.issubset(df.columns):\n",
    "            df[\"retntion time\"] = (pd.Series(df[\"rt_min\"]).astype(float) + pd.Series(df[\"rt_max\"]).astype(float)) / 2.0\n",
    "        else:\n",
    "            raise KeyError(\"Couldn't infer 'retntion time' (looked for ms1_rt, retention_time, rt, rt_min/rt_max).\")\n",
    "\n",
    "    df[\"sample_name\"] = df[\"sample_name\"].astype(str)\n",
    "    df[\"group_name\"]  = df[\"group_name\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def _build_align_functions_from_drift(drift_path: str):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      1) Wide matrix CSV: index=runs, columns=bin centers (minutes)\n",
    "      2) Long table  CSV: target_name, bin_center_min, avg_rt_drift\n",
    "    All missing values are filled with 0.\n",
    "    Returns (fns, default_fn) where default_fn is the zero-curve.\n",
    "    \"\"\"\n",
    "    p = _resolve_csv(drift_path)\n",
    "\n",
    "    # Try wide matrix first\n",
    "    try:\n",
    "        wide = pd.read_csv(p, index_col=0)\n",
    "        # convert column names to numeric bin centers\n",
    "        bin_centers = []\n",
    "        ok = True\n",
    "        for c in wide.columns:\n",
    "            try:\n",
    "                bin_centers.append(float(c))\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and len(bin_centers) > 0:\n",
    "            order = np.argsort(bin_centers)\n",
    "            cols_sorted = [wide.columns[i] for i in order]\n",
    "            wide = wide.loc[:, cols_sorted]\n",
    "            x_all = np.array([float(c) for c in cols_sorted], dtype=float)\n",
    "\n",
    "            # fill all missing with 0\n",
    "            wide = wide.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            fns = {}\n",
    "            for run, row in wide.iterrows():\n",
    "                y = row.to_numpy(dtype=float)  # NaNs already 0\n",
    "                if x_all.size == 1:\n",
    "                    c = float(y[0])\n",
    "                    fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "                else:\n",
    "                    def make_f(xv, yv):\n",
    "                        def f(rt):\n",
    "                            rt = np.asarray(rt, float)\n",
    "                            return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                        return f\n",
    "                    fns[str(run)] = make_f(x_all, y)\n",
    "\n",
    "            default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "            return fns, default_fn\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: long table\n",
    "    long = pd.read_csv(p)\n",
    "    # normalize headers\n",
    "    rename = {}\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            for c in long.columns:\n",
    "                if c.lower() == need.lower():\n",
    "                    rename[c] = need\n",
    "    if rename:\n",
    "        long = long.rename(columns=rename)\n",
    "    for need in (\"target_name\", \"bin_center_min\", \"avg_rt_drift\"):\n",
    "        if need not in long.columns:\n",
    "            raise KeyError(f\"Drift file missing column '{need}'\")\n",
    "\n",
    "    # full grid of bin centers\n",
    "    all_bins = np.sort(long[\"bin_center_min\"].astype(float).unique())\n",
    "\n",
    "    fns = {}\n",
    "    for run, grp in long.groupby(\"target_name\"):\n",
    "        # initialize y as zeros (missing -> 0)\n",
    "        y = np.zeros_like(all_bins, dtype=float)\n",
    "        x_run = grp[\"bin_center_min\"].astype(float).to_numpy()\n",
    "        y_run = grp[\"avg_rt_drift\"].astype(float).to_numpy()\n",
    "        # map provided points\n",
    "        idx_map = {bx: i for i, bx in enumerate(all_bins)}\n",
    "        for xr, yr in zip(x_run, y_run):\n",
    "            i = idx_map.get(xr, None)\n",
    "            if i is not None and np.isfinite(yr):\n",
    "                y[i] = yr  # others remain 0\n",
    "\n",
    "        if all_bins.size == 1:\n",
    "            c = float(y[0])\n",
    "            fns[str(run)] = (lambda c: (lambda rt: np.full_like(np.asarray(rt, float), c)))(c)\n",
    "        else:\n",
    "            def make_f(xv, yv):\n",
    "                def f(rt):\n",
    "                    rt = np.asarray(rt, float)\n",
    "                    return np.interp(rt, xv, yv, left=yv[0], right=yv[-1])\n",
    "                return f\n",
    "            fns[str(run)] = make_f(all_bins, y)\n",
    "\n",
    "    default_fn = lambda rt: np.zeros_like(np.asarray(rt, float))\n",
    "    return fns, default_fn\n",
    "\n",
    "def _sum_rows_chunked(M, idxs, chunk_rows=1024, out_dtype=np.float32):\n",
    "    \"\"\"Memory-safe sum over selected rows.\"\"\"\n",
    "    if idxs.size == 0:\n",
    "        return np.zeros(M.shape[1], dtype=out_dtype)\n",
    "    acc = np.zeros(M.shape[1], dtype=np.float64)\n",
    "    for s in range(0, idxs.size, chunk_rows):\n",
    "        block = M[idxs[s:s+chunk_rows]]\n",
    "        acc += block.sum(axis=0, dtype=np.float64)\n",
    "    return acc.astype(out_dtype, copy=False)\n",
    "\n",
    "# ------------------ main (per-sample) ------------------\n",
    "\n",
    "def bin_ms1_npz_with_alignment_per_sample(\n",
    "    npz_path: str,\n",
    "    drift_path: str,\n",
    "    out_csv_path: str,\n",
    "    bin_width: float = 10.0,\n",
    "    overlap: float = 2.5,\n",
    "    num_bins: int = 8,\n",
    "    chunk_rows: int = 1024\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Align per-scan RT using per-run drift curves, then for **each sample_name**\n",
    "    sum MS1 spectra into 8 bins (10 min) with ±2.5 min overlap on aligned RT.\n",
    "    Missing drift values -> 0. Writes ONE CSV with 8 rows per sample.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(npz_path)\n",
    "\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    if \"ms1_matrix\" not in z:\n",
    "        raise KeyError(\"NPZ must contain 'ms1_matrix'\")\n",
    "    MS1 = z[\"ms1_matrix\"]       # shape: (N, L)\n",
    "    N, L = MS1.shape\n",
    "\n",
    "    # metadata with sample_name, group_name, retntion time\n",
    "    metadata = _safe_metadata_from_npz_with_lut(z, n_rows=N)\n",
    "\n",
    "    # build align functions; default is zero-curve\n",
    "    align_fns, default_fn = _build_align_functions_from_drift(drift_path)\n",
    "\n",
    "    # per-scan aligned RT\n",
    "    rt_raw = metadata[\"retntion time\"].to_numpy(dtype=float)\n",
    "    runs   = metadata[\"sample_name\"].astype(str).to_numpy()\n",
    "    groups = metadata[\"group_name\"].astype(str).to_numpy()\n",
    "\n",
    "    rt_corr = np.zeros_like(rt_raw, dtype=float)\n",
    "    for run in np.unique(runs):\n",
    "        f = align_fns.get(run, default_fn)  # if run missing -> zero drift\n",
    "        m = (runs == run)\n",
    "        if np.any(m):\n",
    "            rt_corr[m] = f(rt_raw[m])\n",
    "    rt_aligned = rt_raw - rt_corr\n",
    "\n",
    "    # fixed bins: [0, 80) stepped by 10, with ±2.5 overlap on aligned RT\n",
    "    starts  = np.arange(0.0, num_bins * bin_width, bin_width, dtype=float)\n",
    "    ends    = starts + bin_width\n",
    "    centers = 0.5 * (starts + ends)\n",
    "\n",
    "    # Precompute for clipping\n",
    "    rt_min = float(np.nanmin(rt_aligned)) if rt_aligned.size else 0.0\n",
    "    rt_max = float(np.nanmax(rt_aligned)) if rt_aligned.size else 0.0\n",
    "\n",
    "    cast_cols = [f\"cast_{i:05d}\" for i in range(L)]\n",
    "    rows = []\n",
    "\n",
    "    # ---- PER-SAMPLE LOOP ----\n",
    "    unique_runs = np.unique(runs)\n",
    "    for run in unique_runs:\n",
    "        idx_run = np.flatnonzero(runs == run)\n",
    "        if idx_run.size == 0:\n",
    "            continue\n",
    "\n",
    "        # group label for this run (assume constant within run)\n",
    "        grp_vals = np.unique(groups[idx_run])\n",
    "        group_label = grp_vals[0] if grp_vals.size > 0 else \"Unknown\"\n",
    "\n",
    "        rt_run = rt_aligned[idx_run]\n",
    "\n",
    "        for t0, t1, mid in zip(starts, ends, centers):\n",
    "            win_start = max(t0 - overlap, rt_min)\n",
    "            win_end   = min(t1 + overlap, rt_max)\n",
    "\n",
    "            # indices of this run that fall in the window\n",
    "            mask_local = (rt_run >= win_start) & (rt_run < win_end)\n",
    "            idxs = idx_run[mask_local]\n",
    "            n_scans = int(idxs.size)\n",
    "\n",
    "            if n_scans > 0:\n",
    "                vec = _sum_rows_chunked(MS1, idxs, chunk_rows=chunk_rows, out_dtype=np.float32)\n",
    "                rt_obs_min = float(rt_run[mask_local].min())\n",
    "                rt_obs_max = float(rt_run[mask_local].max())\n",
    "            else:\n",
    "                vec = np.zeros(L, dtype=np.float32)\n",
    "                rt_obs_min, rt_obs_max = np.nan, np.nan\n",
    "\n",
    "            # include sample_name so output is per-sample\n",
    "            rows.append([\n",
    "                run, group_label,\n",
    "                t0, t1, win_start, win_end, mid,\n",
    "                n_scans, rt_obs_min, rt_obs_max\n",
    "            ] + vec.tolist())\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"sample_name\", \"group_name\",\n",
    "            \"rt_start_min\",\"rt_end_min\",\n",
    "            \"expanded_start_min\",\"expanded_end_min\",\n",
    "            \"rt_center_min\",\"n_scans\",\n",
    "            \"rt_aligned_min_obs\",\"rt_aligned_max_obs\"\n",
    "        ] + cast_cols\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True)\n",
    "    out_df.to_csv(out_csv_path, index=False)\n",
    "    return out_csv_path\n",
    "\n",
    "# ------------------ run ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    npz_path   = r\"F:\\casts\\databank\\TreatmentD.ms1.npz\"\n",
    "    drift_path = r\"F:\\casts\\databank\\rt_drifts_matrix\"  # auto-tries .csv\n",
    "    out_csv    = r\"F:\\casts\\databank\\TreatmentD_aligned_bins_per_sample.csv\"\n",
    "\n",
    "    wrote = bin_ms1_npz_with_alignment_per_sample(\n",
    "        npz_path=npz_path,\n",
    "        drift_path=drift_path,\n",
    "        out_csv_path=out_csv,\n",
    "        bin_width=10.0,\n",
    "        overlap=2.5,\n",
    "        num_bins=8,\n",
    "        chunk_rows=1024  # lower if you still see MemoryError\n",
    "    )\n",
    "    print(\"Saved:\", wrote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2ea5a",
   "metadata": {},
   "source": [
    "Imporing tdportal report to databank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7777a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_import(tdportal, databank, cast_path):\n",
    "  def str_to_int(st):\n",
    "      internal = []\n",
    "      digits = re.findall(r'\\d+', st)\n",
    "      for i in range(0, len(digits)):\n",
    "          internal.append(int(digits[i]))\n",
    "      return(internal)\n",
    "\n",
    "  scan_number = [0]*len(tdportal['File Name'])\n",
    "  td_samples = []\n",
    "\n",
    "  for i in range(0, len(tdportal['File Name'])):\n",
    "      scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
    "      if tdportal['File Name'][i] not in td_samples:\n",
    "        td_samples.append(tdportal['File Name'][i])\n",
    "\n",
    "  my_dic_scan = {key: [] for key in td_samples}\n",
    "  my_dic_index = {key: [] for key in td_samples}\n",
    "\n",
    "  for i in range(0, len(tdportal['File Name'])):\n",
    "      my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
    "      my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
    "\n",
    "  for i in range(0, len(td_samples)):\n",
    "      nested_list = my_dic_scan[td_samples[i]]\n",
    "      flat_list = []\n",
    "      for item in nested_list:\n",
    "          if isinstance(item, list):\n",
    "              flat_list.extend(item)\n",
    "          else:\n",
    "              flat_list.append(item)\n",
    "      my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "\n",
    "  for i in range(0, len(td_samples)):\n",
    "      nested_list = my_dic_index[td_samples[i]]\n",
    "      flat_list = []\n",
    "      for item in nested_list:\n",
    "          if isinstance(item, list):\n",
    "              flat_list.extend(item)\n",
    "          else:\n",
    "              flat_list.append(item)\n",
    "      my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "  sequence, MASS, Accession, missing, PFR = [], [], [], [], []\n",
    "\n",
    "  for i in tqdm(range(len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
    "      try:\n",
    "          sample = databank['sample_name'][i]\n",
    "          scan   = databank['scan'][i]\n",
    "\n",
    "          if scan in my_dic_scan[sample]:\n",
    "              tt = my_dic_index[sample][my_dic_scan[sample].index(scan)]\n",
    "              sequence.append(tdportal.at[tt, 'Sequence'])\n",
    "              MASS.append(tdportal.at[tt, 'Average Mass'])\n",
    "              Accession.append(tdportal.at[tt, 'Accession'])\n",
    "              PFR.append(tdportal.at[tt, 'PFR'])\n",
    "          else:\n",
    "              sequence.append(None)\n",
    "              MASS.append(None)\n",
    "              Accession.append(None)\n",
    "              PFR.append(None)\n",
    "\n",
    "      except KeyError as e:\n",
    "          missing.append(sample)\n",
    "        # Handles missing sample_name or missing index key\n",
    "        # You could also log: print(f\"Missing key: {e}\")\n",
    "          sequence.append(None)\n",
    "          MASS.append(None)\n",
    "          Accession.append(None)\n",
    "          PFR.append(None)\n",
    "\n",
    "      except Exception as e:\n",
    "        # Catches other unexpected issues (out-of-range, missing column, etc.)\n",
    "        # print(f\"Unexpected error: {e}\")\n",
    "          sequence.append(None)\n",
    "          MASS.append(None)\n",
    "          Accession.append(None)\n",
    "          PFR.append(None)\n",
    "\n",
    "  print(set(missing))\n",
    "\n",
    "  databank['sequence'] = sequence\n",
    "  databank['MASS'] = MASS\n",
    "  databank['Accession'] = Accession\n",
    "  databank['PFR'] = PFR\n",
    "\n",
    "  databank = pd.DataFrame(databank)\n",
    "\n",
    "  databank.to_csv(cast_path, index=False)\n",
    "\n",
    "  return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54cf0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scans: 100%|█████████████████████████████████| 1219397/1219397 [08:10<00:00, 2486.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "tdportal = pd.read_csv(r'F:\\casts\\databank\\csv_files\\tdportal.csv')\n",
    "df = pd.read_csv(r'F:\\casts\\databank\\csv_files\\aligned_metadata.csv')\n",
    "cast_path = 'F:/casts/databank/csv_files/databank_pfr.csv'\n",
    "\n",
    "ID_import(tdportal, df, cast_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612f8ce",
   "metadata": {},
   "source": [
    "This code removes the rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d03545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved to: F:/casts/databank/csv_files/databank_pfr_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and output paths\n",
    "input_path = r'F:/casts/databank/csv_files/databank_pfr.csv'\n",
    "output_path = r'F:/casts/databank/csv_files/databank_pfr_clean.csv'\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Remove rows where PFR is missing (NaN or empty string)\n",
    "df_clean = df.dropna(subset=[\"PFR\"])   # removes NaN\n",
    "df_clean = df_clean[df_clean[\"PFR\"].astype(str).str.strip() != \"\"]  # removes empty strings\n",
    "\n",
    "# Save cleaned CSV\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned file saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
