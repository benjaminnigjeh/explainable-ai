{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient (keep model as-is, multi-class)\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    \"\"\"\n",
    "    Gradient wrt inputs of log(sum_{i in pos_ids} p_i) - log(sum_{j in neg_ids} p_j).\n",
    "    x1: (1, D)\n",
    "    \"\"\"\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(\n",
    "    X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Average input gradient across samples and models.\n",
    "    Returns (D,) np.ndarray.\n",
    "    \"\"\"\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model (unchanged)\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Cosine & plotting helpers\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y drawn above baseline; bottom_y mirrored below (negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Config (adjust path as needed)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/casts/dataset_rt.csv\"  # <- adjust\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 10\n",
    "\n",
    "# two independent runs\n",
    "SEED_BASES  = [111, 777]\n",
    "\n",
    "# Output root (we'll nest per bin)\n",
    "OUT_ROOT = \"./group_compare_all_bins\"\n",
    "\n",
    "# ----------------------------\n",
    "# Training (multi-class) for one run; return list of models\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# Grouping input (ask user) + parsers\n",
    "# ----------------------------\n",
    "def _parse_groupings_from_string(s: str):\n",
    "    \"\"\"\n",
    "    Accepts a few forgiving formats. Examples:\n",
    "      \"((1),(0));((2),(0));((3),(0))\"\n",
    "      \"1|0;2|0;3|0\"\n",
    "      \"[ (1,2)|(0,3), (3)|(0,1,2) ]\"\n",
    "      \"[((1,), (0,)), ((2,), (0,))]\"\n",
    "    Returns: list of tuples: [ (pos_tuple, neg_tuple), ... ]\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Try Python-literal first (e.g., \"[((1,), (0,)), ...]\")\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        out = []\n",
    "        for pair in obj:\n",
    "            pos, neg = pair\n",
    "            pos_t = tuple(int(x) for x in (pos if isinstance(pos, (list, tuple)) else (pos,)))\n",
    "            neg_t = tuple(int(x) for x in (neg if isinstance(neg, (list, tuple)) else (neg,)))\n",
    "            out.append((pos_t, neg_t))\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try \"a|b; c|d\" style\n",
    "    if '|' in s:\n",
    "        pairs = re.split(r'\\s*;\\s*', s)\n",
    "        out = []\n",
    "        for p in pairs:\n",
    "            if not p:\n",
    "                continue\n",
    "            if '|' not in p:\n",
    "                continue\n",
    "            left, right = p.split('|', 1)\n",
    "            L = tuple(int(x) for x in re.findall(r'-?\\d+', left))\n",
    "            R = tuple(int(x) for x in re.findall(r'-?\\d+', right))\n",
    "            if len(L) and len(R):\n",
    "                out.append((tuple(L), tuple(R)))\n",
    "        if out:\n",
    "            return out\n",
    "\n",
    "    # Try \"((...),(...));((...),(...))\" by extracting integers\n",
    "    chunks = re.findall(r'\\(([^()]*)\\)\\s*,\\s*\\(([^()]*)\\)', s)\n",
    "    out = []\n",
    "    for lft, rgt in chunks:\n",
    "        L = tuple(int(x) for x in re.findall(r'-?\\d+', lft))\n",
    "        R = tuple(int(x) for x in re.findall(r'-?\\d+', rgt))\n",
    "        if len(L) and len(R):\n",
    "            out.append((tuple(L), tuple(R)))\n",
    "    return out\n",
    "\n",
    "def _ask_for_groupings(unique_labels):\n",
    "    \"\"\"\n",
    "    Ask user for groupings via input(), with robust fallback.\n",
    "    ENV override: if CLASS_GROUPINGS is set, parse from it and skip prompt.\n",
    "    Special keyword: 'AUTO' -> generate all non-empty bipartitions (pos vs neg).\n",
    "    \"\"\"\n",
    "    env_s = os.environ.get(\"CLASS_GROUPINGS\", \"\").strip()\n",
    "    if env_s:\n",
    "        groups = _parse_groupings_from_string(env_s)\n",
    "        if not groups and env_s.upper() == \"AUTO\":\n",
    "            return auto_groupings(unique_labels)\n",
    "        if groups:\n",
    "            return groups\n",
    "        print(\"[WARN] Failed to parse CLASS_GROUPINGS from environment; falling back to prompt.\", file=sys.stderr)\n",
    "\n",
    "    prompt = (\n",
    "        \"\\nEnter class groupings as pairs of POS vs NEG (examples):\\n\"\n",
    "        \"  1) Python-literal:  [((1,), (0,)), ((2,), (0,)), ((3,), (0,))]\\n\"\n",
    "        \"  2) Pipe/semicolon:  1|0; 2|0; 3|0\\n\"\n",
    "        \"  3) Tuple pairs:     ((2,3),(0,1)); ((1,2),(0,3))\\n\"\n",
    "        \"Type 'AUTO' for all non-empty bipartitions of observed labels.\\n\"\n",
    "        \"Press Enter for default: ((1),(0)); ((2),(0)); ((3),(0)); ((2,3),(0,1))\\n\"\n",
    "        \"Your groupings: \"\n",
    "    )\n",
    "    try:\n",
    "        s = input(prompt)\n",
    "    except EOFError:\n",
    "        s = \"\"  # non-interactive: default\n",
    "\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        # default sensible set\n",
    "        return [((1,), (0,)), ((2,), (0,)), ((3,), (0,)), ((2,3), (0,1))]\n",
    "    if s.upper() == \"AUTO\":\n",
    "        return auto_groupings(unique_labels)\n",
    "\n",
    "    parsed = _parse_groupings_from_string(s)\n",
    "    if parsed:\n",
    "        return parsed\n",
    "\n",
    "    print(\"[WARN] Could not parse input; using default groupings.\", file=sys.stderr)\n",
    "    return [((1,), (0,)), ((2,), (0,)), ((3,), (0,)), ((2,3), (0,1))]\n",
    "\n",
    "from itertools import combinations, chain\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def auto_groupings(labels):\n",
    "    \"\"\"\n",
    "    Produce all unique non-empty bipartitions (pos vs neg) of observed labels.\n",
    "    We enforce min(pos) < min(neg) to avoid duplicates (pos/neg swapped).\n",
    "    \"\"\"\n",
    "    labels = tuple(sorted(labels))\n",
    "    out = []\n",
    "    for pos in powerset(labels):\n",
    "        if not pos:        # skip empty\n",
    "            continue\n",
    "        neg = tuple(sorted(set(labels) - set(pos)))\n",
    "        if not neg:        # skip empty\n",
    "            continue\n",
    "        if min(pos) < min(neg):\n",
    "            out.append((tuple(sorted(pos)), tuple(sorted(neg))))\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Main (all bins): ask for groupings, loop per bin\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load once\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    # Discover bins\n",
    "    all_bins = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    if len(all_bins) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only 0..3 by default (adjust if needed)\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "\n",
    "    # Ask for groupings (based on actually present labels)\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    # sanity: labels should be contiguous 0..C-1\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "\n",
    "    groupings = _ask_for_groupings(unique_labels)\n",
    "    print(f\"\\nUsing {len(groupings)} grouping(s): {groupings}\")\n",
    "\n",
    "    # Prepare output root\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    # Process every bin\n",
    "    for BIN_VALUE in all_bins:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "        fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "        if fdf.empty:\n",
    "            print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Normalize all features except ['bin','target'] by (max+1) *within this bin*\n",
    "        cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "        fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "        Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "        X = np.nan_to_num(fdf.drop(columns=['bin', 'target']).to_numpy(), copy=False)\n",
    "\n",
    "        if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "            print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "              f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "        # Output dirs per bin\n",
    "        OUT_DIR  = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "        PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "        os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "        # Train models ONCE per run (A, B), then reuse across all groupings\n",
    "        models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "        models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "        # common x-grid\n",
    "        # We'll size per gradient later, but define a helper\n",
    "        def save_and_plot_for_grouping(pos_ids, neg_ids):\n",
    "            # compute gradients for both runs (same X, different ensembles)\n",
    "            grad_A = compute_avg_group_logodds_gradient(X, models_A,\n",
    "                                                        pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "            grad_B = compute_avg_group_logodds_gradient(X, models_B,\n",
    "                                                        pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "\n",
    "            # tagging for filenames\n",
    "            pos_tag = \"_\".join(map(str, pos_ids))\n",
    "            neg_tag = \"_\".join(map(str, neg_ids))\n",
    "            tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "            # Save raw gradients\n",
    "            np.save(os.path.join(OUT_DIR, f\"grad_runA__{tag}.npy\"), grad_A)\n",
    "            np.save(os.path.join(OUT_DIR, f\"grad_runB__{tag}.npy\"), grad_B)\n",
    "\n",
    "            # CSV with truncated grid for convenience\n",
    "            n_grid = min(10000, min(grad_A.size, grad_B.size))\n",
    "            x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "            csv_path = os.path.join(OUT_DIR, f\"grads_AB__{tag}.csv\")\n",
    "            pd.DataFrame({\"m/z\": x_grid,\n",
    "                          \"grad_runA\": grad_A[:n_grid],\n",
    "                          \"grad_runB\": grad_B[:n_grid]}).to_csv(csv_path, index=False)\n",
    "\n",
    "            # Split into pos / neg(abs)\n",
    "            yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "            yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "            yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "            yA_neg = np.where(yA < 0, -yA, 0.0)  # absolute value\n",
    "            yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "            # Cosine similarities\n",
    "            cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "            cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "            # Plots\n",
    "            pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                         f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "            neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                         f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "            mirror_plot(\n",
    "                x_grid, yA_pos, yB_pos,\n",
    "                title=pos_title,\n",
    "                outfile=os.path.join(PLOT_DIR, f\"{tag}__mirror_pos.png\")\n",
    "            )\n",
    "            mirror_plot(\n",
    "                x_grid, yA_neg, yB_neg,\n",
    "                title=neg_title,\n",
    "                outfile=os.path.join(PLOT_DIR, f\"{tag}__mirror_negabs.png\")\n",
    "            )\n",
    "\n",
    "            # JSON summary\n",
    "            summary = {\n",
    "                \"bin\": BIN_VALUE,\n",
    "                \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                \"comparison\": f\"log(sum p[{pos_ids}]) - log(sum p[{neg_ids}])\",\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"k_splits\": K_SPLITS,\n",
    "                \"n_repeats\": N_REPEATS,\n",
    "                \"seed_bases\": SEED_BASES,\n",
    "                \"cosine_pos\": cos_pos,\n",
    "                \"cosine_neg_abs\": cos_neg,\n",
    "                \"paths\": {\n",
    "                    \"grads_csv\": csv_path,\n",
    "                    \"grad_runA_npy\": os.path.join(OUT_DIR, f\"grad_runA__{tag}.npy\"),\n",
    "                    \"grad_runB_npy\": os.path.join(OUT_DIR, f\"grad_runB__{tag}.npy\"),\n",
    "                    \"mirror_pos_png\": os.path.join(PLOT_DIR, f\"{tag}__mirror_pos.png\"),\n",
    "                    \"mirror_negabs_png\": os.path.join(PLOT_DIR, f\"{tag}__mirror_negabs.png\"),\n",
    "                }\n",
    "            }\n",
    "            with open(os.path.join(OUT_DIR, f\"summary__{tag}.json\"), \"w\") as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "\n",
    "            print(f\"  Saved {tag}  |  Cos(pos)={cos_pos:.6f}  Cos(neg|abs|)={cos_neg:.6f}\")\n",
    "\n",
    "        # run all groupings\n",
    "        for (pos_ids, neg_ids) in groupings:\n",
    "            # sanity: ensure ids exist\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for idx in (*pos_ids, *neg_ids):\n",
    "                assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "            save_and_plot_for_grouping(pos_ids, neg_ids)\n",
    "\n",
    "        # cleanup models for this bin\n",
    "        try:\n",
    "            for m in models_A: del m\n",
    "            for m in models_B: del m\n",
    "        except Exception:\n",
    "            pass\n",
    "        tf.keras.backend.clear_session(); gc.collect()\n",
    "\n",
    "    print(\"\\nAll bins processed.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494ba996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 8 grouping(s): [((2,), (1,)), ((2,), (0,)), ((1,), (0,)), ((3,), (2,)), ((3,), (0, 1, 2)), ((2, 1), (0,)), ((3,), (1,)), ((3,), (0,))]\n",
      "\n",
      "================= BIN 35 =================\n",
      "Bin 35: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _group_logodds_grad_for_model at 0x000001A1456F0360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _group_logodds_grad_for_model at 0x000001A1456F0360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Saved pos_2__neg_1  |  Cos(pos)=0.902391  Cos(neg|abs|)=0.935754\n",
      "  Saved pos_2__neg_0  |  Cos(pos)=0.974754  Cos(neg|abs|)=0.862151\n",
      "  Saved pos_1__neg_0  |  Cos(pos)=0.967700  Cos(neg|abs|)=0.911179\n",
      "  Saved pos_3__neg_2  |  Cos(pos)=0.917789  Cos(neg|abs|)=0.988955\n",
      "  Saved pos_3__neg_0_1_2  |  Cos(pos)=0.924846  Cos(neg|abs|)=0.988356\n",
      "  Saved pos_2_1__neg_0  |  Cos(pos)=0.976204  Cos(neg|abs|)=0.907891\n",
      "  Saved pos_3__neg_1  |  Cos(pos)=0.860500  Cos(neg|abs|)=0.987387\n",
      "  Saved pos_3__neg_0  |  Cos(pos)=0.944084  Cos(neg|abs|)=0.980941\n",
      "WARNING:tensorflow:From c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "================= BIN 45 =================\n",
      "Bin 45: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "  Saved pos_2__neg_1  |  Cos(pos)=0.819245  Cos(neg|abs|)=0.762299\n",
      "  Saved pos_2__neg_0  |  Cos(pos)=0.980210  Cos(neg|abs|)=0.834645\n",
      "  Saved pos_1__neg_0  |  Cos(pos)=0.973553  Cos(neg|abs|)=0.847506\n",
      "  Saved pos_3__neg_2  |  Cos(pos)=0.713385  Cos(neg|abs|)=0.986402\n",
      "  Saved pos_3__neg_0_1_2  |  Cos(pos)=0.833365  Cos(neg|abs|)=0.984717\n",
      "  Saved pos_2_1__neg_0  |  Cos(pos)=0.980719  Cos(neg|abs|)=0.862161\n",
      "  Saved pos_3__neg_1  |  Cos(pos)=0.453375  Cos(neg|abs|)=0.982993\n",
      "  Saved pos_3__neg_0  |  Cos(pos)=0.779096  Cos(neg|abs|)=0.948932\n",
      "\n",
      "================= BIN 55 =================\n",
      "Bin 55: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "  Saved pos_2__neg_1  |  Cos(pos)=0.593254  Cos(neg|abs|)=0.846396\n",
      "  Saved pos_2__neg_0  |  Cos(pos)=0.950467  Cos(neg|abs|)=0.820848\n",
      "  Saved pos_1__neg_0  |  Cos(pos)=0.965248  Cos(neg|abs|)=0.789730\n",
      "  Saved pos_3__neg_2  |  Cos(pos)=0.823259  Cos(neg|abs|)=0.970186\n",
      "  Saved pos_3__neg_0_1_2  |  Cos(pos)=0.828864  Cos(neg|abs|)=0.973406\n",
      "  Saved pos_2_1__neg_0  |  Cos(pos)=0.966963  Cos(neg|abs|)=0.815025\n",
      "  Saved pos_3__neg_1  |  Cos(pos)=0.843434  Cos(neg|abs|)=0.977491\n",
      "  Saved pos_3__neg_0  |  Cos(pos)=0.554633  Cos(neg|abs|)=0.879213\n",
      "\n",
      "================= BIN 65 =================\n",
      "Bin 65: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "  Saved pos_2__neg_1  |  Cos(pos)=0.763546  Cos(neg|abs|)=0.901769\n",
      "  Saved pos_2__neg_0  |  Cos(pos)=0.915023  Cos(neg|abs|)=0.744971\n",
      "  Saved pos_1__neg_0  |  Cos(pos)=0.951888  Cos(neg|abs|)=0.451910\n",
      "  Saved pos_3__neg_2  |  Cos(pos)=0.710849  Cos(neg|abs|)=0.964244\n",
      "  Saved pos_3__neg_0_1_2  |  Cos(pos)=0.722832  Cos(neg|abs|)=0.972398\n",
      "  Saved pos_2_1__neg_0  |  Cos(pos)=0.947531  Cos(neg|abs|)=0.613226\n",
      "  Saved pos_3__neg_1  |  Cos(pos)=0.706953  Cos(neg|abs|)=0.976597\n",
      "  Saved pos_3__neg_0  |  Cos(pos)=0.566470  Cos(neg|abs|)=0.894270\n",
      "\n",
      "================= BIN 75 =================\n",
      "Bin 75: samples=118, dim=13690  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 111] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 111] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 111] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 111] Fold 5/5 trained 10 models (total: 50)\n",
      "[Seed base 777] Fold 1/5 trained 10 models (total: 10)\n",
      "[Seed base 777] Fold 2/5 trained 10 models (total: 20)\n",
      "[Seed base 777] Fold 3/5 trained 10 models (total: 30)\n",
      "[Seed base 777] Fold 4/5 trained 10 models (total: 40)\n",
      "[Seed base 777] Fold 5/5 trained 10 models (total: 50)\n",
      "  Saved pos_2__neg_1  |  Cos(pos)=0.941026  Cos(neg|abs|)=0.991608\n",
      "  Saved pos_2__neg_0  |  Cos(pos)=0.981443  Cos(neg|abs|)=0.996444\n",
      "  Saved pos_1__neg_0  |  Cos(pos)=0.966449  Cos(neg|abs|)=0.994603\n",
      "  Saved pos_3__neg_2  |  Cos(pos)=0.993909  Cos(neg|abs|)=0.988312\n",
      "  Saved pos_3__neg_0_1_2  |  Cos(pos)=0.972554  Cos(neg|abs|)=0.986739\n",
      "  Saved pos_2_1__neg_0  |  Cos(pos)=0.980243  Cos(neg|abs|)=0.995867\n",
      "  Saved pos_3__neg_1  |  Cos(pos)=0.986697  Cos(neg|abs|)=0.984128\n",
      "  Saved pos_3__neg_0  |  Cos(pos)=0.649253  Cos(neg|abs|)=0.942581\n",
      "\n",
      "All bins processed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for _gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def hard_free():\n",
    "    \"\"\"\n",
    "    Aggressively release memory after each bin.\n",
    "    - Close matplotlib figures\n",
    "    - Clear TF session/graphs\n",
    "    - Run Python GC\n",
    "    - On Linux, trim libc heap pages back to the OS\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.close('all')\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        gc.collect(); gc.collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Linux-only heap trim\n",
    "    try:\n",
    "        import ctypes, platform\n",
    "        if platform.system().lower() == \"linux\":\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def helper_regex(text):\n",
    "    m = re.search(rf\"{'Full'}\\s+(\\w+)\", str(text))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient (keep model as-is, multi-class)\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    \"\"\"\n",
    "    Gradient wrt inputs of log(sum_{i in pos_ids} p_i) - log(sum_{j in neg_ids} p_j).\n",
    "    x1: (1, D)\n",
    "    \"\"\"\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(\n",
    "    X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Average input gradient across samples and models.\n",
    "    Returns (D,) np.ndarray.\n",
    "    \"\"\"\n",
    "    X_t = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N = int(X_t.shape[0])\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X_t[i:i+1]  # (1, D)\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)  # (D,)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)  # (D,)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model (unchanged)\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Cosine & plotting helpers\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    \"\"\"\n",
    "    Mirror plot: top_y drawn above baseline; bottom_y mirrored below (negative).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def single_run_plot(x, y, title, outfile, label=\"Gradient\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, y, linewidth=1.0, label=label)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Config (adjust path as needed)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/casts/dataset_rt.csv\"  # <- adjust\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 10\n",
    "\n",
    "# two independent runs\n",
    "SEED_BASES  = [111, 777]\n",
    "\n",
    "# Output root (we'll nest per bin)\n",
    "OUT_ROOT = \"./group_compare_all_bins_8_groups\"\n",
    "\n",
    "# ----------------------------\n",
    "# Training (multi-class) for one run; return list of models\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr,\n",
    "                  epochs=EPOCHS,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# Grouping input (ask user) + parsers\n",
    "# ----------------------------\n",
    "def _parse_groupings_from_string(s: str):\n",
    "    \"\"\"\n",
    "    Accepts a few forgiving formats. Examples:\n",
    "      \"((1),(0));((2),(0));((3),(0))\"\n",
    "      \"1|0;2|0;3|0\"\n",
    "      \"[ (1,2)|(0,3), (3)|(0,1,2) ]\"\n",
    "      \"[((1,), (0,)), ((2,), (0,))]\"\n",
    "    Returns: list of tuples: [ (pos_tuple, neg_tuple), ... ]\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Try Python-literal first (e.g., \"[((1,), (0,)), ...]\")\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        out = []\n",
    "        for pair in obj:\n",
    "            pos, neg = pair\n",
    "            pos_t = tuple(int(x) for x in (pos if isinstance(pos, (list, tuple)) else (pos,)))\n",
    "            neg_t = tuple(int(x) for x in (neg if isinstance(neg, (list, tuple)) else (neg,)))\n",
    "            out.append((pos_t, neg_t))\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try \"a|b; c|d\" style\n",
    "    if '|' in s:\n",
    "        pairs = re.split(r'\\s*;\\s*', s)\n",
    "        out = []\n",
    "        for p in pairs:\n",
    "            if not p:\n",
    "                continue\n",
    "            if '|' not in p:\n",
    "                continue\n",
    "            left, right = p.split('|', 1)\n",
    "            L = tuple(int(x) for x in re.findall(r'-?\\d+', left))\n",
    "            R = tuple(int(x) for x in re.findall(r'-?\\d+', right))\n",
    "            if len(L) and len(R):\n",
    "                out.append((tuple(L), tuple(R)))\n",
    "        if out:\n",
    "            return out\n",
    "\n",
    "    # Try \"((...),(...));((...),(...))\" by extracting integers\n",
    "    chunks = re.findall(r'\\(([^()]*)\\)\\s*,\\s*\\(([^()]*)\\)', s)\n",
    "    out = []\n",
    "    for lft, rgt in chunks:\n",
    "        L = tuple(int(x) for x in re.findall(r'-?\\d+', lft))\n",
    "        R = tuple(int(x) for x in re.findall(r'-?\\d+', rgt))\n",
    "        if len(L) and len(R):\n",
    "            out.append((tuple(L), tuple(R)))\n",
    "    return out\n",
    "\n",
    "def powerset(iterable):\n",
    "    from itertools import combinations, chain\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def auto_groupings(labels):\n",
    "    \"\"\"\n",
    "    Produce all unique non-empty bipartitions (pos vs neg) of observed labels.\n",
    "    We enforce min(pos) < min(neg) to avoid duplicates (pos/neg swapped).\n",
    "    \"\"\"\n",
    "    labels = tuple(sorted(labels))\n",
    "    out = []\n",
    "    for pos in powerset(labels):\n",
    "        if not pos:        # skip empty\n",
    "            continue\n",
    "        neg = tuple(sorted(set(labels) - set(pos)))\n",
    "        if not neg:        # skip empty\n",
    "            continue\n",
    "        if min(pos) < min(neg):\n",
    "            out.append((tuple(sorted(pos)), tuple(sorted(neg))))\n",
    "    return out\n",
    "\n",
    "def _ask_for_groupings(unique_labels):\n",
    "    \"\"\"\n",
    "    Ask user for groupings via input(), with robust fallback.\n",
    "    ENV override: if CLASS_GROUPINGS is set, parse from it and skip prompt.\n",
    "    Special keyword: 'AUTO' -> generate all non-empty bipartitions (pos vs neg).\n",
    "    \"\"\"\n",
    "    env_s = os.environ.get(\"CLASS_GROUPINGS\", \"\").strip()\n",
    "    if env_s:\n",
    "        groups = _parse_groupings_from_string(env_s)\n",
    "        if not groups and env_s.upper() == \"AUTO\":\n",
    "            return auto_groupings(unique_labels)\n",
    "        if groups:\n",
    "            return groups\n",
    "        print(\"[WARN] Failed to parse CLASS_GROUPINGS from environment; falling back to prompt.\", file=sys.stderr)\n",
    "\n",
    "    prompt = (\n",
    "        \"\\nEnter class groupings as pairs of POS vs NEG (examples):\\n\"\n",
    "        \"  1) Python-literal:  [((1,), (0,)), ((2,), (0,)), ((3,), (0,))]\\n\"\n",
    "        \"  2) Pipe/semicolon:  1|0; 2|0; 3|0\\n\"\n",
    "        \"  3) Tuple pairs:     ((2,3),(0,1)); ((1,2),(0,3))\\n\"\n",
    "        \"Type 'AUTO' for all non-empty bipartitions of observed labels.\\n\"\n",
    "        \"Press Enter for default: ((1),(0)); ((2),(0)); ((3),(0)); ((2,3),(0,1))\\n\"\n",
    "        \"Your groupings: \"\n",
    "    )\n",
    "    try:\n",
    "        s = input(prompt)\n",
    "    except EOFError:\n",
    "        s = \"\"  # non-interactive: default\n",
    "\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        # default sensible set\n",
    "        return [((1,), (0,)), ((2,), (0,)), ((3,), (0,)), ((2,3), (0,1))]\n",
    "    if s.upper() == \"AUTO\":\n",
    "        return auto_groupings(unique_labels)\n",
    "\n",
    "    parsed = _parse_groupings_from_string(s)\n",
    "    if parsed:\n",
    "        return parsed\n",
    "\n",
    "    print(\"[WARN] Could not parse input; using default groupings.\", file=sys.stderr)\n",
    "    return [((1,), (0,)), ((2,), (0,)), ((3,), (0,)), ((2,3), (0,1))]\n",
    "\n",
    "# ----------------------------\n",
    "# Main (all bins): ask for groupings, loop per bin\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load once\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    # Discover bins\n",
    "    all_bins = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    all_bins = [35, 45, 55, 65, 75]\n",
    "    if len(all_bins) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only 0..3 by default (adjust if needed)\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "\n",
    "    # Ask for groupings (based on actually present labels)\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    # sanity: labels should be contiguous 0..C-1\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "\n",
    "    groupings = _ask_for_groupings(unique_labels)\n",
    "    print(f\"\\nUsing {len(groupings)} grouping(s): {groupings}\")\n",
    "\n",
    "    # Prepare output root\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    # Process every bin\n",
    "    for BIN_VALUE in all_bins:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "\n",
    "        # Strict scope so we can drop refs in finally\n",
    "        models_A = models_B = None\n",
    "        X = Y = fdf = None\n",
    "\n",
    "        try:\n",
    "            fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "            if fdf.empty:\n",
    "                print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all features except ['bin','target'] by (max+1) *within this bin*\n",
    "            cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "            fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "            Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "            X = np.nan_to_num(fdf.drop(columns=['bin', 'target']).to_numpy(), copy=False).astype(np.float32)\n",
    "\n",
    "            if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "                print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "                  f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "            # Output dirs per bin\n",
    "            OUT_DIR   = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "            RUN_A_DIR = os.path.join(OUT_DIR, \"run_A\")\n",
    "            RUN_B_DIR = os.path.join(OUT_DIR, \"run_B\")\n",
    "            COMP_DIR  = os.path.join(OUT_DIR, \"compare_AB\")\n",
    "            for d in [OUT_DIR, RUN_A_DIR, RUN_B_DIR, COMP_DIR]:\n",
    "                os.makedirs(d, exist_ok=True)\n",
    "                os.makedirs(os.path.join(d, \"plots\"), exist_ok=True)\n",
    "                os.makedirs(os.path.join(d, \"npy\"), exist_ok=True)\n",
    "                os.makedirs(os.path.join(d, \"csv\"), exist_ok=True)\n",
    "\n",
    "            # Train models ONCE per run (A, B), then reuse across all groupings\n",
    "            models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "            models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "            # helper: fixed grid for CSV/plots\n",
    "            def _make_grid(n):\n",
    "                n_grid = min(10000, n)\n",
    "                x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "                return n_grid, x_grid\n",
    "\n",
    "            # compute and save for one grouping\n",
    "            def save_and_plot_for_grouping(pos_ids, neg_ids):\n",
    "                # compute gradients for both runs (same X, different ensembles)\n",
    "                grad_A = compute_avg_group_logodds_gradient(\n",
    "                    X, models_A, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8\n",
    "                )\n",
    "                grad_B = compute_avg_group_logodds_gradient(\n",
    "                    X, models_B, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8\n",
    "                )\n",
    "\n",
    "                # tagging for filenames\n",
    "                pos_tag = \"_\".join(map(str, pos_ids))\n",
    "                neg_tag = \"_\".join(map(str, neg_ids))\n",
    "                tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "                # --------- SAVE EACH RUN SEPARATELY ----------\n",
    "                # grid (same for both)\n",
    "                n_grid, x_grid = _make_grid(min(grad_A.size, grad_B.size))\n",
    "\n",
    "                # Run A\n",
    "                np.save(os.path.join(RUN_A_DIR, \"npy\", f\"grad__{tag}.npy\"), grad_A)\n",
    "                pd.DataFrame({\"m/z\": x_grid, \"grad\": grad_A[:n_grid]}).to_csv(\n",
    "                    os.path.join(RUN_A_DIR, \"csv\", f\"grad__{tag}.csv\"), index=False\n",
    "                )\n",
    "                single_run_plot(\n",
    "                    x_grid, grad_A[:n_grid],\n",
    "                    title=f\"Bin {BIN_VALUE} — Run A gradient [{tag}]\",\n",
    "                    outfile=os.path.join(RUN_A_DIR, \"plots\", f\"{tag}__runA.png\"),\n",
    "                    label=\"Run A grad\"\n",
    "                )\n",
    "                with open(os.path.join(RUN_A_DIR, f\"summary__{tag}.json\"), \"w\") as fA:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"run\": \"A\",\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "                        \"k_splits\": K_SPLITS, \"n_repeats\": N_REPEATS,\n",
    "                        \"seed_base\": SEED_BASES[0],\n",
    "                        \"paths\": {\n",
    "                            \"grad_npy\": os.path.join(RUN_A_DIR, \"npy\", f\"grad__{tag}.npy\"),\n",
    "                            \"grad_csv\": os.path.join(RUN_A_DIR, \"csv\", f\"grad__{tag}.csv\"),\n",
    "                            \"grad_plot\": os.path.join(RUN_A_DIR, \"plots\", f\"{tag}__runA.png\"),\n",
    "                        }\n",
    "                    }, fA, indent=2)\n",
    "\n",
    "                # Run B\n",
    "                np.save(os.path.join(RUN_B_DIR, \"npy\", f\"grad__{tag}.npy\"), grad_B)\n",
    "                pd.DataFrame({\"m/z\": x_grid, \"grad\": grad_B[:n_grid]}).to_csv(\n",
    "                    os.path.join(RUN_B_DIR, \"csv\", f\"grad__{tag}.csv\"), index=False\n",
    "                )\n",
    "                single_run_plot(\n",
    "                    x_grid, grad_B[:n_grid],\n",
    "                    title=f\"Bin {BIN_VALUE} — Run B gradient [{tag}]\",\n",
    "                    outfile=os.path.join(RUN_B_DIR, \"plots\", f\"{tag}__runB.png\"),\n",
    "                    label=\"Run B grad\"\n",
    "                )\n",
    "                with open(os.path.join(RUN_B_DIR, f\"summary__{tag}.json\"), \"w\") as fB:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"run\": \"B\",\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "                        \"k_splits\": K_SPLITS, \"n_repeats\": N_REPEATS,\n",
    "                        \"seed_base\": SEED_BASES[1],\n",
    "                        \"paths\": {\n",
    "                            \"grad_npy\": os.path.join(RUN_B_DIR, \"npy\", f\"grad__{tag}.npy\"),\n",
    "                            \"grad_csv\": os.path.join(RUN_B_DIR, \"csv\", f\"grad__{tag}.csv\"),\n",
    "                            \"grad_plot\": os.path.join(RUN_B_DIR, \"plots\", f\"{tag}__runB.png\"),\n",
    "                        }\n",
    "                    }, fB, indent=2)\n",
    "\n",
    "                # --------- A vs B COMPARISON (mirror) ----------\n",
    "                yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "                yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "                yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "                yA_neg = np.where(yA < 0, -yA, 0.0)  # absolute value\n",
    "                yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "                # Cosine similarities\n",
    "                cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "                cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "                # Save combined CSV used for mirror inspection\n",
    "                comb_csv = os.path.join(COMP_DIR, \"csv\", f\"grads_AB__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"m/z\": x_grid,\n",
    "                    \"grad_runA\": yA,\n",
    "                    \"grad_runB\": yB,\n",
    "                    \"pos_runA\": yA_pos,\n",
    "                    \"pos_runB\": yB_pos,\n",
    "                    \"negabs_runA\": yA_neg,\n",
    "                    \"negabs_runB\": yB_neg,\n",
    "                }).to_csv(comb_csv, index=False)\n",
    "\n",
    "                # Mirror plots\n",
    "                pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                             f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "                neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                             f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_pos, yB_pos,\n",
    "                    title=pos_title,\n",
    "                    outfile=os.path.join(COMP_DIR, \"plots\", f\"{tag}__mirror_pos.png\")\n",
    "                )\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_neg, yB_neg,\n",
    "                    title=neg_title,\n",
    "                    outfile=os.path.join(COMP_DIR, \"plots\", f\"{tag}__mirror_negabs.png\")\n",
    "                )\n",
    "\n",
    "                # Comparison JSON\n",
    "                with open(os.path.join(COMP_DIR, f\"summary__{tag}.json\"), \"w\") as fC:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"comparison\": \"Run A vs Run B\",\n",
    "                        \"cosine_pos\": cos_pos,\n",
    "                        \"cosine_neg_abs\": cos_neg,\n",
    "                        \"paths\": {\n",
    "                            \"combined_csv\": comb_csv,\n",
    "                            \"mirror_pos_png\": os.path.join(COMP_DIR, \"plots\", f\"{tag}__mirror_pos.png\"),\n",
    "                            \"mirror_negabs_png\": os.path.join(COMP_DIR, \"plots\", f\"{tag}__mirror_negabs.png\"),\n",
    "                        }\n",
    "                    }, fC, indent=2)\n",
    "\n",
    "                print(f\"  Saved {tag}  |  Cos(pos)={cos_pos:.6f}  Cos(neg|abs|)={cos_neg:.6f}\")\n",
    "\n",
    "            # run all groupings\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for (pos_ids, neg_ids) in groupings:\n",
    "                for idx in (*pos_ids, *neg_ids):\n",
    "                    assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "                save_and_plot_for_grouping(pos_ids, neg_ids)\n",
    "\n",
    "        finally:\n",
    "            # Explicitly drop large references from this bin\n",
    "            try:\n",
    "                if models_A is not None:\n",
    "                    for _m in models_A:\n",
    "                        del _m\n",
    "                del models_A\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if models_B is not None:\n",
    "                    for _m in models_B:\n",
    "                        del _m\n",
    "                del models_B\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                del X\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                del Y\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                del fdf\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Close any stray figs and free TF/CPU/GPU memory back to OS\n",
    "            hard_free()\n",
    "\n",
    "    print(\"\\nAll bins processed.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ----------------------------\n",
    "    # User-configurable globals\n",
    "    # ----------------------------\n",
    "    # CSV_PATH, OUT_ROOT, EPOCHS, etc. are defined above. Run main.\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c326fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_2__neg_1.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_2__neg_0.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_1__neg_0.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_3__neg_2.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_3__neg_0_1_2.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_2_1__neg_0.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_3__neg_1.csv ...\n",
      "Processing F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\75__pos_3__neg_0.csv ...\n",
      "\n",
      "Done. Wrote 32 files to F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\\result\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Process all CSV files in a folder:\n",
    "- For each CSV, extract [\"m/z\", one of target cols]\n",
    "- Save into ./result/<basename>_<col>.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_COLS = [\"pos_runA\", \"pos_runB\", \"negabs_runA\", \"negabs_runB\"]\n",
    "MZ_COL = \"m/z\"\n",
    "\n",
    "def split_csv(input_path: str, out_dir: str) -> list[str]:\n",
    "    \"\"\"Split one CSV into multiple smaller CSVs.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    if MZ_COL not in df.columns:\n",
    "        print(f\"[SKIP] {input_path} (no '{MZ_COL}' column)\")\n",
    "        return []\n",
    "\n",
    "    available_targets = [c for c in TARGET_COLS if c in df.columns]\n",
    "    if not available_targets:\n",
    "        print(f\"[SKIP] {input_path} (none of {TARGET_COLS} found)\")\n",
    "        return []\n",
    "\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    written = []\n",
    "    for col in available_targets:\n",
    "        out_df = df[[MZ_COL, col]].copy()\n",
    "        out_path = os.path.join(out_dir, f\"{base}_{col}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "    return written\n",
    "\n",
    "def process_folder(folder_path: str):\n",
    "    \"\"\"Process all CSV files in a folder and save into subfolder 'result'.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise NotADirectoryError(f\"Not a folder: {folder_path}\")\n",
    "\n",
    "    out_dir = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    all_outputs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            print(f\"Processing {fpath} ...\")\n",
    "            outputs = split_csv(fpath, out_dir)\n",
    "            all_outputs.extend(outputs)\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(all_outputs)} files to {out_dir}\")\n",
    "    return all_outputs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: change this path\n",
    "    INPUT_FOLDER = \"F:/group_compare_all_bins_8_groups/bin_75/compare_AB/csv\"  # e.g. \"/mnt/data/myfolder\"\n",
    "    process_folder(INPUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} → {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"✅ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\decon\"\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f934ae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mirror plot to: F:\\decon\\decon\\5__pos_1__neg_0_negabs_mass.png\n"
     ]
    }
   ],
   "source": [
    "# mirror_plot_cosine.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ----------- Input file paths (edit these) -----------\n",
    "file_a = Path(r\"F:\\decon\\decon\\5__pos_1__neg_0_negabs_runA_mass.txt\")\n",
    "file_b = Path(r\"F:\\decon\\decon\\5__pos_1__neg_0_negabs_runB_mass.txt\")\n",
    "\n",
    "# ----------- Load spectra -----------\n",
    "def load_spectrum(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"mz\", \"intensity\"], engine=\"python\")\n",
    "    df = df.dropna()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\").fillna(0.0)\n",
    "    df = df[np.isfinite(df[\"mz\"]) & np.isfinite(df[\"intensity\"])]\n",
    "    return df.groupby(\"mz\", as_index=False)[\"intensity\"].sum().sort_values(\"mz\")\n",
    "\n",
    "spec_a = load_spectrum(file_a)\n",
    "spec_b = load_spectrum(file_b)\n",
    "\n",
    "# ----------- Align spectra on a common m/z grid -----------\n",
    "aligned = pd.merge(spec_a, spec_b, on=\"mz\", how=\"outer\", suffixes=(\"_a\", \"_b\")).fillna(0.0).sort_values(\"mz\")\n",
    "v_a = aligned[\"intensity_a\"].to_numpy(dtype=float)\n",
    "v_b = aligned[\"intensity_b\"].to_numpy(dtype=float)\n",
    "\n",
    "# ----------- Cosine similarity -----------\n",
    "def cosine_similarity(x, y):\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    y_norm = np.linalg.norm(y)\n",
    "    if x_norm == 0.0 or y_norm == 0.0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.dot(x, y) / (x_norm * y_norm))\n",
    "\n",
    "cos_sim = cosine_similarity(v_a, v_b)\n",
    "\n",
    "# ----------- Normalize for plotting -----------\n",
    "def max_norm(v):\n",
    "    m = np.max(np.abs(v)) if len(v) else 1.0\n",
    "    return (v / m) if m > 0 else v\n",
    "\n",
    "plot_a = max_norm(v_a)\n",
    "plot_b = -max_norm(v_b)  # mirror below axis\n",
    "\n",
    "# ----------- Clean filename (remove run info) -----------\n",
    "def clean_name(filename: str) -> str:\n",
    "    return re.sub(r\"_run[AB]\", \"\", filename)\n",
    "\n",
    "short_name = clean_name(file_a.stem)\n",
    "\n",
    "# ----------- Plot -----------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(aligned[\"mz\"].values, plot_a, color=\"blue\")\n",
    "ax.plot(aligned[\"mz\"].values, plot_b, color=\"red\")\n",
    "\n",
    "ax.axhline(0, linewidth=1, color=\"black\")\n",
    "ax.set_xlabel(\"m/z\")\n",
    "ax.set_ylabel(\"Normalized intensity (top vs mirrored)\")\n",
    "\n",
    "title = f\"{short_name} | Cosine similarity = {cos_sim:.4f}\"\n",
    "ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ----------- Save output PNG -----------\n",
    "out_path = file_a.parent / f\"{short_name}.png\"\n",
    "plt.savefig(out_path, dpi=160)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved mirror plot to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1183a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mirror plot to: F:\\decon\\plot\\5__pos_1__neg_0_negabs_mass.png\n"
     ]
    }
   ],
   "source": [
    "# mirror_plot_cosine.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ----------- Input file paths (edit these) -----------\n",
    "file_a = Path(r\"F:\\decon\\decon\\5__pos_1__neg_0_negabs_runA_mass.txt\")\n",
    "file_b = Path(r\"F:\\decon\\decon\\5__pos_1__neg_0_negabs_runB_mass.txt\")\n",
    "\n",
    "# ----------- Output folder (edit this) -----------\n",
    "output_folder = Path(r\"F:\\decon\\plot\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------- Load spectra -----------\n",
    "def load_spectrum(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"mz\", \"intensity\"], engine=\"python\")\n",
    "    df = df.dropna()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\").fillna(0.0)\n",
    "    df = df[np.isfinite(df[\"mz\"]) & np.isfinite(df[\"intensity\"])]\n",
    "    return df.groupby(\"mz\", as_index=False)[\"intensity\"].sum().sort_values(\"mz\")\n",
    "\n",
    "spec_a = load_spectrum(file_a)\n",
    "spec_b = load_spectrum(file_b)\n",
    "\n",
    "# ----------- Align spectra on a common m/z grid -----------\n",
    "aligned = pd.merge(spec_a, spec_b, on=\"mz\", how=\"outer\", suffixes=(\"_a\", \"_b\")).fillna(0.0).sort_values(\"mz\")\n",
    "v_a = aligned[\"intensity_a\"].to_numpy(dtype=float)\n",
    "v_b = aligned[\"intensity_b\"].to_numpy(dtype=float)\n",
    "\n",
    "# ----------- Cosine similarity -----------\n",
    "def cosine_similarity(x, y):\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    y_norm = np.linalg.norm(y)\n",
    "    if x_norm == 0.0 or y_norm == 0.0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.dot(x, y) / (x_norm * y_norm))\n",
    "\n",
    "cos_sim = cosine_similarity(v_a, v_b)\n",
    "\n",
    "# ----------- Normalize for plotting -----------\n",
    "def max_norm(v):\n",
    "    m = np.max(np.abs(v)) if len(v) else 1.0\n",
    "    return (v / m) if m > 0 else v\n",
    "\n",
    "plot_a = max_norm(v_a)\n",
    "plot_b = -max_norm(v_b)  # mirror below axis\n",
    "\n",
    "# ----------- Clean filename (remove run info) -----------\n",
    "def clean_name(filename: str) -> str:\n",
    "    return re.sub(r\"_run[AB]\", \"\", filename)\n",
    "\n",
    "short_name = clean_name(file_a.stem)\n",
    "\n",
    "# ----------- Plot -----------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(aligned[\"mz\"].values, plot_a, color=\"blue\")\n",
    "ax.plot(aligned[\"mz\"].values, plot_b, color=\"red\")\n",
    "\n",
    "ax.axhline(0, linewidth=1, color=\"black\")\n",
    "ax.set_xlabel(\"m/z\")\n",
    "ax.set_ylabel(\"Normalized intensity (top vs mirrored)\")\n",
    "\n",
    "title = f\"{short_name} | Cosine similarity = {cos_sim:.4f}\"\n",
    "ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ----------- Save output PNG -----------\n",
    "out_path = output_folder / f\"{short_name}.png\"\n",
    "plt.savefig(out_path, dpi=160)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved mirror plot to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_mirror_plots.py\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= Optional defaults (used if no CLI flags are given) =========\n",
    "# Set these to your folder paths if you want to run without CLI args:\n",
    "INPUT_DIR_DEFAULT  = r\"F:/decon/decon/\"  # e.g., r\"F:\\path\\to\\your\\folder\"\n",
    "OUTPUT_DIR_DEFAULT = r\"F:/decon/plot/\"  # leave empty \"\" to save into input dir\n",
    "PATTERN_DEFAULT    = \"*.txt\"\n",
    "MZ_DECIMALS_DEFAULT = 5\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "def try_parse_args():\n",
    "    \"\"\"\n",
    "    Try to parse CLI args. If none provided (or parsing fails due to missing required args),\n",
    "    return None to trigger fallback to top-of-file defaults.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Create mirror plots for runA/runB replicate pairs in a folder.\"\n",
    "    )\n",
    "    parser.add_argument(\"--input_dir\", type=str, required=True,\n",
    "                        help=\"Folder containing spectra text files.\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=None,\n",
    "                        help=\"Folder to save PNG plots (defaults to input_dir).\")\n",
    "    parser.add_argument(\"--pattern\", type=str, default=PATTERN_DEFAULT,\n",
    "                        help=f\"Glob pattern for spectra files (default: {PATTERN_DEFAULT}).\")\n",
    "    parser.add_argument(\"--mz_decimals\", type=int, default=MZ_DECIMALS_DEFAULT,\n",
    "                        help=f\"Round m/z to this many decimals before alignment (default: {MZ_DECIMALS_DEFAULT}).\")\n",
    "\n",
    "    # If script called with no extra args (len==1), skip argparse error and use defaults\n",
    "    if len(sys.argv) == 1:\n",
    "        return None  # signal fallback\n",
    "\n",
    "    try:\n",
    "        return parser.parse_args()\n",
    "    except SystemExit:\n",
    "        # Argparse already printed an error (e.g., missing --input_dir). Fall back.\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_spectrum(path: Path, mz_decimals: int = 5) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"mz\", \"intensity\"], engine=\"python\")\n",
    "    df = df.dropna()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\").fillna(0.0)\n",
    "    df = df[np.isfinite(df[\"mz\"]) & np.isfinite(df[\"intensity\"])]\n",
    "    if mz_decimals is not None and mz_decimals >= 0:\n",
    "        df[\"mz\"] = df[\"mz\"].round(mz_decimals)\n",
    "    return df.groupby(\"mz\", as_index=False)[\"intensity\"].sum().sort_values(\"mz\")\n",
    "\n",
    "\n",
    "def cosine_similarity(x, y) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    nx, ny = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    if nx == 0.0 or ny == 0.0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.dot(x, y) / (nx * ny))\n",
    "\n",
    "\n",
    "def max_norm(v: np.ndarray) -> np.ndarray:\n",
    "    if v.size == 0:\n",
    "        return v\n",
    "    m = np.max(np.abs(v))\n",
    "    return v / m if m > 0 else v\n",
    "\n",
    "\n",
    "def split_key_and_run(stem: str):\n",
    "    \"\"\"\n",
    "    From a filename stem like: '5__pos_1__neg_0_negabs_runA_mass'\n",
    "    return:\n",
    "      key_without_run: '5__pos_1__neg_0_negabs_mass'\n",
    "      run: 'A' or 'B' (or None)\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(?P<prefix>.*)_run(?P<run>[AB])(?P<suffix>.*)\", stem)\n",
    "    if not m:\n",
    "        return stem, None\n",
    "    key = f\"{m.group('prefix')}{m.group('suffix')}\"\n",
    "    run = m.group(\"run\")\n",
    "    return key, run\n",
    "\n",
    "\n",
    "def make_mirror_plot(aligned: pd.DataFrame, v_a: np.ndarray, v_b: np.ndarray,\n",
    "                     title_name: str, out_path: Path):\n",
    "    plot_a = max_norm(v_a)\n",
    "    plot_b = -max_norm(v_b)  # mirrored\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(aligned[\"mz\"].values, plot_a)  # default color\n",
    "    ax.plot(aligned[\"mz\"].values, plot_b)  # default color\n",
    "    ax.axhline(0, linewidth=1, color=\"black\")\n",
    "    ax.set_xlabel(\"m/z\")\n",
    "    ax.set_ylabel(\"Normalized intensity (top vs mirrored)\")\n",
    "    ax.set_title(title_name)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = try_parse_args()\n",
    "\n",
    "    if args is None:\n",
    "        # Fallback to top-of-file defaults\n",
    "        if not INPUT_DIR_DEFAULT:\n",
    "            print(\n",
    "                \"[ERROR] No --input_dir provided and INPUT_DIR_DEFAULT is empty.\\n\"\n",
    "                \"Either run with CLI flags, e.g.\\n\"\n",
    "                '  python batch_mirror_plots.py --input_dir \"F:\\\\path\\\\to\\\\folder\" --output_dir \"F:\\\\path\\\\to\\\\pngs\"\\n'\n",
    "                \"or set INPUT_DIR_DEFAULT at the top of this script.\"\n",
    "            )\n",
    "            sys.exit(2)\n",
    "\n",
    "        input_dir = Path(INPUT_DIR_DEFAULT).expanduser().resolve()\n",
    "        output_dir = (Path(OUTPUT_DIR_DEFAULT).expanduser().resolve()\n",
    "                      if OUTPUT_DIR_DEFAULT else input_dir)\n",
    "        pattern = PATTERN_DEFAULT\n",
    "        mz_decimals = MZ_DECIMALS_DEFAULT\n",
    "        print(f\"[INFO] Using defaults: input_dir={input_dir}, output_dir={output_dir}, \"\n",
    "              f\"pattern={pattern}, mz_decimals={mz_decimals}\")\n",
    "    else:\n",
    "        input_dir = Path(args.input_dir).expanduser().resolve()\n",
    "        output_dir = Path(args.output_dir).expanduser().resolve() if args.output_dir else input_dir\n",
    "        pattern = args.pattern\n",
    "        mz_decimals = args.mz_decimals\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(input_dir.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[INFO] No files found in {input_dir} matching pattern {pattern}.\")\n",
    "        return\n",
    "\n",
    "    # Build pairs: (key_without_run) -> {'A': Path, 'B': Path}\n",
    "    pairs = {}\n",
    "    for p in files:\n",
    "        key, run = split_key_and_run(p.stem)\n",
    "        if run not in (\"A\", \"B\"):\n",
    "            continue\n",
    "        entry = pairs.setdefault(key, {})\n",
    "        entry[run] = p\n",
    "\n",
    "    total_pairs = sum(1 for v in pairs.values() if \"A\" in v and \"B\" in v)\n",
    "    print(f\"[INFO] Found {len(pairs)} candidate keys; {total_pairs} A/B pairs will be processed.\")\n",
    "\n",
    "    for key, d in sorted(pairs.items()):\n",
    "        if \"A\" not in d or \"B\" not in d:\n",
    "            missing = \"A\" if \"A\" not in d else \"B\"\n",
    "            print(f\"[WARN] Skipping '{key}': missing run{missing}.\")\n",
    "            continue\n",
    "\n",
    "        path_a, path_b = d[\"A\"], d[\"B\"]\n",
    "        try:\n",
    "            spec_a = load_spectrum(path_a, mz_decimals=mz_decimals)\n",
    "            spec_b = load_spectrum(path_b, mz_decimals=mz_decimals)\n",
    "\n",
    "            aligned = (\n",
    "                pd.merge(spec_a, spec_b, on=\"mz\", how=\"outer\", suffixes=(\"_a\", \"_b\"))\n",
    "                .fillna(0.0)\n",
    "                .sort_values(\"mz\")\n",
    "            )\n",
    "            v_a = aligned[\"intensity_a\"].to_numpy(dtype=float)\n",
    "            v_b = aligned[\"intensity_b\"].to_numpy(dtype=float)\n",
    "            cs = cosine_similarity(v_a, v_b)\n",
    "\n",
    "            title = f\"{key} | Cosine similarity = {cs:.4f}\"\n",
    "            out_path = output_dir / f\"{key}.png\"\n",
    "            make_mirror_plot(aligned, v_a, v_b, title, out_path)\n",
    "\n",
    "            print(f\"[OK] Saved: {out_path}  (cosine={cs:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed on key '{key}' ({path_a.name} vs {path_b.name}): {e}\")\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_mirror_plots.py\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= Optional defaults if you run without CLI flags =========\n",
    "INPUT_DIR_DEFAULT  = r\"F:/decon/decon/\"  # e.g., r\"F:\\path\\to\\your\\folder\"\n",
    "OUTPUT_DIR_DEFAULT = r\"F:/decon/plot/plot/\"  # leave empty \"\" to save into input dir\n",
    "PATTERN_DEFAULT    = \"*_run[AB]_mass.*\"  # match runA/runB files\n",
    "MZ_DECIMALS_DEFAULT = 5\n",
    "# ==================================================================\n",
    "\n",
    "def try_parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Create mirror plots for runA/runB replicate pairs in a folder.\"\n",
    "    )\n",
    "    parser.add_argument(\"--input_dir\", type=str, required=True,\n",
    "                        help=\"Folder containing spectra files.\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=None,\n",
    "                        help=\"Folder to save PNG plots (defaults to input_dir).\")\n",
    "    parser.add_argument(\"--pattern\", type=str, default=PATTERN_DEFAULT,\n",
    "                        help=f\"Glob pattern for spectra files (default: {PATTERN_DEFAULT}).\")\n",
    "    parser.add_argument(\"--mz_decimals\", type=int, default=MZ_DECIMALS_DEFAULT,\n",
    "                        help=f\"Round m/z to this many decimals before alignment (default: {MZ_DECIMALS_DEFAULT}).\")\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        return None  # fall back to top-of-file defaults\n",
    "    try:\n",
    "        return parser.parse_args()\n",
    "    except SystemExit:\n",
    "        return None\n",
    "\n",
    "def load_spectrum(path: Path, mz_decimals: int = 5) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"mz\", \"intensity\"], engine=\"python\")\n",
    "    df = df.dropna()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\").fillna(0.0)\n",
    "    df = df[np.isfinite(df[\"mz\"]) & np.isfinite(df[\"intensity\"])]\n",
    "    if mz_decimals is not None and mz_decimals >= 0:\n",
    "        df[\"mz\"] = df[\"mz\"].round(mz_decimals)\n",
    "    return df.groupby(\"mz\", as_index=False)[\"intensity\"].sum().sort_values(\"mz\")\n",
    "\n",
    "def cosine_similarity(x, y) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    nx, ny = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    if nx == 0.0 or ny == 0.0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.dot(x, y) / (nx * ny))\n",
    "\n",
    "def max_norm(v: np.ndarray) -> np.ndarray:\n",
    "    if v.size == 0:\n",
    "        return v\n",
    "    m = np.max(np.abs(v))\n",
    "    return v / m if m > 0 else v\n",
    "\n",
    "# ==== STRICT pattern: <prefix>_run(A|B)_mass(.ext) -> key = <prefix>_mass ====\n",
    "RUN_RE = re.compile(r\"^(?P<prefix>.+)_run(?P<run>[AB])_mass(?:\\.[^.]+)?$\", re.IGNORECASE)\n",
    "\n",
    "def split_key_and_run_from_name(filename: str):\n",
    "    \"\"\"\n",
    "    filename is the base name (with extension). Match:\n",
    "      <prefix>_runA_mass(.ext) OR <prefix>_runB_mass(.ext)\n",
    "    Returns:\n",
    "      key_without_run = '<prefix>_mass'\n",
    "      run = 'A' or 'B' (or None if not matching)\n",
    "    \"\"\"\n",
    "    m = RUN_RE.match(filename)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    key = f\"{m.group('prefix')}_mass\"\n",
    "    run = m.group(\"run\").upper()\n",
    "    return key, run\n",
    "\n",
    "def make_mirror_plot(aligned: pd.DataFrame, v_a: np.ndarray, v_b: np.ndarray,\n",
    "                     title_name: str, out_path: Path):\n",
    "    plot_a = max_norm(v_a)\n",
    "    plot_b = -max_norm(v_b)  # mirrored\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(aligned[\"mz\"].values, plot_a)  # default color\n",
    "    ax.plot(aligned[\"mz\"].values, plot_b)  # default color\n",
    "    ax.axhline(0, linewidth=1, color=\"black\")\n",
    "    ax.set_xlabel(\"m/z\")\n",
    "    ax.set_ylabel(\"Normalized intensity (top vs mirrored)\")\n",
    "    ax.set_title(title_name)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def main():\n",
    "    args = try_parse_args()\n",
    "\n",
    "    if args is None:\n",
    "        if not INPUT_DIR_DEFAULT:\n",
    "            print(\n",
    "                \"[ERROR] No --input_dir provided and INPUT_DIR_DEFAULT is empty.\\n\"\n",
    "                \"Either run with CLI flags, e.g.\\n\"\n",
    "                '  python batch_mirror_plots.py --input_dir \"F:\\\\path\\\\to\\\\folder\" --output_dir \"F:\\\\path\\\\to\\\\pngs\"\\n'\n",
    "                \"or set INPUT_DIR_DEFAULT at the top of this script.\"\n",
    "            )\n",
    "            sys.exit(2)\n",
    "        input_dir = Path(INPUT_DIR_DEFAULT).expanduser().resolve()\n",
    "        output_dir = (Path(OUTPUT_DIR_DEFAULT).expanduser().resolve()\n",
    "                      if OUTPUT_DIR_DEFAULT else input_dir)\n",
    "        pattern = PATTERN_DEFAULT\n",
    "        mz_decimals = MZ_DECIMALS_DEFAULT\n",
    "        print(f\"[INFO] Using defaults: input_dir={input_dir}, output_dir={output_dir}, \"\n",
    "              f\"pattern={pattern}, mz_decimals={mz_decimals}\")\n",
    "    else:\n",
    "        input_dir = Path(args.input_dir).expanduser().resolve()\n",
    "        output_dir = Path(args.output_dir).expanduser().resolve() if args.output_dir else input_dir\n",
    "        pattern = args.pattern\n",
    "        mz_decimals = args.mz_decimals\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(input_dir.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[INFO] No files found in {input_dir} matching pattern {pattern}.\")\n",
    "        return\n",
    "\n",
    "    # Build pairs strictly by the <prefix>_runX_mass pattern\n",
    "    pairs = {}\n",
    "    for p in files:\n",
    "        key, run = split_key_and_run_from_name(p.name)\n",
    "        if key is None or run not in (\"A\", \"B\"):\n",
    "            # Not a strict runA/runB _mass file -> skip\n",
    "            continue\n",
    "        entry = pairs.setdefault(key, {})\n",
    "        entry[run] = p\n",
    "\n",
    "    total_pairs = sum(1 for v in pairs.values() if \"A\" in v and \"B\" in v)\n",
    "    print(f\"[INFO] Found {len(pairs)} candidate keys; {total_pairs} A/B pairs will be processed.\")\n",
    "\n",
    "    for key, d in sorted(pairs.items()):\n",
    "        if \"A\" not in d or \"B\" not in d:\n",
    "            missing = \"A\" if \"A\" not in d else \"B\"\n",
    "            print(f\"[WARN] Skipping '{key}': missing run{missing}.\")\n",
    "            continue\n",
    "\n",
    "        path_a, path_b = d[\"A\"], d[\"B\"]\n",
    "        try:\n",
    "            spec_a = load_spectrum(path_a, mz_decimals=mz_decimals)\n",
    "            spec_b = load_spectrum(path_b, mz_decimals=mz_decimals)\n",
    "\n",
    "            aligned = (\n",
    "                pd.merge(spec_a, spec_b, on=\"mz\", how=\"outer\", suffixes=(\"_a\", \"_b\"))\n",
    "                .fillna(0.0)\n",
    "                .sort_values(\"mz\")\n",
    "            )\n",
    "            v_a = aligned[\"intensity_a\"].to_numpy(dtype=float)\n",
    "            v_b = aligned[\"intensity_b\"].to_numpy(dtype=float)\n",
    "            cs = cosine_similarity(v_a, v_b)\n",
    "\n",
    "            title = f\"{key} | Cosine similarity = {cs:.4f}\"\n",
    "            out_path = output_dir / f\"{key}.png\"   # filename WITHOUT cosine values\n",
    "            make_mirror_plot(aligned, v_a, v_b, title, out_path)\n",
    "\n",
    "            print(f\"[OK] Saved: {out_path}  (cosine={cs:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed on key '{key}' ({path_a.name} vs {path_b.name}): {e}\")\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
