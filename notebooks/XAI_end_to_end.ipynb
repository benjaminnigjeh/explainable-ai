{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd458237",
   "metadata": {},
   "source": [
    "Generating informative features from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for _gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def hard_free():\n",
    "    \"\"\"Aggressively release memory after each bin.\"\"\"\n",
    "    try: plt.close('all')\n",
    "    except: pass\n",
    "    try: tf.keras.backend.clear_session()\n",
    "    except: pass\n",
    "    try: gc.collect(); gc.collect()\n",
    "    except: pass\n",
    "    try:\n",
    "        import ctypes, platform\n",
    "        if platform.system().lower() == \"linux\":\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except: pass\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8) -> np.ndarray:\n",
    "    X_t = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N = int(X_t.shape[0])\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X_t[i:i+1]\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (cosine + plotting)\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Splitter (4 CSVs per combined CSV) with bin prefix\n",
    "# ----------------------------\n",
    "TARGET_COLS = [\"pos_runA\", \"pos_runB\", \"negabs_runA\", \"negabs_runB\"]\n",
    "MZ_COL = \"m/z\"\n",
    "\n",
    "def split_csv(input_path: str, out_dir: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split one combined CSV into 4 CSVs, prefixing filenames with bin number.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    if MZ_COL not in df.columns:\n",
    "        print(f\"[SKIP] {input_path} (no '{MZ_COL}' column)\")\n",
    "        return []\n",
    "    available_targets = [c for c in TARGET_COLS if c in df.columns]\n",
    "    if not available_targets:\n",
    "        print(f\"[SKIP] {input_path} (none of {TARGET_COLS} found)\")\n",
    "        return []\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    written = []\n",
    "    for col in available_targets:\n",
    "        out_df = df[[MZ_COL, col]].copy()\n",
    "        out_path = os.path.join(out_dir, f\"bin{bin_value}_{base}_{col}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "    return written\n",
    "\n",
    "def process_folder(folder_path: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split all CSVs in a folder to 'result/' and return list of result paths.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"[WARN] Not a folder: {folder_path}\")\n",
    "        return []\n",
    "    out_dir = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    all_outputs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            print(f\"Splitting {fpath} ...\")\n",
    "            outputs = split_csv(fpath, out_dir, bin_value)\n",
    "            all_outputs.extend(outputs)\n",
    "\n",
    "    print(f\"Split done. Wrote {len(all_outputs)} files to {out_dir}\")\n",
    "    return all_outputs\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/casts/dataset_rt.csv\"   # input dataset with 'bin' and 'target'\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 1\n",
    "SEED_BASES  = [111, 777]                   # two independent runs\n",
    "OUT_ROOT    = r\"F:/test/resr/group_compare_only\"      # everything goes directly under bin_<N>/\n",
    "BIN_WHITELIST = None  # e.g. [35, 75]\n",
    "\n",
    "# Fixed grouping list (no prompt, no AUTO)\n",
    "GROUPINGS = [((1,), (0,)), ((2,), (0,)), ((3,), (0,)), ((2,3), (0,1))]\n",
    "\n",
    "# ----------------------------\n",
    "# KFold+Repeats trainer\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va), verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (bin-root outputs + keep only split CSVs)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Discover bins\n",
    "    bins_found = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    if BIN_WHITELIST is not None:\n",
    "        bins_to_process = [b for b in bins_found if b in set(BIN_WHITELIST)]\n",
    "    else:\n",
    "        bins_to_process = bins_found\n",
    "    if len(bins_to_process) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only classes 0..3 by default\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "    groupings = GROUPINGS\n",
    "    print(f\"\\nUsing fixed grouping(s): {groupings}\")\n",
    "\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    for BIN_VALUE in bins_to_process:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "        models_A = models_B = None\n",
    "        X = Y = fdf = None\n",
    "\n",
    "        try:\n",
    "            fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "            if fdf.empty:\n",
    "                print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all features except ['bin','target'] within this bin\n",
    "            cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "            fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "            Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "            X = np.nan_to_num(fdf.drop(columns=['bin', 'target']).to_numpy(), copy=False).astype(np.float32)\n",
    "\n",
    "            if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "                print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "                  f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "            # Output dirs per bin (everything under bin folder)\n",
    "            OUT_DIR   = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "            CSV_DIR   = os.path.join(OUT_DIR, \"csv\")\n",
    "            PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            os.makedirs(CSV_DIR, exist_ok=True)\n",
    "            os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "            # Train ensembles ONCE per run (A, B)\n",
    "            models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "            models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "            # grid helper\n",
    "            def _make_grid(n):\n",
    "                n_grid = min(10000, n)\n",
    "                x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "                return n_grid, x_grid\n",
    "\n",
    "            def save_compare_only(pos_ids, neg_ids):\n",
    "                grad_A = compute_avg_group_logodds_gradient(X, models_A, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_B = compute_avg_group_logodds_gradient(X, models_B, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "\n",
    "                pos_tag = \"_\".join(map(str, pos_ids))\n",
    "                neg_tag = \"_\".join(map(str, neg_ids))\n",
    "                tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "                n_grid, x_grid = _make_grid(min(grad_A.size, grad_B.size))\n",
    "                yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "                yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "                yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "                yA_neg = np.where(yA < 0, -yA, 0.0)  # abs\n",
    "                yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "                cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "                cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "                # --- write combined CSV under bin/<csv> ---\n",
    "                comb_csv = os.path.join(CSV_DIR, f\"grads_AB__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"m/z\": x_grid,\n",
    "                    \"grad_runA\": yA,\n",
    "                    \"grad_runB\": yB,\n",
    "                    \"pos_runA\": yA_pos,\n",
    "                    \"pos_runB\": yB_pos,\n",
    "                    \"negabs_runA\": yA_neg,\n",
    "                    \"negabs_runB\": yB_neg,\n",
    "                }).to_csv(comb_csv, index=False)\n",
    "\n",
    "                # plots under bin/<plots>\n",
    "                pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                             f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "                neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                             f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_pos, yB_pos,\n",
    "                    title=pos_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_pos.png\")\n",
    "                )\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_neg, yB_neg,\n",
    "                    title=neg_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_negabs.png\")\n",
    "                )\n",
    "\n",
    "                # summary JSON directly under bin/\n",
    "                with open(os.path.join(OUT_DIR, f\"summary__{tag}.json\"), \"w\") as fC:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"comparison\": \"Run A vs Run B\",\n",
    "                        \"cosine_pos\": cos_pos,\n",
    "                        \"cosine_neg_abs\": cos_neg,\n",
    "                        \"paths\": {\n",
    "                            \"combined_csv\": comb_csv,\n",
    "                            \"plots_dir\": PLOTS_DIR,\n",
    "                        }\n",
    "                    }, fC, indent=2)\n",
    "\n",
    "                print(f\"  [COMPARE] {tag}  |  Cos(pos)={cos_pos:.6f}  Cos(neg|abs|)={cos_neg:.6f}\")\n",
    "\n",
    "            # run all groupings -> write combined CSVs/plots/JSONs\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for (pos_ids, neg_ids) in groupings:\n",
    "                for idx in (*pos_ids, *neg_ids):\n",
    "                    assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "                save_compare_only(pos_ids, neg_ids)\n",
    "\n",
    "            # --- Split step: create split CSVs into csv/result/ ---\n",
    "            split_outputs = process_folder(CSV_DIR, int(BIN_VALUE))\n",
    "\n",
    "            # Move split files from csv/result/ -> csv/ and delete originals\n",
    "            result_dir = os.path.join(CSV_DIR, \"result\")\n",
    "            moved = []\n",
    "            if os.path.isdir(result_dir):\n",
    "                for fname in os.listdir(result_dir):\n",
    "                    src = os.path.join(result_dir, fname)\n",
    "                    dst = os.path.join(CSV_DIR, fname)\n",
    "                    os.replace(src, dst)\n",
    "                    moved.append(dst)\n",
    "                # try to remove now-empty result dir\n",
    "                try: os.rmdir(result_dir)\n",
    "                except OSError: pass\n",
    "\n",
    "            # Delete original combined CSVs (keep only split)\n",
    "            for fname in os.listdir(CSV_DIR):\n",
    "                if fname.lower().endswith(\".csv\") and fname.startswith(\"grads_AB__\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(CSV_DIR, fname))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not remove {fname}: {e}\")\n",
    "\n",
    "            # Save manifest of final CSVs\n",
    "            with open(os.path.join(OUT_DIR, \"split_manifest.json\"), \"w\") as fman:\n",
    "                json.dump({\n",
    "                    \"bin\": BIN_VALUE,\n",
    "                    \"final_csvs\": sorted([os.path.basename(p) for p in moved]),\n",
    "                }, fman, indent=2)\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                if models_A is not None:\n",
    "                    for _m in models_A: del _m\n",
    "                del models_A\n",
    "            except: pass\n",
    "            try:\n",
    "                if models_B is not None:\n",
    "                    for _m in models_B: del _m\n",
    "                del models_B\n",
    "            except: pass\n",
    "            for v in [\"X\",\"Y\",\"fdf\"]:\n",
    "                try: del globals()[v]\n",
    "                except: pass\n",
    "            hard_free()\n",
    "\n",
    "    print(\"\\nAll bins processed. (bin-root outputs; csv keeps only split files)\\n\")\n",
    "\n",
    "# ---- run ----\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139423a",
   "metadata": {},
   "source": [
    "Generating deconvoluted spectra from the informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7cf998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: bin5_grads_AB__pos_1__neg_0_pos_runA.csv → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_pos_runB.csv → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_negabs_runA.csv → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: bin5_grads_AB__pos_1__neg_0_negabs_runB.csv → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB\n",
      "✅ All files processed. Results saved in: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\n",
      "Staging: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt → C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt → C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Staging: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt → C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt → C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_3srhqajl\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt → F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\\bin5_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "📂 Clean result folder ready with only *_mass.txt files: F:\\test\\resr\\group_compare_only\\bin_5\\csv\\result\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def _unique_dst_path(dst_dir, fname):\n",
    "    \"\"\"Return a unique path in dst_dir for fname, adding a numeric suffix if needed.\"\"\"\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    candidate = os.path.join(dst_dir, fname)\n",
    "    i = 1\n",
    "    while os.path.exists(candidate):\n",
    "        candidate = os.path.join(dst_dir, f\"{base}__{i}{ext}\")\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "def _prefixed_name(src_path, result_root):\n",
    "    \"\"\"\n",
    "    Build a safer filename using the immediate parent folder under result/ as a prefix\n",
    "    to reduce collisions: e.g., result/sampleA/sampleA_mass.txt -> sampleA__sampleA_mass.txt\n",
    "    \"\"\"\n",
    "    # src_path like .../result/<parent>/<file>\n",
    "    parent = os.path.basename(os.path.dirname(src_path))\n",
    "    fname = os.path.basename(src_path)\n",
    "    return f\"{parent}__{fname}\" if parent and parent != \"result\" else fname\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder (top-level only)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} → {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"✅ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "    # 1) Collect *_mass.txt paths from result_root (including subfolders)\n",
    "    collected = []\n",
    "    for root, _, files in os.walk(result_root):\n",
    "        for f in files:\n",
    "            if f.endswith(\"_mass.txt\"):\n",
    "                collected.append(os.path.join(root, f))\n",
    "\n",
    "    if not collected:\n",
    "        print(\"⚠️ No *_mass.txt files found under:\", result_root)\n",
    "        return\n",
    "\n",
    "    # 2) Copy them to a temp folder FIRST (so deleting result/ content won't break src paths)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"mass_collect_\")\n",
    "    copied = []\n",
    "    for src in collected:\n",
    "        try:\n",
    "            # Prefix with subfolder name to avoid collisions\n",
    "            safe_name = _prefixed_name(src, result_root)\n",
    "            dst = os.path.join(temp_dir, safe_name)\n",
    "            dst = _unique_dst_path(temp_dir, os.path.basename(dst))  # ensure uniqueness\n",
    "            print(f\"Staging: {src} → {dst}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            copied.append(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skip (copy error): {src} — {e}\")\n",
    "\n",
    "    # 3) Clean the result_root completely\n",
    "    for item in os.listdir(result_root):\n",
    "        item_path = os.path.join(result_root, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not remove {item_path}: {e}\")\n",
    "\n",
    "    # 4) Move staged files back into a clean result_root\n",
    "    for staged in copied:\n",
    "        try:\n",
    "            final_dst = os.path.join(result_root, os.path.basename(staged))\n",
    "            final_dst = _unique_dst_path(result_root, os.path.basename(final_dst))\n",
    "            print(f\"Finalizing: {staged} → {final_dst}\")\n",
    "            shutil.move(staged, final_dst)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Move error for {staged}: {e}\")\n",
    "\n",
    "    # 5) Remove temp dir (ignore errors)\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"📂 Clean result folder ready with only *_mass.txt files:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\test\\resr\\group_compare_only\\bin_5\\csv\"  # <-- replace with your folder\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
